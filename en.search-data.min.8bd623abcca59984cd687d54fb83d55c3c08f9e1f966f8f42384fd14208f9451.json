[{"id":0,"href":"/docs/cloud-infrastructure/","title":"Cloud Infrastructure","section":"Docs","content":"Introduction #  Super dope content here!\n"},{"id":1,"href":"/docs/developer-ready-cloud/","title":"Developer Ready Cloud","section":"Docs","content":"Introduction #  Super dope content here!\n"},{"id":2,"href":"/docs/dr-migration/","title":"DR \u0026 Migration","section":"Docs","content":"Introduction #  Welcome! Here you can find suggestions on how to plan, set up and operate your DR and Migration services.\nVMware Cloud Director Availability #  VMware Cloud Director Availability is a Disaster-Recovery-as-a-Service solution. It is specially designed and available for participants in VMware Cloud Provider Program and allows them to protect and migrate vApps and VMs:\n From on-premises vCenter Server site to a VMware Cloud Director cloud From VMware Cloud Director cloud to an on-premises vCenter Server environment Between VMware Cloud Director managed clouds  Use Cases #  VMware Cloud Director Availability supports two different use cases – disaster recovery and migration of vApps/VMs. Both of them rely on replication of virtual machines. In both cases at least one of the sides is a VMware Cloud Director managed cloud and the other side could be another VMware Cloud Director cloud or vCenter on-premises site. VMware Cloud Director Availability cannot protect or migrate bare metal servers or VMs managed by non-VMware hypervisors.\nDisaster Recovery #  A tenant can purchase DRaaS provided by a public cloud and based on VMware Cloud Director Availability to protect their virtual machines running in their on-premises datacenter. The Cloud Provider assigns a portion of compute, storage, and network resources from their cloud and groups them in an Organization Virtual Data Center (OrgVDC) that belongs to the tenant. Also, the Cloud Provider can enable this OrgVDC (respectively, the tenant) to protect their on-premises virtual workloads to the cloud with specific parameters like minimum RPO, bandwidth control, direction of the protection, and more. It allows the tenant to failover their VMs in case of an outage in their on-premises data center.\nMigration #  When tenants plan to migrate workloads to the cloud, they may use the Migration workflow to simplify the process. When a “New Migration” is configured, VMware Cloud Director Availability starts replication of the vApp/VM from the source to the destination.\nAppliances #  There are three VMware Cloud Director Availability appliances required for the setup in the cloud and one on-premises appliance to be deployed at the tenant\u0026rsquo;s on-premises site.\nCloud Replication Management Appliance #  The Cloud Replication Management appliance is responsible for the communication with VMware Cloud Director. Based on this communication, it discovers resources (OrgVCD, storage policies, datastores, networks, etc.) managed by VMware Cloud Director and used by the tenants. This information is required for discovering vApps/VMs that can be replicated/migrated or suitable destination locations for incoming replications/migrations.\nIt also provides the VMware Cloud Director Availability UI and API interfaces. Another role of this appliance is to communicate with all the local and remote Replicators and receive data regarding each protected/migrated workload. Two VMware Cloud Director Availability services cover these functionalities in Cloud Replication Management appliance – the cloud.service and the manager.service.\nCloud Service #  This service understands the VMware Cloud Director constructs – OrgVCD, vApps, networks managed by VMware Cloud Director, storage policies, etc. To achieve this VMware Cloud Director Availability Cloud Management appliance communicates with the VMware Cloud Director API through the VMware Cloud Director LB. The Cloud Replication Management appliance does not communicate with the VMware Cloud Director consoleproxy cells/interfaces. Based on this, VMware Cloud Director Availability can:\n discover VMware Cloud Director managed vApps/VMs and protect/migrate them to another DR-enabled cloud or on-premises vCenter discover a suitable destination for incoming replications/migrations  The cloud service manages pairings with other DR-enabled clouds, policies, SLA profiles, and their assignment to VMware Cloud Director organizations. It provides information about the replication and system tasks. Also, it reports the replication compute resources consumption per tenant and per PVDC, the storage consumption per datastore, plus many other high- and low-level details. The cloud service management interface is available on https://vcda_manager_fqdn:443/admin. It is possible to log in with the local OS root account, an SSO account if the appliance has registration in an SSO domain, or with a VMware Cloud Director System Administrator account if the initial configuration is already completed.\nManager Service #  The manager service manages the registrations of local and remote replicators. During the pairing process, remote replicators are registered in the Cloud Replication Management appliance. For each replication, it chooses one replicator from the source site and one from the destination site. The destination replicator is responsible for discovering the appropriate resources to create the replica disks at the destination and write data. Replicators send information to the Cloud Replication Management appliance about their operation – statuses, amount of data replicated, operation start time, time to complete, etc. The manager service is also used to manage replicators. It can trigger the rebalancing of replications across all replicators or put a replicator in maintenance mode, which leads to assigning each of its replications to another one. This is useful when the current replicators need to be offloaded by adding a new replicator in the solution.\nThe manager service management interface is available at https://vcda_manager_fqdn:8441. It is possible to log in with the local OS root account and an SSO account if the appliance is already registered in Lookup Service.\nCloud Replicator #  Cloud Replicator is responsible for moving the replication data around - to and from the ESXi hosts and the cloud. For outgoing replications/migrations, it communicates with the VMKernel interface of the ESXi host, captures and encrypts replication data, optionally compresses it, and sends it to the remote replicator, which can be another Cloud Replicator or on-premises Replicator. For incoming replications/migrations the Cloud Replicator receives replication data from a Replicator (cloud or on-premises), decompresses and decrypts this data, and sends it to the ESXi to be written on a datastore. Cloud Replicator is the only component that can scale out as the number of protections/migrations increases.\nCloud Tunnel #  The Cloud Tunnel appliance is the single-entry point to the VMware Cloud Director Availability instance in the cloud. Its role is to handle and forward the incoming management and replication traffic. It is the only VMware Cloud Director Availability appliance that needs a dedicated Internet-accessible endpoint.\nOn-premises Appliance #  The VMware Cloud Director Availability on-premises appliance is deployed in the tenant data center. It creates a pairing relation to VMware Cloud Director Availability in the cloud and can protect and/or migrate VMs running locally. The on-premises appliance does not require a public endpoint as it only initiates connectivity to the cloud. A single on-premises appliance can protect VMs from a single SSO domain even if there are multiple vCenter Servers in this SSO domain. If the requirement is to do replications from the cloud to on-premises, then a single appliance is required for each vCenter due to how placement works. Also, one on-premises appliance can be paired to a single VMware Cloud Director Availability instance in the cloud. If the tenant intends to use more than one DR service in the cloud, a dedicated appliance is required for each cloud DR service.\nReference #  Official documentation\n"},{"id":3,"href":"/docs/networking-security/","title":"Networking \u0026 Security","section":"Docs","content":"Introduction #  Super dope content here!\n"},{"id":4,"href":"/docs/cloud-infrastructure/planning/","title":"Planning and Preparation","section":"Cloud Infrastructure","content":"Planning and Preparation #  Your content comes here!\n"},{"id":5,"href":"/docs/developer-ready-cloud/planning/","title":"Planning and Preparation","section":"Developer Ready Cloud","content":"Planning and Preparation #  Your content comes here!\n"},{"id":6,"href":"/docs/dr-migration/planning/","title":"Planning and Preparation","section":"DR \u0026 Migration","content":"Planning and Preparation #  Everything you need to know before deploying VMware Cloud Director Availability.\nHardware Requirements #     Appliance Type Description and Services Hardware Requirements     Cloud Replication Management Appliance A dedicated appliance, that runs the following VMware Cloud Director Availability services: * Manager Service * Cloud Service with embedded VMware Cloud Director Availability Tenant Portal - 2 vCPUs - 4 GB RAM\n- 10 GB Storage   Cloud Replicator Appliance A dedicated appliance for the Replicator Service that handles the replication traffic for a site. For large-scale environments, you can deploy more than one Cloud Replicator Appliance per cloud site. - 4 vCPUs - 6 GB RAM - 10 GB Storage   Cloud Tunnel Appliance A dedicated appliance for the Tunnel Service. - 2 vCPUs - 2 GB RAM\n- 10 GB Storage    Other Requirements #  The resource vCenter Server instances within a VMware Cloud Director site must be within the same single sign-on domain. All Replicator Service, Manager Service, Cloud Service, and Tunnel Service instances within the respective site must be configured with that same single sign-on domain.\nNetwork Ports #     Source Destination Port Number Protocol Description     VMware Cloud Director Availability Replicator ESXi Hosts 902 TCP and UDP Used by the VMware Cloud Director Availability Replicator service for replication traffic to the destination ESXi hosts.   VMware Cloud Director Availability Replicator VMware Platform Services Controller® 443 TCP Used for single sign-on and Lookup Service communication.   VMware Cloud Director Availability Replicator vCenter Server 443 TCP Used by the local VMware Cloud Director Availability vApp Replication Manager service or the VMware Cloud Director Availability Replicator service for communication with the local vCenter Server.   VMware Cloud Director Availability Replicator VMware Cloud Director Availability vApp Replication Manager 8044 TCP Used for vCloud Availability vApp Replication Manager management from the vCloud Availability Replicator.   VMware Cloud Director Availability Replicator VMware Cloud Director Availability Tunnel 8048 TCP Used for VMware Cloud Director Availability vApp Replication Manager management from the VMware Cloud Director Availability Replicator.   VMware Cloud Director Availability vApp Replication Manager VMware Platform Services Controller® 443 TCP Used for single sign-on and Lookup Service communication.   VMware Cloud Director Availability vApp Replication Manager VMware Cloud Director service 443 TCP Used for VMware Cloud Director Director management from the VMware Cloud Director Availability vApp Replication Manager.    VMware Cloud Director Availability vApp Replication Manager VMware Cloud Director Availability Replicator 8043 TCP Used for VMware Cloud Director Availability Replicator management from the VMware Cloud Director Availability vApp Replication Manager.    VMware Cloud Director Availability vApp Replication Manager VMware Cloud Director Availability Tunnel 8047, 8048 TCP Used for VMware Cloud Director Availability Tunnel management from the VMware Cloud Director Availability vApp Replication Manager.    VMware Cloud Director Availability Tunnel VMware Cloud Director Availability Replicator 8043, 44045 TCP Used for VMware Cloud Director Availability Replicator management from the VMware Cloud Director Availability Tunnel.    VMware Cloud Director Availability Tunnel VMware Cloud Director Availability vApp Replication Manager 8044, 8046 TCP Used for VMware Cloud Director Availability vApp Replication Manager service management from the VMware Cloud Director Availability Tunnel appliance.    VMware Cloud Director Availability Tunnel VMware Platform Services Controller® 443 TCP Used for VMware Platform Services Controller® communication management from the VMware Cloud Director Availability vApp Replication Manager and VMware Cloud Director Availability Replicator.    ESXi Hosts VMware Cloud Director Availability Replicator 31031, 44045, 44046 TCP Used by the ESXi hosts for replication traffic to the destination VMware Cloud Director Availability Replicator service.    Firewall VMware Cloud Director Availability Tunnel 8048 TCP Used for redirecting external traffic management to the VMware Cloud Director Availability Tunnel service.     Important Considerations #  Replication Traffic #  A typical good practice is to separate the management and resource vCenters/clusters. The number of hosts in resource vCenters/clusters is significantly higher than the number in the management cluster. A recommendation is to deploy Replicators on resource hosts and not in the management cluster so more Replicators can be deployed and a DRS rule can be created to keep the Replicator VMs on different hosts for better load distribution. Also, the replication traffic path from the Replicator appliances to the replication network on the resource hosts will be enhanced. The hosts can use the management vmkernel interface to communicate with the Replicator. Another option is to have a dedicated vmkernel interface for replication purposes only. Using the management vmkernel simplifies the configuration but significantly reduces the control options available to the administrator and can lead to a risk for routing uncompressed replication traffic, which is highly non-desired. The general recommendation is to use a dedicated vmkernel for the replication traffic. In this case, the administrator will have better control over the infrastructure. Using NIOC, the administrator will be able to set shares for different types of vmkernel traffic. It enables carrying the replication traffic over dedicated uplinks.\nNote: If any other VMware HBR-based replication products (vSphere Replication, VMware Site Recovery Manager or VMware HCX) are used in this cloud, configuring a dedicated vmkernel interface marked with “vSphere Replication” and “vSphere NFC Replication” will lead to all of these products try to use it. This means the network connectivity between their appliances and the replication vmkernel interfaces will be required to enable all of these products to operate successfully. For more information, please consult with respective product documentation.\nStorage Requirements #  For a successful test failover, the destination storage must accommodate double the source virtual machine disk size. VMware Cloud Director Availability 4.2 and later do not consume double the disk size during failover.\n Example required space in the datastore, for a source virtual machine with a 2 TB virtual disk. When the replication is created, VMware Cloud Director Availability allocates 2 TB in the destination storage. VMware Cloud Director Availability allocates additional 2 TB when starting a test failover task. After finishing the test failover task, the additional 2 TB space is unallocated. Example for a VMware vSAN storage, with the same virtual machine. The same storage implication applies, where the vSAN must accommodate double the virtual machine disk size. When the replication is created in this example, VMware Cloud Director Availability allocates 2 TB multiplied by the vSAN_Protection_Level_Disk_Space_Penalty. When starting a test failover task, additional 2 TB are allocated multiplied by the vSAN_Protection_Level_Disk_Space_Penalty.  "},{"id":7,"href":"/docs/networking-security/planning/","title":"Planning and Preparation","section":"Networking \u0026 Security","content":"Planning and Preparation #  Your content comes here!\n"},{"id":8,"href":"/docs/sovereign-cloud/planning/","title":"Planning and Preparation","section":"Sovereign Cloud","content":"Planning and Preparation #  Your content comes here!\n"},{"id":9,"href":"/docs/sovereign-cloud/","title":"Sovereign Cloud","section":"Docs","content":"Introduction #  Super dope content here!\n"},{"id":10,"href":"/docs/cloud-infrastructure/design/","title":"Design","section":"Cloud Infrastructure","content":"Design #  Your content comes here!\n"},{"id":11,"href":"/docs/developer-ready-cloud/design/","title":"Design","section":"Developer Ready Cloud","content":"Design #  Your content comes here!\n"},{"id":12,"href":"/docs/dr-migration/design/","title":"Design","section":"DR \u0026 Migration","content":"Design #  Architecture #  The following diagram shows the network flow between the VMware Cloud Director Availability components and the other infrastructure components part of the replication process.\nOther Components #  Platform Services Controller #  All the VMware Cloud Director Availability appliances need to be configured with a Lookup Service for two purposes:\n Service authentication and resource discovery – the Replicator and Management appliances require registration with a Lookup Service to authenticate to the resource vCenters and discover resources which is required for operations from the VMware Cloud Director Availability workflow. Authentication to the VMware Cloud Director Availability service by the administrator which is not mandatory. The Lookup Service has to be the one used by vCenters providing compute resources to VMware Cloud Director. For cases when the management and resource vCenters use different Lookup Services and the VMware Cloud Director Availability appliances are deployed at the management vCenter, they need to be configured with the Lookup Service of the resource vCenter(s) from/to which they will protect VMs/vApps.  The Lookup Service and the associated Single Sign-On (SSO) domain is the boundary of a single VMware Cloud Director Availability instance. This is because a single Cloud Replication Management appliance can be registered with a single Lookup Service providing DR and migration services only for workloads in this SSO domain. For cases when the cloud provider has multiple PVDCs in more than one SSO domain, a VMware Cloud Director Availability instance (tunnel, manager, replicators) needs to be deployed per each SSO domain. It is possible to deploy a dedicated VMware Cloud Director Availability instance per PVDC, but one can protect as many PVDCs as exist in a single SSO domain.\nvCenter Server #  VMware Cloud Director Availability needs to discover the workload VMs that need to be protected or migrated and destination resources (clusters, datastores, networks, etc.) for incoming replications and/or migrations in vCenters that are configured to provide the resources for VMware Cloud Director.\nThe Replicator is the component that communicates with the vCenter Servers. The tunnel has no such requirement.\nVMware Cloud Director #  The integration with VMware Cloud Director is done through the Cloud Replication Management appliance. The Cloud service communicates with VMware Cloud Director to discover the vApps/VMs for outgoing replications/migrations or destination resources (OrgVDC, network, storage) for incoming replications/migrations. VMware Cloud Director Availability uses the VMware Cloud Director API to request the information and initiate any operations.\nAs part of the initial setup, the administrator provides the VMware Cloud Director API HTTPS Base URL followed by /api and user account, which is a member of the System organization with its password. Then VMware Cloud Director Availability inspects the certificate presented by VMware Cloud Director. If the FQDN of the API HTTPS Base URL is not set as the Subject Alternative Name DNS entry, the registration cannot be completed. It is not enough to have this FQDN present in the certificate Common Name field. When a TLS connection to VMware Cloud Director is established and the authentication with the provided credentials is successful, VMware Cloud Director Availability uses the VMware Cloud Director REST API to discover its UI endpoint. Depending on its value, the VMware Cloud Director Availability HTTP server is dynamically reconfigured - the respective domain is added to the CORS allowlist.\nAdditional Information: CORS is an HTTP mechanism that provides better security to end-users. Depending on how the server is configured, browsers may reject attempts to load resources from a server that is on domain A, but someone tries to load it in domain B. The VMware Cloud Director Availability Cloud service is set to allow only one VMware Cloud Director endpoint - the VMware Cloud Director UI endpoint or, if it is not configured, the API endpoint. Once the certificate and credentials check are successful, VMware Cloud Director Availability deploys the files required for extension operation in the VMware Cloud Director cells.\nESXi hosts #  The Replicators communicate with hosts to:\n Configure the source VM for replication by enabling IO filter and configure it where to send replication data Allocate resources in the host for the destination replication  These operations are performed by the HBRsrv service. For authentication purposes, it needs to communicate to the hosts on port 80/tcp. For the data traffic, it uses 902/tcp to hosts to send the replication data (for incoming replications) and 44046/tcp from the hosts to receive replication data (for outgoing replications).\nData Paths #  Two appliances are responsible for the replication data path – the Replicator and the Tunnel. The Cloud Replication Management appliance does not handle any replication data.\nCloud Tunnel #  The Tunnel is a transparent proxy that is aware of the TLS protocol. It receives the incoming traffic and redirects it to the backend components based on its type. The management and HTTPS/API traffic is sent to the Cloud Replication Management, and the replication data traffic is sent to Replicator. It does not perform any operations on the replication traffic. The Tunnel appliance does not consume significant compute resources, but in environments with many replications may handle a significant amount of network traffic. As it is published to the Internet via DNAT, it is recommended to connect it to a DMZ network where all the necessary security measures for such publish rules are taken.\nCloud Replicator #  The Replicator appliance runs three services – HBRsrv, LWDproxy, and Replicator.\nThe HBRsrv is a low-level component that communicates with the replication IO filter in ESXi hosts. The protocol used for communication is called LWD (Light Weight Delta) - a VMware proprietary protocol. This service is not aware of vCenter and does not communicate with it. The HBRsrv service is responsible for saving the VM deltas generated during the RPO window on the destination datastore. It is a common service for other VMware replication products, and VMware Cloud Director Availability reuses it for moving replication data at the last mile. It configures the ESXi filter to send the replication traffic between the ESXi host and source Replicator in \u0026ldquo;plain text\u0026rdquo;. When the data is received by the source LWDproxy, it gets encrypted and optionally compressed. On the destination site, there\u0026rsquo;s another LWDproxy that decrypts and decompresses the LWD traffic, and the destination Replicator sends the replication data to an ESXi host.\nThe LWDproxy is a VMware Cloud Director Availability service that ensures that replication traffic is sent to the Tunnel and is encrypted and compressed outside the local infrastructure. The LWDProxy service communicates with the Tunnel for sending/receiving replication data. Encryption and compression result in increased CPU utilization for the Replicator appliance. This may reduce the number of replications a single Replicator can handle.\nThe Replicator service is a VMware Cloud Director Availability management service that sends configuration instructions to the HBRsrv and LWDproxy services for every replication handled by the Replicator appliance. It understands the vCenter objects like hosts, VMs, datastores, etc. In this way, the Replicator discovers and prepares the source VMs for replication and creates objects (independent disks) for the incoming replications. It is supposed there will be hundreds or thousands of VM replications/migrations in a cloud environment. Even if the number of replications is lower than the maximum supported handled by a single Replicator, it is highly recommended to deploy at least two or more replicators. With several Replicators, the load generated by the replications can be spread over multiple hosts. Balance the replication traffic on more physical uplinks and distribute the load better. A single Replicator can be put in maintenance mode during a maintenance window without affecting replication traffic.\n"},{"id":13,"href":"/docs/networking-security/design/","title":"Design","section":"Networking \u0026 Security","content":"Design #  Your content comes here!\nRelative\nAbsolute\n"},{"id":14,"href":"/docs/sovereign-cloud/design/","title":"Design","section":"Sovereign Cloud","content":"Design #  Your content comes here!\n"},{"id":15,"href":"/docs/cloud-infrastructure/implementation/","title":"Implementation","section":"Cloud Infrastructure","content":"Implementation #  Welcome!\nConfiguring VMware Cloud Director service for SDDC Endpoints #  The following sections describes how to configure various SDDC endpoints for VMware Cloud Director service.\nConfiguring VMware Cloud Director service for VMware Cloud on AWS #  See the instructions below on how to deploy VMware Cloud on AWS (VMConAWS) for multi-tenancy with VMware Cloud Director service.\nVMware Cloud Director Service for VMware Cloud on AWS Deployment Guide\nConfiguring VMware Cloud Director service for Google Cloud VMware Engine #  See the instructions below on how to deploy Google Cloud VMware Engine for multi-tenancy with VMware Cloud Director service.\nVMware Cloud Director Service for Google Cloud VMware Engine Deployment Guide\n"},{"id":16,"href":"/docs/developer-ready-cloud/implementation/","title":"Implementation","section":"Developer Ready Cloud","content":"Implementation #  You content comes here!\n"},{"id":17,"href":"/docs/dr-migration/implementation/","title":"Implementation","section":"DR \u0026 Migration","content":"Implementation #  Welcome!\n"},{"id":18,"href":"/docs/networking-security/implementation/","title":"Implementation","section":"Networking \u0026 Security","content":"Implementation #  You content comes here!\n"},{"id":19,"href":"/docs/sovereign-cloud/implementation/","title":"Implementation","section":"Sovereign Cloud","content":"Implementation #  You content comes here!\n"},{"id":20,"href":"/docs/cloud-infrastructure/operations/","title":"Operations","section":"Cloud Infrastructure","content":"Operations #  vCenter Server Metering #  Usage Meter collects product usage information from all vCenter servers that are registered with Usage Meter.\nNote: Usage Meter does not traverse Enhanced Linked Mode, and it requires registering all vCenter servers, part of this mode, to meter their usage data.\nVMware Cloud Provider Program partners should utilize VCPP licenses for all servers used in the service delivery path or administration control plane. VMware perpetual licenses, including OEM versions, can only be utilized to support internal IT operations that are not part of service delivery. In addition, VMware perpetual licenses may not be used to support the management or operations of an environment utilized to host unaffiliated third parties. Virtual machines running on hosts with perpetual or demo license keys are metered by Usage Meter.\nConfiguration #  The Usage Meter administrator must configure the endpoint and credentials for each vCenter server to be metered using the Usage Meter web application.\nFeature Detection #  When metering vCenter, Usage Meter performs two types of usage collections: event-based and inventory collections.\nEvent history collections #  Usage meter subscribes to vCenter events to detect changes in a virtual machine and host state. Changes in a virtual machine state are delivered every hour to Usage Meter and recorded.\nNote: Usage Meter collects data from hosts and VMs in an active state. Even when there is a connectivity issue with a host, Usage Meter will still report metering data if the host VMs are in a powered-on state. This means that providers will be billed for VMs even when there are temporary interruptions in host connectivity. If providers want to be sure they are not being billed for VMs, they need to turn those VMs off.\nInventory collections #  Usage Meter queries vCenter servers every eight hours to collect full inventory information.\nvCenter Reporting #  The vCenter Server Standard edition is part of the Flex Core bundle and therefore reported as part of that bundle. vCenter usage is bundled by default and cannot be reported standalone.\n"},{"id":21,"href":"/docs/developer-ready-cloud/operations/","title":"Operations","section":"Developer Ready Cloud","content":"Operations #  API Tokens in Cloud Director #  VMware Cloud Director 10.3.1 introduced API Tokens. This allows a user to generate API tokens for programmatic access to VCD. It works for both, provider, and tenant users. An automation script or 3rd-party solution can then use the API token to make API requests to Cloud Director on behalf of the user.\nThese steps are used to create API tokens:\n The provider propagates the right to use and manage API token to the tenant The Cloud Director user (provider as well as tenant user) creates an API token An API client (e.g. an automation script) uses the API token to make requests (If needed) The user revokes the API token  Preparation #  As for most new features, fine-grained access control through rights bundles is possible. To enable a tenant to use API token, the provider must publish a rights bundle to the tenant. Privileges can be defined for a user to manage the user’s own tokens, and to manage all Organization user’s token (for example for an Organization Administrator).\nCreate the API Token #  As provider or tenant user with proper privileges you can use the “User Preferences” menu to create the API token. Each token can be labeled with a name. Be aware that the actual token key is only visible once in the creation wizard and cannot be retrieved afterwards.\nRevoke the API Token #  API tokens do not expire, but existing API tokens can be revoked. This also invalidates active API client sessions that used the token to authenticate. All users can revoke their own tokens. Administrators (those with \u0026lsquo;manage all user\u0026rsquo;s API tokens\u0026rsquo; right) can revoke other user\u0026rsquo;s tokens. Tenant administrators can do so within their own Organization, while system administrators can do so for any user.\nUse the API Token #  Semantically the API token usage follows the OAuth 2.0 specification (RFC 6749, section 6).\nThe API token can then be used by a 3rd-party solution or custom API client to access the VCD API as the user, without the need to authenticate with username and password credentials.\nAPI Client Example #  Request the bearer token for subsequent calls using the API token:\nPOST https://host_name/oauth/provider/token Accept: application/json Content-Type: application/x-www-form-urlencoded Body: grant\\_type=refresh\\_token\u0026amp;refresh\\_token=Generated\\_API_Token  Security note: It’s recommended to send the API Token as part of the request body (and not as part of the URL, even if that technically works for x-www-form-urlencoded type requests), to avoid it being logged in transit.\nResponse containing the Bearer token:\nHTTP/1.1 200 OK Content-Type: application/json Body: { \u0026quot;access\\_token\u0026quot;:\u0026quot;Generated\\_Access_Token\u0026quot;, \u0026quot;token_type\u0026quot;:\u0026quot;Bearer\u0026quot;, \u0026quot;expires_in\u0026quot;:86400, \u0026quot;refresh_token\u0026quot;:null }  Subsequent API call now can use the returned Bearer token in the \u0026ldquo;access_token\u0026rdquo; field as usual. There is no need for any changes in the client code.\nSubsequent Call using the Bearer token:\nGET https://host_name/api/org Accept: application/*+xml;version=36.1 Authorization: Bearer Generated\\_Access\\_Token  Notes #  The session expiration can be configured in the provider portal under General \u0026gt; Timeouts.\nThe VCD Provider for Terraform for example supports API Token authentication as of version 3.5:\nprovider \u0026quot;vcd\u0026quot; { user = \u0026quot;none\u0026quot; password = \u0026quot;none\u0026quot; auth\\_type = \u0026quot;api\\_token\u0026quot; api_token = Generated API token sysorg = \u0026quot;System\u0026quot; ...  For security reasons, certain tasks are not possible when authenticated through an API token:\n Change the user password Perform user management tasks Create more tokens View or revoke other tokens  When accessing VMware Cloud Director by using an API access token, applications have only view rights for the following resources.\n User Group Roles Global roles Rights bundles  The API Token feature in VMware Cloud Director offers a secure way for automation solutions to access its API, even in environments that enforce Multi-factor Authentication for user logins.\nReference links: #  VCD Tenant Portal Documentation: https://docs.vmware.com/en/VMware-Cloud-Director/10.3/VMware-Cloud-Director-Tenant-Portal-Guide/GUID-A1B3B2FA-7B2C-4EE1-9D1B-188BE703EEDE.html\nVCD Provider Portal Documentation: https://docs.vmware.com/en/VMware-Cloud-Director/10.3/VMware-Cloud-Director-Service-Provider-Admin-Portal-Guide/GUID-A1B3B2FA-7B2C-4EE1-9D1B-188BE703EEDE.html\nOAuth 2.0 Specification (RFC 6749): https://datatracker.ietf.org/doc/html/rfc6749#section-6\nVCD Provider for Terraform 3.5.1 Documentation: https://registry.terraform.io/providers/vmware/vcd/latest/docs\n"},{"id":22,"href":"/docs/dr-migration/operations/","title":"Operations","section":"DR \u0026 Migration","content":"Operations #  Concent comes here \\o/\n"},{"id":23,"href":"/docs/networking-security/operations/","title":"Operations","section":"Networking \u0026 Security","content":"Operations #  Your content comes here!\n"},{"id":24,"href":"/docs/sovereign-cloud/operations/","title":"Operations","section":"Sovereign Cloud","content":"Operations #  Your content comes here!\n"},{"id":25,"href":"/docs/cloud-infrastructure/cds-gcve-deployment-guide-msp/","title":"VMware Cloud Director Service for Google Cloud VMware Engine Deployment Guide","section":"Cloud Infrastructure","content":"Introduction #  This white paper is intended for VCPP Cloud Providers who need guidance on how to configure Google Cloud VMware Engine with VMware Cloud Director service (CDs). The content below describes the manual deployment process required to setup the Google Cloud projects, configure them, deploy a Software Defined Data Center (SDDC), associate it to CDs, and the process to configure a virtual private network (VPN) solution for connectivity to isolated networks.\nA virtual private network (VPN) provides traffic tunneling through an encrypted connection, preventing it from being seen or modified in transit. VMware NSX® Data Center for vSphere includes a remote access VPN feature (IPsec) that allows the connection from a remote site to connect securely to the private networks and applications in the organization virtual data center.\nDue to a limitation in VPN technology in Google Cloud, the provider will need to select an alternative solution such as an open source or commercially available, depending on the required features and available support. Examples of open source solutions include OpenVPN or StrongSwan. This deployment guide will walk through the steps required to implement the StrongSwan solution to provide the VPN connectivity between the provider-managed customer project and the T1 edge that sits in front of the tenant networks. The VPN solution is deployed and configured manually in addition to being managed separately from CDs.\nDisclamer\nVMware does not endorse, recommend, or support any third-party utility or solution.\nA general knowledge of networking and security, as well as on VMware Cloud Director concepts is also required.\nConfigure Provider Project and Create Required Resources #  The following section describes the steps required to setup the cloud provider project. Before proceeding, the projects should be created, and the account used to configure them have the appropriate permission to configure all aspects of the projects.\nSetup the Provider Project #  The following steps describes the steps required to configure the provider project.\n Log into the Google Cloud console and click \u0026ldquo;Select a project\u0026rdquo; dropdown.     On the \u0026ldquo;Select a project\u0026rdquo; box, click the link for the name of the provider project.     In the top left pane, click the three horizontal bars and navigate down to Network -\u0026gt; Network Services -\u0026gt; and click Cloud DNS.      If prompted to enable the API first, click Enable API.\n  In the top left pane, click the three horizontal bars and navigate down to Compute -\u0026gt; and select VMware Engine. Note that this will open a second browser tab, keep both tabs open for easier navigating when configuring later.\n     Click Enable API.     In the top left pane click the three horizontal bars and navigate to Networking -\u0026gt; VPC Network -\u0026gt; and select VPC Networks.     Click CREATE VPC NETWORK.      Enter in the following:\n  Name: Subnet name such as the region the environment is in such as \u0026ldquo;asia-southeast1\u0026rdquo;\n  Region: Select the region the environment is in to host the subnet such as \u0026ldquo;asia-southeast1\u0026rdquo;\n  IP Address Range: Provide a range such as 100.64.0.0/20\n  Check the box for \u0026ldquo;I confirm that my subnet configuration includes a range outside of the RFC 1918 address space\u0026rdquo;\n  Private Google Access: Select the ON radial button\n        Under Dynamic Routing Mode:\n  Select Global\n  Set the Maximum MTU to 1500\n  Click Create\n       Once the task has completed, scroll to the bottom of the page, and click the name of the provider management network that was just created.     In the VPC network details screen, click the PRIVATE SERVICE CONNECTION tab and click the Enable API Button.     Once enabled, click ALLOCATE IP RANGE.      For Allocate an internal IP range enter:\n  Name: service-network\n  IP Range: The next subnet available in the provider range previously entered, 100.64.16.0/24 in this case\n  Click ALLOCATE\n       Click the PRIVATE CONNECTIONS TO SERVICES tab and click CREATE CONNECTION.     On the private connection screen, ensure Google Cloud Platform is selected and select the service-network that was created under Assigned Allocation and click Connect.    Create and Configure the Software Defined Data Center #  The following section describes the steps required to setup the Software Defined Data Center (SDDC) that tenants will consume for resources.\n Go to the browser tab that has GCVE open and click Create a Private Cloud.      Enter the following information:\n  Private Cloud Name: Name of the SDDC to create\n  Location: The GCP data center to create the SDDC\n  Node Type: Multi Node for production deployments\n  Node Count: Number of nodes to initially use for the SDDC (4 minimum)\n  vSphere/vSAN Range: IP range to use for vSphere and vSAN\n  HCX Deployment Range: IP range to use for HCX (while the input is required, using HCX is not required)\n  Click Review and Create when ready\n     Click Create\n  The SDDC creation process takes around one hour to complete.\n    Once the SDDC completion process works, the next steps are completed most easily by have two browser tabs open: one for the GCVE environment and one on the provider project settings.\n  On the browser tab with GCVE, in the left pane click Network and in the right pane click Add Private Connection.\n      Enter the following information:\n  Service: VPC Network\n  Region: The region the SDDC was created in\n  Peer Project ID: this will be the provider project name, which can be found in the browser tab with the provider project by clicking Google Cloud Platform and then copying the Project Name field and pasting it into the Peer Project ID field\n       Peer Project Number: This is the project ID, which can be found in the tab where the Project Name filed is immediately under that; copy and paste the value into the Peer Project Number field      Peer VPC ID: This will be \u0026ldquo;provider-mgmt-network\u0026rdquo; unless it was named differently. If it was name differently, enter the name used.\n  Tenant Project ID: To get the tenant project ID value, on the browser tab with the provider project, in the left pane click VPC Network -\u0026gt; VPC Network Peering and in the right pane, copy the value for Peered Project ID and paste it into this field.\n     Verify the Routing Mode is set to Global and click Submit.     After about 5 minutes, the Region Status should show Connected.     Navigate back to VPC Network -\u0026gt; VPC Networks -\u0026gt; and click on the provider-mgmt-network.     Click on the VPC NETWORK PEERING tab and click into the servicenetworking name.     Click on EDIT.     Under Exchange Custom Routes, check the boxes for Import Custom Routes and Export Custom Routes and click SAVE.     After saving, the route Status should all change to a Status of accepted.     On the GCVE browser tab, click Network in the left pane and then in the right pane click the REGIONAL SETTINGS tab and click Add Region.     On the Add Region screen, select the Region the SDDC is in, enable Internet Access and Public IP Service, and use the next provider CIDR block for the Edge Services CIDR and click Submit. After a few minutes, it should show that the status is Operational and Enabled.    Setup the Tenant Projects #  The following steps describes the steps required to configure the tenant projects. Many of the steps are the same done in the provider tenant in creating a tenant service network and peering projects.\n In the Google Cloud Portal, click the project name dropdown beside Google Cloud Platform.     On the Select a project screen, click the ALL tab and select the tenant project.     In the top left pane click the three horizontal bars and navigate to VPC Network -\u0026gt; VPC Networks -\u0026gt; and click the Default network name.     Click DELETE VPC NETWORK.      Click DELETE when prompted to confirm\n  Click CREATE VPC NETWORK.\n      Enter the following:\n  Name: Name for the network such as \u0026ldquo;tenantname-transit\u0026rdquo;\n  New Subnet Name: Name to match the region such as asia-southeast1\n  Region: The region the SDDC is in\n  IP Address Range: A CIDR range out of the next available range. Check the box for \u0026ldquo;I confirm the subnet configuration includes a range outside the RFC 1918 address space\u0026rdquo;\n  Private Google Access: Turn On\n  Click Done on the Subnet Section\n  Dynamic Routing Mode: Global\n  MTU: 1500\n  Click CREATE\n    Once the network creation has completed, click into the network name.\n     Click the PRIVATE SERVICE CONNECTION tab and then click ENABLE API.     Once completed, click ALLOCATE IP RANGE.      Set the following:\n  Name: service-network\n  IP Range Custom: Next available CIDR range\n  Click Allocate\n       Click the PRIVATE CONNECTIONS TO SERVICES tab and then click CREATE CONNECTION.     On the private connection screen, ensure Google Cloud Platform is selected and for Assigned Allocation, select the service-network that was just created and click CONNECT.     In the left pane click on VPC Network Peering and in the right pane click into the servicenetworking name.     Click EDIT.     Under Exchange Custom Routes, check the boxes for Import Custom Routes and Export Custom Routes and click SAVE.     Navigate to the GCVE page and in the left pane click Network and in the right pane click Add Private Connection.      Enter the following information:\n  Service: VPC Network\n  Region: Region the SDDC is in\n  Peer Project ID: this will be the provider project name, which can be found in the browser tab with the provider project by clicking Google Cloud Platform and then copying the Project Name field and pasting it into the Peer Project ID field\n       Peer Project Number: This is the project ID, which can be found in the tab where the Project Name filed is immediately under that; copy and paste the value into the Peer Project Number field      Peer VPC ID: This will be \u0026ldquo;tenantname-transit\u0026rdquo; unless it was named differently. If it was name differently, enter the name used.\n  Tenant Project ID: To get the tenant project ID value, on the browser tab with the provider project, in the left pane click VPC Network -\u0026gt; VPC Network Peering and in the right pane, copy the value for Peered Project ID and paste it into this field.\n     Verify the Routing Mode is set to Global and click Submit.      After about 5 minutes the Region status should show Connected.\n  Repeat the previous steps for configure each tenant project.\n  Create a Jumphost in the Provider Project and Allow Network Access #  The following section describes the steps required to create a jumphost in the provider project to use for vCenter and NSX access as well as other potential tasks made easier with local access.\n In a browser navigate to the provider project as previously described and in the top right pane click the three horizontal bars and select Compute Engine -\u0026gt; VM Instances.     Click CREATE INSTANCE.      Enter the following information:\n  Name: A name for the VM to help the region and function\n  Region: Same region as the SDDC\n  Zone: Zone in the region of the SDDC\n       Scroll down to the Boot Disk section and click CHANGE      Change the following:\n  Operating System: Windows Server\n  Version: Select a current version, such as 2019 Datacenter\n  Size: Change the default size if desired\n  Click SELECT\n       Scroll down and expand NETWORKING, DISKS, SECURITY, MANGEMENT, SOLE-TENANCY     Under \u0026ldquo;Edit network interface\u0026rdquo;, click the Network drop down and select the provider-mgmt-network that was previously created and then click DONE\u0026lt; then click CREATE     After several minutes the jumphost should show ready; click on the name of it to open the settings.     Click SET WINDOWS PASSWORD     On the pop-up screen, it will fill in the Username of the person logged in, click SET to set the password.     After a few moments it will display the password that is set. Copy the password and then click CLOSE.     Allow access through the firewall by clicking the three horizontal bars in the top left and select VPC Network -\u0026gt; Firewall.     Click CREATE FIREWALL RULE.      Set the following for the firewall rule:\n  Name: Name stating the service provided such as \u0026ldquo;rdp-in\u0026rdquo;\n  Network: The provider-mgmt-network that was created previously\n  Direction of Traffic: Ingress\n  Targets: All instances in the network\n  Source IPv4 ranges: 0.0.0.0/0\n  Protocols and ports: TCP 3389\n  Click CREATE\n    Click CREATE FIREWALL RULE again\n  Set the following for the firewall rule:\n  Name: This will be for east-west connectivity, so name it to identify what it is such as \u0026ldquo;ew\u0026rdquo; or \u0026ldquo;east-west\u0026rdquo;\n  Network: The provider-mgmt-network that was created previously\n  Direction of Traffic: Egress\n  Targets: All instances in the network\n  Source IPv4 ranges: The range of the management network, such as 100.64.0.0/16\n  Protocols and ports: Allow all\n  Click CREATE\n    On the settings of the jumphost that was created, copy the External IP to use in RDP.\n     Log into the jumphost with the external IP and the credentials previously created to verify access.  Prepare for and Deploy the Reverse Proxy #  The following section describes the steps required to prepare the environment for the proxy, generate it, and then deploy and associate the proxy to the SDDC that was created.\nCreate the CDs Instance (If not already created) #  The following steps describes the steps required to create a CDs instance if one does not already exist that the SDDC should be associated to.\n In Partner Navigator, navigate to VMware Cloud Director service and click CREATE INSTANCE.     Enter the required information and click CREATE INSTANCE.   The CDs instance will take around 30 minutes to complete.  Generate the Proxy #  The following steps describes the steps required to generate the proxy that will be used for the connection from CDs to the SDDC. These steps are completed from the partner navigator portal and require the CDs instance to already exist.\n  On the GCP provider based jumphost, log into Partner Navigator, navigate to VMware Cloud Director service.\n  On the CDs instance to associate the SDDC to click Actions and select Generate VMware Reverse Proxy OVF.\n      Enter the following:\n  API Token: API token from your account in Partner Navigator\n  Datacenter Name: Datacenter\n  vCenter FQDN: The FQDN of the VCSA appliance under Google Cloud VMware Engine -\u0026gt; Resources -\u0026gt; VSHPERE MANAGEMENT NETWORK\n       Management IP for vCenter: The IP address of the VCSA appliance     NSX URL: URL of the NSX manager     Additional hosts within the SDDC to proxy: The IP address of the ESXi hosts that are part of the SDDC. Note that each IP MUST be on a separate line     Once the information has been entered, click GENERATE VMWARE REVERSE PROXY OVF      The Activity Log on the CDs instance can be monitored for the status of the task. Skip ahead to Prepare the Environment for the Proxy if desired to complete those steps while waiting for the proxy to generate.\n  Once the task has completed, click the three horizontal dots, and select View Files.\n     Click the down arrow icon to download the OVF file to the provider jumphost locally.    Prepare the Environment for the Proxy #  Before deploying the proxy, a network segment must be created and DHCP setup so that it can get an IP address. The following steps describe how to create the segment and configure DHCP.\n On the GCVE page in the Google Cloud Portal, click Resource in the left pane and click the name of the SDDC.     Click on VSPHERE MANAGEMENT NETWORK tab and in the right click the NSX Manager FQDN and copy the link location.      RDP to the jumphost that was created on the provider network previously, open a browser and navigate to the FQDN of the NSX manager from the link that was copied.\n  Back on the GCVE browser, click the SUMMARY tab and then in the NSX-T Login Info section, click View.\n     In the Password section, click Copy.      Log into the NSX-T manager URL as admin with the password that was coped.\n  In the NSX-T manager UI, click the Networking tab in the left pane click DHCP.\n     Click ADD DHCP PROFILE.      Enter the following:\n  Profile Name: A name for the DHCP profile\n  Profile Type: DHCP Server\n  Server IP Address: A CIDR subnet for the scope\n  Edge Cluster: The edge cluster that was created\n  Click SAVE\n       In the left pane click Tier-1 Gateways and in the right pane beside Tier1, click the three vertical dots and select Edit.     Click Set DHCP Configuration.     For Type select DHCP Server and for DHCP Server Profile select the DHCP Profile that was created and click SAVE.     Click SAVE.     In the left pane click Segments, then click ADD SEGMENT.      Enter the following:\n  Segment Name: Proxy\n  Connected Gateway: Tier1 | Tier1\n  Transport Zone: TZ-Overlay\n  Subnet: The subnet CIDR\n  Click EDIT DHCP CONFIG\n       Check the DHCP Config enabled, set a DHCP range, enter the DNS servers from GCVE, and then click APPLY (the DNS servers can be found on the GCVE page, clicking Resources in the left tab and then clicking the name of the SDDC, and on the SUMMARY tab under Private Cloud DNS Servers.    Deploy and Connect the Proxy #  The following steps describes the steps required to deploy the proxy and associate it to the CDs instance in Partner Navigator.\n  On the GCP provider based jumphost, open a browser to the vCenter UI.\n  Right click Cluster and select Deploy OVF Template.\n     Select Local File and navigate to the reverse proxy OVF that was downloaded, select it and click NEXT.     Provide a virtual machine name and click NEXT.     Select the Cluster name and click NEXT.      Click Next.\n  Select the vsanDatastore and click NEXT.\n      Select the Proxy network that was previously created and click NEXT.\n  On the Customize Template page, copy and save the root password and click NEXT.\n  Click FINISH to being the deployment.\n  After deployment, power on the appliance.\n  Log into the proxy appliance and verify it has an IP address by running \u0026ldquo;ip a\u0026rdquo;.\n  Run the command \u0026ldquo;systemctl status transporter-client.service\u0026rdquo; and ensure it shows running.\n     If the transporter-client.service is showing an error, verify that DNS resolution is working properly and that it can access the Internet. The below screenshot shows an error when DNS is not working.     Run the command \u0026ldquo;transporter-status.sh\u0026rdquo; and verify it shows connected.     In Partner Navigator, go to the CDs instance the proxy was generated from and click Actions -\u0026gt; Associate a Datacenter via VMware Proxy.      Enter the following:\n  Datacenter name: Datacenter\n  vCenter FQDN: VCSA FQDN that was used to generate the proxy\n  NSX URL: URL of NSX manager that was used to generate the proxy\n  It will attempt an initial connection to the proxy and if it connects, it will display Connection Established\n        On the Credentials page, enter the following:\n  vCenter Username: cloudowner@gve.local\n  vCenter Password: The password for the supplied username\n  Disconnected Network Segment: Enter the name of the network the proxy is on (Proxy)\n  Authentication to NSX Type: Authenticate via NSX Username and Password\n  NSX Username: admin\n  NSX Password: The password for NSX admin account\n       Check the box to acknowledge charges will begin and click SUBMIT.     The Activity Log on the CDs instance can be monitored for the status of the association task.      It should take about 5 to 10 minutes for the task to completed.\n  Once the task has finished, it can take up to 4 hours to show up as an associated SDDC in the CDs instance. Opening the VCD instance to bring up the UI should show the SDDC as a PVCD that can be used to create VDCs for tenants; you do not have to wait for it to show up as associated in the Partner Navigator portal.\n  Deploy and Configure IPsec Tunnel #  The following section describes the steps required to deploy and configure a StrongSwan VPN appliance in the tenant\u0026rsquo;s project to connect to their T1 in the SDDC that was deployed via an IPsec tunnel. This is merely a demonstration of how to deploy a VPN appliance and any suitable appliance can be used.\nThe steps below are based on CentOS 7 as the operating system; using another flavor of Linux may result in different steps or actions required to get it to work properly.\nThe default routes to the Internet will use instance tags to keep from the routes leaking back into the GCVE environment. This tag can be whatever the provider desires, but it must be uniform across all routes that point to the Internet and be applied to any VM that will need to be access to/from to the Internet in the provider owned customer project.\nDeploy a Linux Instance and Configure StrongSwan #  The following steps describes the steps required to deploy a virtual machine, install StrongSwan and configure it for an IPSec tunnel connection to a tenant T1.\n First create a firewall rule in the tenant project by going to VPC Network -\u0026gt; Firewall and click CREATE FIREWALL RULE.      Enter the following:\n  Name: gcve-transit\n  Network: tenatname-transit\n  Priority: 100\n  Direction of Traffic: Ingress\n  Action on match: Allow\n  Targets: All instances in the network\n  Source IPv4 Ranges: Range for transit network \u0026ndash; such as 100.64.0.0/16\n  Protocols and ports: Allow all\n  Click Save\n    Click CREATE FIREWALL RULE again and enter the following:\n  Name: ipsec-egress\n  Network: tenatname-transit\n  Priority: 100\n  Direction of Traffic: Egress\n  Action on match: Allow\n  Targets: All instances in the network\n  Destination IPv4 Ranges: Range for transit network \u0026ndash; such as 100.64.0.0/16\n  Protocols and ports: IPsec Ports\n  Click Save\n    Create a new instance in the tenant and set:\n  Boot Disk: Change to CentOS 7\n  Expand NETWORKING, DISKS, SECRUITY, MANGEMENT, SOLE-TENANCY\n  IP Forwarding: Check the box to Enable\n  Ensure the network interface is on the \u0026ldquo;tenantname-transit\u0026rdquo; network\n  Click CREATE\n      Once completed, click on the name of the instance to bring up its settings.\n     At the top of the screen, click EDIT.\n     Scroll down and in the Network Tags box, put the network tag name, then save the settings.\n     Under the Connect column click SSH to connect to it.\n  Run sudo su and then run \u0026ldquo;yum install strongswan -y\u0026rdquo; to install strongswan.\n  Copy the command below and paste into the shell with ctrl v\n   cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; EOF\nnet.ipv4.ip_forward = 1\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.all.send_redirects = 0\nEOF\n   Run the command sysctl -p\n  CD to /etc/strongswan/swanctl/conf.d and run vi nsxt.conf.\n Enter the following information in the nsxt.conf file replacing localAddr with the local IP of the tunnel, remoteAddr with the remote IP of the tunnel, remoteTS with the network CIDR of the remote end of the tunnel, and PresharedKey with the secret used for the tunnel.     connections {\ngw-gw {\nlocal_addrs = localAddr\nremote_addrs = remoteAddr\nlocal {\nauth = psk\nid = localAddr\n}\nremote {\nauth = psk\nid = remoteAddr\n}\nchildren {\nnet-net {\nlocal_ts = 0.0.0.0/0\nremote_ts = remoteTS\nstart_action = start\nupdown = /usr/local/libexec/ipsec/_updown iptables\nesp_proposals = aes128gcm128-modp2048\n}\n}\nversion = 2\nproposals = aes128-sha256-modp2048\n}\n}\nsecrets {\nike-1 {\nid-1 = localAddr\nsecret = PresharedKey\n}\nike-2 {\nid-2 = remoteAddr\nsecret = PresharedKey\n}\nike-3 {\nid-3a = localAddr\nid-3b = remoteAddr\nsecret = PresharedKey\n}\nike-4 {\nsecret = PresharedKey\n}\nike-5 {\nid-5 = localAddr\nsecret = PresharedKey\n}\n}\n   Run the command swanctl --load-all\n  CD to /etc and run \u0026ldquo;vi ipsec.secrets\u0026rdquo;\n Enter the following line, replacing the words with their values: localTunnelIP remoteTunnelIP : PSK \u0026lsquo;PresharedKey\u0026quot; and save the file.        Run the command sudo strongswan restart\n  Run the command yum install iptables-services and once installed run systemctl start iptables\n  Add the following iptables rules in to allow traffic to be forwarded. Any line that contains remoteNet should have that replaced with the CIDR of the remote network in GCVE. Note that each line must be copied and pasted into the SSH session on the VPN server one by one.\n   iptables -A INPUT -i eth0 -p esp -j ACCEPT\niptables -A INPUT -i eth0 -p ah -j ACCEPT\niptables -A INPUT -i eth0 -p udp -m udp --sport 500 --dport 500 -j ACCEPT\niptables -A INPUT -i eth0 -p udp -m udp --sport 4500 --dport 4500 -j ACCEPT\niptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT\niptables -A FORWARD -s remoteNet -d 0.0.0.0/0 -i eth0 -m policy --dir in --pol ipsec --reqid 1 --proto esp -j ACCEPT\niptables -A FORWARD -s 0.0.0.0/0 -d remoteNet -o eth0 -m policy --dir out --pol ipsec --reqid 1 --proto esp -j ACCEPT\niptables -A OUTPUT -o eth0 -p esp -j ACCEPT\niptables -A OUTPUT -o eth0 -p ah -j ACCEPT\niptables -A OUTPUT -o eth0 -p udp -m udp --sport 500 --dport 500 -j ACCEPT\niptables -A OUTPUT -o eth0 -p udp -m udp --sport 4500 --dport 4500 -j ACCEPT\niptables -A OUTPUT -p tcp -m tcp --sport 22 -j ACCEPT\niptables -A FORWARD -s 0.0.0.0/0 -d remoteNet -i eth0 -m policy --dir in --pol ipsec --reqid 1 --proto esp -j ACCEPT\niptables -A FORWARD -s remoteNet -d 0.0.0.0/0 -o eth0 -m policy --dir out --pol ipsec --reqid 1 --proto esp -j ACCEPT\n  Delete the two listed REJECT rules by running \u0026ldquo;iptables -D SECTION_NAME position#\u0026rdquo;. For example, in the screen shot below, the REJECT under INPUT is the 5^th^ rule down, so the command to delete it is \u0026ldquo;iptables -D INPUT 5\u0026rdquo;. Notice after running the command the REJECT rule under INPUT is no longer present.      To delete the REJECT under FORWARD, run \u0026ldquo;iptables -D FORWARD 1\u0026rdquo; as it is in position 1.\n  Run the command service iptables save\n  Run systemctl restart iptables\n  In the GCP console in the tenant project navigate to VPC Network -\u0026gt; Routes and click CREATE ROUTE.\n  Enter the following:\n  Name: tenantname-networkcidr\n  Network: tenantname-transit\n  Destination Range: IP CIDR range in the SDDC for the tenant\n  Priority: 100\n  Next Hop: Specify an instance\n  Next Hop Instance: The StrongSwan VM.\n  Click CREATE\n      Configure IPSec VPN in NSX and Configure Tenant Firewall Rules #  The following steps describes the steps required to create a CDs instance if one does not already exist that the SDDC should be associated to.\n  Log into Partner Navigator and navigate to Cloud Director Service and open the instance that is managing the GCVE SDDC.\n  In the left pane click Edge Gateways and in the right pane click on the name of the tenant\u0026rsquo;s edge.\n     Click on IPSec VPN and then click NEW.      Enter the following:\n  General Settings:\n  Name: Name the IPSec tunnel such as tenantname-gcve-ipsec\n  Click NEXT\n    Peer Authentication Mode:\n  Authentication Mode: Pre-Shared Key\n  Pre-Shared Key: Enter the PSK used for the tunnel\n  Click NEXT\n    Endpoint Configuration:\n  Local Endpoint:\n- IP Address: Local IP address of the tunnel (edge external network address) - Networks: Local network CIDR(s) for the tunnel    Remote Endpoint:\n- IP Address: Remote IP address of the VPN appliance - Networks: 0.0.0.0/0   Remote ID: Remote IP address of the VPN appliance            Click NEXT, then click FINISH to save the IPSec tunnel configuration.\n  Click on VIEW STATISTICS.\n     After a few moments, the tunnel should show the Tunnel Status and IKE Service Status as Up.      Log into the GCP provider jump host, navigate the NSX URL and log in as admin.\n  Click the Security tab, then in the left pane select Gateway Firewall, and in the right pane click the Gateway dropdown and select the tenant\u0026rsquo;s T1 to add the firewall rule to.\n     Click ADD POLICY.     Click in the Name box for the policy and provide a name such as \u0026ldquo;TenantName Tenant Rules\u0026rdquo;.     Click the three horizontal dots to the left of the policy name and select Add Rule.      Enter the following:\n  Sources: Add the remote GCP tenant project\u0026rsquo;s CIDR block\n  Destination: Select Any for any local network or alternatively it can be locked down to a single CIDR\n  Services: Any (or filter to specifics if desired)\n  Action: Allow\n        Add another rule called Allow Outbound and set the following:\n  Sources: Select Any for any local network or alternatively it can be locked down to a single CIDR\n  Destination: Add the remote GCP tenant project\u0026rsquo;s CIDR block\n  Action Allow\n  Once ready, click PUBLISH.\n       Test the tunnel connectivity by deploying an instance in the GCP tenant project that was configured for the tunnel and the tenant in the SDDC to confirm it is functioning. Here we see that SSH/HTTP is connected between both tenant workloads.    Peer Existing Customer VPC #  The following section describes the steps required to pair a tenant owned customer VPC to an existing customer VPC. This step is optional as a customer may not have an existing GCP project.\nThe steps below will require information from the customer and given to the customer to complete the peering process.\nConfigure Provider Owned Customer VPC for Peering #  The following steps describes the steps required to peer the provider owned customer VPC to an existing customer owned VPC to enable connectivity between them.\n In the GCP console, switch to the tenant to configure project and go to VPC Network -\u0026gt; VPC Network Peering.     Click CREATE PEERING CONNECTION.     Click CONTINUE.      Enter the following:\n  Name: name the VPC connection something obvious such as \u0026ldquo;tenantname-to-gpc-vpc\u0026rdquo;\n  Your VPC Network: Select the tenant\u0026rsquo;s transit network\n  Peered VPC Network: In another project\n  Project ID: The name of the customer owned project\n  VPC Network Name: The default network name in the customer\u0026rsquo;s project\n  Exchange Custom Routes: Ensure both Import and Export custom routes are checked\n  Exchange Subnet Routes with Public IP: Select Export subnet routes with public IP\n  Click CREATE\n       The status of the peering will show with a Status of Inactive until the peering process is completed on the customer VPC side.    Customer to Configure the Customer Owned VPC for Peering #  The following steps describes the steps required to peer the customer owned VPC to the provider owned customer VPC to enable connectivity between them.\n  Complete the same process as shown in the previous step and provide the customer the following information to complete the peering:\n  Peered VPC Network: In another project\n  Project ID: The name of the provider owned customer project\n  VPC Network Name: The default network name in the tenant owned customer\u0026rsquo;s project \u0026ldquo;tenantname-transit\u0026rdquo;\n  Exchange Custom Routes: Ensure both Import and Export custom routes are checked\n  Exchange Subnet Routes with Public IP: Select Export subnet routes with public IP\n  Click CREATE\n    Once the customer has completed the peering process, click REFRESH on the VPC network peering page and the Status should change to Active.\n     Click on Routes on the VPC Network page, then click on the PEERING tab and it should display a list of peering routes discovered through the peering process.     To test the connectivity, try to SSH/ping from a workload on the customers GCVE environment into a workload in the GCP peering VPC. A firewall rule will need to be in place on the customer\u0026rsquo;s VPC side allowing the connectivity if it is not already present.    Setup NAT VMs for Internet Access (Optional) #  Note that this section is optional and only required if the customer will have Internet traffic egressing from the provider owned customer project.\nThe following section describes the steps required to setup a group of VMs for NAT for Internet access for workloads within GCVE.\nThese steps are required if the Internet access is egressing from the provider owned customer project or if a customer is routing all traffic to their peered project. The NAT VMs should be created and configured in the project that Internet access is egressing from.\nPrepare and Deploy NAT VMs #  The following steps describes the steps required to prepare the environment to deploy the VMs used for NAT for Internet access and should be run from the project where the traffic will egress using NAT\u0026rsquo;s and ILB\u0026rsquo;s based on GCP compute instances.\nAnother third party solution can be used for this part which is encouraged for more flexible configuration and easier day two operations.\nTwo NAT VMs will be deployed, one in a different AZ in the region for redundancy in an active/passive configuration. The machine size for the NAT VMs below is small for testing purposes, these should be sized appropriately based on the expected throughput.\nThe shell commands are embedded in the attached text document here. ![Graphical user interface, application Description automatically\n generated](/images/cloud-infrastructure-cds-gcve/cloud-infrastructure-cds-gcve130.emf)\n  In the project where the Internet traffic will egress, in the top blue bar, click the Cloud Shell icon to launch the shell.      Prior to copying the shell commands, do a find and replace and replace the following entries. Note: Be careful not to insert any extra spaces or carriage returns to avoid syntax errors.\n  Find: gcve-team-project ; Replace with: tenant-project-name (ex: tenant1-project)\n  Find: --subnet=cds-tenant01-us-west2 ; Replace with: --subnet=tenant-transit-subnet (ex: asia-southeast1)\n  Find: cds-tenant01 ; Replace with: tenant-transit-network (ex: tenant1-transit)\n  Find: -region=us-west2 ; Replace with: -region=project-region-name (ex: asia-southeast1)\n  Find: --zone=us-west2-a ; Replace with: --zone=project-region-az-a (ex: asia-southeast1-a)\n  Find: --zone=us-west2-b ; Replace with: --zone=project-region-az-b (ex: asia-southeast1-b)\n  Find: cds-natgw-startupscripts/nat_gateway_startup.sh ; Replace with: tenant-bucket-name/name_of_startup_script.sh (ex: tenant1-storage/nat_gateway_startup.sh)\n  Find: us-west2 ; Replace with: project-region-name (ex: asia-southeast1)\n  Fine: n1-standard-2 ; Replace with: name of properly sized instance type requried\n    If there are known instances with private Ips that need public Internet routing, run the command below to allocate public IP(s) to add to the NAT startup script prior to uploading. This would need to be done for each instance that needs incoming Internet traffic.\n  gcloud compute addresses create natgw-asia-southeast1-forwarding-external-01 --region asia-southeast1\n Change the region to match where the project is located    Run the following command to display the IP that was allocated:\n gcloud compute addresses describe natgw-asia-southeast1-forwarding-external-01 --region asia-southeast1          Note this IP address to use in the startup script section below.\n  Create a storage bucket and save the startup script:\n  Under the Google Cloud Platform menu, click Cloud Storage.\n     Click CREATE BUCKET.\n     Enter a name for the bucket such as \u0026ldquo;tenantname-storage\u0026rdquo; and click CREATE.\n  Open the below attached text file.Replace the text line \u0026ldquo;iptables -t nat -A PREROUTING -d $nlb_vip -i ens4 -j DNAT --to $test_endpoint_ip \u0026quot; for $nbl_vip with the public IP allocated for the workload requiring incoming Internet connections and $test_endpoint_ip with the private IP of the workload servicing the traffic (web server, etc).   If there as a public IP address allocated for a device that needs incoming Internet traffic, replace the line below with the public IP that was allocated and the correct private IP. Also delete the other line if it is not required; this iptables command would be a line for each public IP that will be forwarded. Change the public IP after -d to what was previously allocated and the IP after DNAT to the private IP of the host (not the T1 gateway, the private IP of the device, such as the Windows server)\n iptables -t nat -A PREROUTING -d 35.236.94.128 -i ens4 -j DNAT --to 10.0.0.3    Save the file locally as \u0026ldquo;nat_gateway_startup.sh\u0026rdquo; or something similar and close it.\n  Back in the storage bucket that was created, click into the bucket name, and then click UPLOAD FILES.\n  Upload the nat_gateway_startup.sh file that was saved locally.\n    From the text file with the shell commands, copy and paste the contents to create and configure the NAT and ILB:\n  Open the GCP cloud shell\n  Copy and paste first line into Google cloud shell to create the SSH firewall rule.\n  Skip the two lines that being with \u0026ldquo;glcoud compute networks\u0026rdquo; as they should already be created.\n  Copy the two lines with \u0026ldquo;gcloud compute addresses create\u0026rdquo; and paste those into the shell and hit enter to create the addresses.\n  Copy the next two lines with \u0026ldquo;nat_#_ip=$\u0026rdquo; and paste those into the shell and hit enter to set the NAT IP variables.\n  Copy the block commands with \u0026ldquo;gcloud compute instance-templates\u0026rdquo; and paste those into the shell and hit enter the create the templates.\n  Copy the lines \u0026ldquo;gcloud compute health-checks create\u0026rdquo; down through the three lines with \u0026ldquo;glcoud compute firewall-rules create\u0026rdquo; and paste them into the shell and hit enter to create the health check and firewall rules.\n  Copy the lines \u0026ldquo;gcloud compute instance-groups managed create\u0026rdquo; and paste them into the shell and hit enter to create the instances.\n  Copy the line \u0026ldquo;gcloud compute health-checks create\u0026rdquo; and paste it into the shell and hit enter to create the next health check.\n  Copy the line \u0026ldquo;gcloud compute backend-services create\u0026rdquo; and paste it into the shell and hit enter to create the natgw backend.\n  Copy the two lines \u0026ldquo;gcloud compute backend-services add-backend\u0026rdquo; and paste them into the shell and hit enter to add the two nats to the backend that was just created.\\\n  The lines under Just Outbound NAT can be skipped if the customer has both incoming and outgoing traffic.\n  Copy the line \u0026ldquo;gcloud compute forwarding-rules create\u0026rdquo; and paste it into the shell and hit enter to create the forwarding rule.\n  Copy the lines for \u0026ldquo;gcloud compute routes create\u0026rdquo; and paste them into the shell and hit enter to create the two routes.\n  Under Public IP Exposure settings, copy the line \u0026ldquo;gcloud compute backend-services create\u0026rdquo; and paste it into the shell and hit enter to create the backend service for the ILB.\n  Copy the two lines with \u0026ldquo;gcloud compute backend-services add-backend\u0026rdquo; and paste them into the shell and hit enter to add the hosts to the backend.\n  If a public IP was allocated previously for an existing workload, copy the last line with \u0026ldquo;gcloud beta compute forwarding-rules\u0026rdquo; and paste it into the shell and hit enter to add the forwarding rule.\n  Note: When adding a new public IP for a workload, the last two lines in this file would need to be reran to allocate the public IP, then create a forwarding rule for it.\n    Configure Firewall and Routes #  The following steps describes the steps required create the firewall rule and routes required to load balancer Internet traffic across the 3 NAT internet gateways that were previously deployed.\nThe default routes for the NAT will use instance tags to keep from the routes leaking back into the GCVE environment. This tag can be whatever the provider desires, but it must be uniform across all 3 routes that will direct traffic to the Internet via the NAT gateways. This applies to the routes created below for nat1, nat2, and nat3. This tag must match the one used on the instances created previously (VPN host).\n  In the provider owned customer tenant, navigate to Cloud shell and enter the following command to allow intervpc communication, changing the \u0026ldquo;\u0026mdash;network=tenant1-transit\u0026rdquo; to the customer\u0026rsquo;s transit network.\n gcloud compute firewall-rules create intervpc-communication1 --direction=INGRESS --priority=100 --network=tenant1-transit --action=ALLOW --rules=all --source-ranges=10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,100.64.0.0/16 --target-tags=natgw    Create a firewall rule for the NAT health check by running the following command, changing the \u0026ldquo;\u0026mdash;network-tenant1-transit\u0026rdquo; to the name of the customer\u0026rsquo;s transit network.\n gcloud compute firewall-rules create \u0026quot;natfirewall\u0026quot; --allow tcp:80 --target-tags natgw --source-ranges \u0026quot;209.85.152.0/22\u0026quot;,\u0026quot;209.85.204.0/22\u0026quot;,\u0026quot;35.191.0.0/16\u0026quot; --network=tenant1-transit    Click on Routes in the left pane and click CREATE ROUTE and enter the following:\n  Name: natgatewayout\n  Network: tenantname-transit\n  Destination IP range: 0.0.0.0/0\n  Priority: 50\n  Next Hope: Default internet gateway\n  Click CREATE\n        Click CREATE ROUTE and enter the following:\n  Name: nat1\n  Network: tenantname-transit\n  Destination IP range: 0.0.0.0/0\n  Priority: 100\n  Instance tags: intravpcdefault\n  Next Hop: Specify an instance\n  Next Hop Instance: nat-1\n  Click CREATE\n        Create two more routes, one named nat2 and one nat3 with the same priority of 100 and specify the next instance as nat-2 and nat-3 respectively for their routes as well as the same instance tag.\n  To test the functionality of the NAT VMs:\n  Open an SSH session to each NAT VM and installing tcpdump with \u0026ldquo;sudo apt install tcpdump -y\u0026rdquo;\n  On each host, run tcpdump with a filter set to the IP of a test workload in GCVE with \u0026ldquo;tcpdump -ni ens4 host ip_of_workload\u0026rdquo;\n  On the GCVE workload, repeatedly curl a public URL such as vmware.com and the traffic hits should spread across the three NAT VMs.\n      Conclusion #  At this point, the VMware Cloud Director service Instance is ready to deploy tenant VMs. For more information see the documentation for VMware Cloud Director service and VMware Cloud Director.\n"},{"id":26,"href":"/docs/cloud-infrastructure/cds-vmc-deployment-guide-msp/","title":"VMware Cloud Director Service for VMware Cloud on AWS Deployment Guide","section":"Cloud Infrastructure","content":"VMware Cloud Director Service for VMware Cloud on AWS Deployment Guide #  VMware Cloud Director™ service enables cloud providers to use VMware Cloud on AWS in multi-tenant modality with enhanced VMware NSX-T support, allowing provisioning of custom-sized, tenant-based, isolated, and secure VMware Cloud resources. This ability to service multiple smaller and medium sized tenants on the same infrastructure, offers flexibility to right-size the VMware Cloud on AWS environments to meet customer needs and requirements of all customer tiers.\nVMware Cloud Director service is a container-based SaaS version of the proven VMware Cloud Director on-premises service-delivery platform. The service, available through VMware Cloud Partner Navigator, helps cloud providers gain better economies of scale and generate new value and revenue for their cloud businesses.\nThis guide details the process of deploying a VMware Cloud Director service Instance, associating it with a VMware Cloud on AWS SDDC designed for use with VMware Cloud Director service, configuring the Provider Virtual Datacenter to use the resources of the associated VMware Cloud on AWS SDDC, configuring the VMware Cloud on AWS SDDC networking to prepare it for multi-tenant use and deploying the first tenant organization.\nBefore using this guide, it is necessary to join the VMware Managed Service Provider Program and have the require contracts in place to use the VMware Cloud services mentioned in this guide. See the MSP VMware Cloud on AWS Operations Handbook and the Cloud Director service Operations Handbook for more details.\nPrepare a VMware Cloud Partner Navigator Provider Organization #  Provision a new VMware Cloud Partner Navigator Organization\n  Click on Administration, then Add Organization\n  Fill out the Add Organization form and click Add Organization\nRequest access to VMware Cloud Director service by emailing: ask_cloud_director_service@VMware.com\n  Supply the Long ID of the Organization that will be used.\n  A onetime use invitation will be returned to activate VMware Cloud Director service.\nEnable the VMware Cloud on AWS and VMware Cloud Director service services in the new Organization\n  Select the new Provider Organization and click Manage Services\n  Click Continue\n  Click Open on both service tiles to activate the services   Click Open to continue\n  NOTE: It is also acceptable to use an existing Provider Organization enabled for VMware Cloud on AWS [which has no SDDCs currently deployed.]{.underline}\nDeploy a SDDC #  Deploy the SDDC in the same VMware Cloud Partner Navigator Organization activated above.\n  Select the VMware Cloud on AWS service tile   Select SDDCs and click Create SDDC\n  Fill out the form to deploy the SDDC according to your requirements. For more details about the deployment process for VMware Cloud on AWS SDDCs see Deploy an SDDC from the VMC Console.\n  NOTE: All VMware Cloud on AWS SDDCs used with VMware Cloud Director service must be deployed in Organizations that have been enabled for VMware Cloud Director service. SDDCs deployed in other Organizations are not compatible with VMware Cloud Director service.\nDeploy VMware Cloud Director service Instance #    Select the VMware Cloud Director service tile  If someone other than the Organization Owner will be deploying VMware Cloud Director service Instances the Organization Owner must first establish a trust relationship between VMware Cloud services and VMware Cloud Director service.\na. In the Cloud Director Instances screen select Configuration then Configure OAuth Trust Relationshipb. Select Dismiss once the trust is established\n![Graphical user interface, text, application Description automatically generated](/images/cloud-infrastructure-cds-vmc/cloud-infrastructure-cds-vmc12.png)    Select Create Instance to begin the instance deployment process.   Enter the data needed in the form and click on NEXT\n  Note: For the Upgrade Category, selecting Preview (if enabled) identifies this Cloud Director service Instance to be patched or upgraded earlier than when Production is selected. Use Preview for service development environments. The Upgrade Category cannot be changed after deployment.\n Acknowledge costs and then click on CREATE INSTANCE\n  Click on Activity Log for detailed information about the deployment progress.\n  When theÂ VMware Cloud DirectorÂ instance deployment is complete, its card displays aÂ ReadyÂ status.\nGenerate API Token #  An API token for the Organization holding the SDDC is used to associate the SDDC with VMware Cloud Director service.\n  Click on your name in the top right and then click on My Account\n  Click on API Tokens\n  Click on GENERATE TOKEN\n  Enter form fields and then click on GENERATE\n  Notes: This token is only used during the association process, so its Token TTL should be short. Minimum required Organization Role is Organization Member. Minimum required Service Roles are VMware Cloud on AWS - Administrator and VMware Cloud on AWS - NSX Cloud Admin. Make sure to store the generated token in a safe place.\nCreate a DHCP Network #  Need to create a network segment that has routed access to the SDDC management network, provides DHCP service and has a DNS server configured. Note: This step can be skipped if you only have one host in your SDDC.\n  Click on ADD SEGMENT\n  Enter segment details and then click on SET DHCP CONFIG\n  Enter details and then click on APPLY\n  Click SAVE\n  S3 Configuration #  If your SDDC is in Oregon (us-west-2), you will need to either disable S3 or configure a S3 VPC endpoint prior to associating the SDDC. If your SDDC is in any other region, this step can be skipped. By default, S3 traffic in the local region fails until either a VPC endpoint is configured or S3 is disabled. Part of the associate process automatically deploys a proxy appliance into the SDDC. That proxy appliance currently is stored on an S3 bucket in Oregon (us-west-2), which given the default configuration makes it inaccessible.\nDisable S3 #   Click DISABLE\n  Configure S3 VPC endpoint #  See the Amazon Virtual Private Cloud AWS PrivateLink documentation: Endpoints for Amazon S3\nAssociate VMware Cloud Director service Instance #    Click Associate a VMC SDDC\n  Enter fields and then click on ASSOCIATE a VMC SDDC\n  Note: If you only have one host in your SDDC, you can use the predefined network named \u0026ldquo;sddc-cgw-network-1\u0026rdquo; for the Proxy VM Network field.\nAssociate Custom Domain (Optional) #  Allows the provider to use their own domain name for VMware Cloud Director service Instances.\n  Click on ACTIONS and then click on Associate Custom Domain\nSee the VMware Cloud Director service documentation Customize the DNS and Certificate Settings for more details.\n  VMware Cloud Director service Instance Configuration #  Launch the Provider Portal #    To use VMware Cloud Services as an Identity Provider for VMware Cloud Director service Instances, select Configure VMware Cloud Services as instance IDP from the Actions menu of each VMware Cloud Director service Instance.   Select Configure IDP\n  Click on OPEN INSTANCE\n  NOTE: To control user access when VMware Cloud Services is used as the VMware Cloud Director service Instance IDP, use the Role Assignment feature in VMware Cloud Partner Navigator to grant users Admin, Read Only or No Access roles to all IDP enabled VMware Cloud Director service Instances. The Admin role will login to instances with the CDS Provider Admin role. The Read Only role will login to instances with the CDS Provider Admin Read Only role.\nCreate Provider VDC #    Click on NEW\n  Enter details and then click on NEXT\n  Select vCenter and then click on NEXT\n  Select Resource Pool, Hardware version and then click on NEXT\n  Select the VMC Workload Storage Policy \u0026ndash; Cluster-1 and then click on NEXT.   Note: Make sure that the other storage policies are left unselected.\n Select NSX-T manager and Geneve Network pool and then click on Next.\n  Click on FINISH\n  Update External Network with valid IP range #    Click on Tier-0 Gateways in the left pane and then click the name of the pre-existing External Network.\n  Click on Network specification and then Edit.\n  Delete existing network spec by selecting it and then clicking on DELETE\n  Click on NEW\n  Enter Gateway CIDR and click on the pencil.\n  Note: We recommended that you use a subnet in the 100.64.0.0/10 range (RFC-6598 \u0026ndash; Carrier-grade NAT) to avoid conflicts with RFC-1918 private address space used in on premises locations and allow for extensive NATing of tenant IPs to the External Network. For example, you may choose 100.68.1.1/24 for the first SDDC deployed and use 100.68.2.1/24 for the second SDDC and so on. It is important to note that NSX-T uses 100.64.0.0/16 for T0-T1 interlink and is not available for use in an NSX-T environment like VMware Cloud on AWS.\n Enter Static IP Pools, click ADD and then click on SAVE\n  Click on **Save\n**  Create Inventory Group for External Networks #    Click Add Group\n  Enter Name and click on Set Members\n  Click on IP Addresses, enter the range associated with the external network previously specified and then click on APPLY\n  Click on SAVE\n  Create Firewall Rule to Allow VMware Cloud Director service Tenant Traffic #    Click Add Rule\n  Add rule details and then click on PUBLISH.\n  Notes: For Sources, make sure to select the Group created previously (VMware Cloud Director service External Network Ips). For Applied To, make sure to select Internet Interface.\nCreate First Tenant #  Create Organization #   Click on New  Fill in details and click on CREATE\n  Create Organization VDC #    Click on NEW\n  Fill in General details and click NEXT\n  Select Organization and click NEXT\n  Select Provider VDC and click NEXT\n  Select Allocation Model and click NEXT\n  Enter details and click NEXT\n  Select Storage Policy and click NEXT\n  Note: Thin provisioning and Fast provisioning are recommended, but not required.\n Select Network Pool and click NEXT\n  Confirm and click FINISH\n  Create Edge Gateway #    Click on NEW\n  Select Organization VDC and click on NEXT\n  Enter Name and click on NEXT\n  Select External Network and click on NEXT\n  Select Edge Cluster and click on NEXT\n  Add IP Allocation, click ADD, then click on NEXT\n  Review and click FINISH\n  Request a public IP for Tenant\u0026rsquo;s edge #    Click REQUEST NEW IP\n  Enter Notes and then click SAVE\n  Create a NAT pointing to the tenant\u0026rsquo;s edge gateway #    Click on ADD NAT RULE.\n  Select previously created public IP and make sure that the Internal IP matches the IP assigned to the edge gateway.\n  Click SAVE\n  Create Organization network #    Open Tenant portal by click on the box with the arrow next to the selected organization\n  Click on NEW\n  Select Scope and then click NEXT\n  Select Routed and then click NEXT\n  Select edge and the click NEXT\n  Enter name and CIDR and then click on NEXT\n  Enter Static IP Pool and then click on NEXT\n  Enter DNS and then click on NEXT  Review and click FINISH\n  Create SNAT to allow outbound traffic #    Select proper edge gateway and then under NAT click on NEW\n  Enter SNAT information and then click on SAVE.\n  Note: Make sure External IP is on the Edge and that the Internal IP matches the org network subnet\nConclusion #  At this point, the VMware Cloud Director service Instance is ready to deploy tenant VMs. For more information see the documentation for VMware Cloud on AWS, VMware Cloud Director service and VMware Cloud Director.\n"}]