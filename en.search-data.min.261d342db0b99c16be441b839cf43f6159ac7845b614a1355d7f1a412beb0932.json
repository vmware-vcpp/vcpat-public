[{"id":0,"href":"/docs/cloud-infrastructure/","title":"Cloud Infrastructure","section":"Docs","content":"About {VVS for CP Title} #  The {VVS for CP Title} validated solution provides detailed design, implementation, configuration, and operation guidance on \u0026hellip; A VMware validated solution is a technical validated implementation that is built and tested by VMware and VMware cloud providers to help customers resolve common business use cases. VMware validated solutions are cost-effective, performant, reliable, and secure. Each solution contains a detailed design, implementation, and operational guidance.\nAutomation for This Design in VMware Cloud Foundation #  The implementation tasks for some design decisions are automated by SDDC Manager. You must perform the implementation manually for the rest of the design decisions as noted in the design implication. To provide a fast and efficient path to automating the {VVS for CP Title} implementation, this document provides Microsoft PowerShell cmdlets as code-based alternatives to completing certain procedures in each SDDC component\u0026rsquo;s user interface. You can directly reuse the PowerShell commands by replacing the provided sample values with values from your VMware Cloud Foundation Planning and Preparation Workbook.\nIntended Audience #  The {VVS for CP Title} documentation is intended for cloud provider architects and administrators who are familiar with and want to use VMware software and a {VVS for CP topic} for VMware Cloud Foundation.\nSupport Matrix #  The {VVS for CP Title} validated solution is compatible with certain versions of the VMware products that are used for implementing the solution.\nSoftware Components in {VVS for CP Title} #  {Insert table of VCF versions and VVS software versions}\nBefore You Apply This Guidance #  To design and implement the {VVS for CP Title} validated solution, your environment must have a certain configuration.\nSupported VMware Cloud Foundation Deployment #  {Insert table of Mgmt and Wkld domain design requirments for this VVS for CP. See example.}\nOverview of {VVS for CP Title} #  By applying the {VVS for CP Title} validated solution, you implement {describe outcome}.\nImplementation Overview of {VVS for CP Title} #  {Insert table of implementation stages and steps for each}\nUpdate History #  This {VVS for CP Title} is updated when necessary. {Table of updates – Revision: dates, Description: Bullet list of changes}\n"},{"id":1,"href":"/docs/cloud-infrastructure/planning/","title":"Cloud Infrastructure Planning and Preparation","section":"Cloud Infrastructure","content":"Cloud Infrastructure Planning and Preparation #  Coming soon!\n"},{"id":2,"href":"/docs/developer-ready-cloud/","title":"Developer Ready Cloud","section":"Docs","content":"Developer Ready Cloud #  Coming soon!\n"},{"id":3,"href":"/docs/developer-ready-cloud/planning/","title":"Developer Ready Cloud Planning and Preparation","section":"Developer Ready Cloud","content":"Developer Ready CloudPlanning and Preparation #  Coming soon!\n"},{"id":4,"href":"/docs/dr-migration/","title":"DR \u0026 Migration","section":"Docs","content":"DR \u0026amp; Migration #  Introduction #  Welcome! Here you can find suggestions on how to plan, set up and operate your DR and Migration services.\nVMware Cloud Director Availability #  VMware Cloud Director Availability is a Disaster-Recovery-as-a-Service solution. It is specially designed and available for participants in VMware Cloud Provider Program and allows them to protect and migrate vApps and VMs:\n From on-premises vCenter Server site to a VMware Cloud Director cloud From VMware Cloud Director cloud to an on-premises vCenter Server environment Between VMware Cloud Director managed clouds  Use Cases #  VMware Cloud Director Availability supports two different use cases – disaster recovery and migration of vApps/VMs. Both of them rely on replication of virtual machines. In both cases at least one of the sides is a VMware Cloud Director managed cloud and the other side could be another VMware Cloud Director cloud or vCenter on-premises site. VMware Cloud Director Availability cannot protect or migrate bare metal servers or VMs managed by non-VMware hypervisors.\nDisaster Recovery #  A tenant can purchase DRaaS provided by a public cloud and based on VMware Cloud Director Availability to protect their virtual machines running in their on-premises datacenter. The Cloud Provider assigns a portion of compute, storage, and network resources from their cloud and groups them in an Organization Virtual Data Center (OrgVDC) that belongs to the tenant. Also, the Cloud Provider can enable this OrgVDC (respectively, the tenant) to protect their on-premises virtual workloads to the cloud with specific parameters like minimum RPO, bandwidth control, direction of the protection, and more. It allows the tenant to failover their VMs in case of an outage in their on-premises data center.\nMigration #  When tenants plan to migrate workloads to the cloud, they may use the Migration workflow to simplify the process. When a “New Migration” is configured, VMware Cloud Director Availability starts replication of the vApp/VM from the source to the destination.\nAppliances #  There are three VMware Cloud Director Availability appliances required for the setup in the cloud and one on-premises appliance to be deployed at the tenant\u0026rsquo;s on-premises site.\nCloud Replication Management Appliance #  The Cloud Replication Management appliance is responsible for the communication with VMware Cloud Director. Based on this communication, it discovers resources (OrgVCD, storage policies, datastores, networks, etc.) managed by VMware Cloud Director and used by the tenants. This information is required for discovering vApps/VMs that can be replicated/migrated or suitable destination locations for incoming replications/migrations.\nIt also provides the VMware Cloud Director Availability UI and API interfaces. Another role of this appliance is to communicate with all the local and remote Replicators and receive data regarding each protected/migrated workload. Two VMware Cloud Director Availability services cover these functionalities in Cloud Replication Management appliance – the cloud.service and the manager.service.\nCloud Service #  This service understands the VMware Cloud Director constructs – OrgVCD, vApps, networks managed by VMware Cloud Director, storage policies, etc. To achieve this VMware Cloud Director Availability Cloud Management appliance communicates with the VMware Cloud Director API through the VMware Cloud Director LB. The Cloud Replication Management appliance does not communicate with the VMware Cloud Director consoleproxy cells/interfaces. Based on this, VMware Cloud Director Availability can:\n discover VMware Cloud Director managed vApps/VMs and protect/migrate them to another DR-enabled cloud or on-premises vCenter discover a suitable destination for incoming replications/migrations  The cloud service manages pairings with other DR-enabled clouds, policies, SLA profiles, and their assignment to VMware Cloud Director organizations. It provides information about the replication and system tasks. Also, it reports the replication compute resources consumption per tenant and per PVDC, the storage consumption per datastore, plus many other high- and low-level details. The cloud service management interface is available on https://vcda_manager_fqdn:443/admin. It is possible to log in with the local OS root account, an SSO account if the appliance has registration in an SSO domain, or with a VMware Cloud Director System Administrator account if the initial configuration is already completed.\nManager Service #  The manager service manages the registrations of local and remote replicators. During the pairing process, remote replicators are registered in the Cloud Replication Management appliance. For each replication, it chooses one replicator from the source site and one from the destination site. The destination replicator is responsible for discovering the appropriate resources to create the replica disks at the destination and write data. Replicators send information to the Cloud Replication Management appliance about their operation – statuses, amount of data replicated, operation start time, time to complete, etc. The manager service is also used to manage replicators. It can trigger the rebalancing of replications across all replicators or put a replicator in maintenance mode, which leads to assigning each of its replications to another one. This is useful when the current replicators need to be offloaded by adding a new replicator in the solution.\nThe manager service management interface is available at https://vcda_manager_fqdn:8441. It is possible to log in with the local OS root account and an SSO account if the appliance is already registered in Lookup Service.\nCloud Replicator #  Cloud Replicator is responsible for moving the replication data around - to and from the ESXi hosts and the cloud. For outgoing replications/migrations, it communicates with the VMKernel interface of the ESXi host, captures and encrypts replication data, optionally compresses it, and sends it to the remote replicator, which can be another Cloud Replicator or on-premises Replicator. For incoming replications/migrations the Cloud Replicator receives replication data from a Replicator (cloud or on-premises), decompresses and decrypts this data, and sends it to the ESXi to be written on a datastore. Cloud Replicator is the only component that can scale out as the number of protections/migrations increases.\nCloud Tunnel #  The Cloud Tunnel appliance is the single-entry point to the VMware Cloud Director Availability instance in the cloud. Its role is to handle and forward the incoming management and replication traffic. It is the only VMware Cloud Director Availability appliance that needs a dedicated Internet-accessible endpoint.\nOn-premises Appliance #  The VMware Cloud Director Availability on-premises appliance is deployed in the tenant data center. It creates a pairing relation to VMware Cloud Director Availability in the cloud and can protect and/or migrate VMs running locally. The on-premises appliance does not require a public endpoint as it only initiates connectivity to the cloud. A single on-premises appliance can protect VMs from a single SSO domain even if there are multiple vCenter Servers in this SSO domain. If the requirement is to do replications from the cloud to on-premises, then a single appliance is required for each vCenter due to how placement works. Also, one on-premises appliance can be paired to a single VMware Cloud Director Availability instance in the cloud. If the tenant intends to use more than one DR service in the cloud, a dedicated appliance is required for each cloud DR service.\nReference #  Official documentation\n"},{"id":5,"href":"/docs/dr-migration/planning/","title":"DR \u0026 Migration Planning and Preparation","section":"DR \u0026 Migration","content":"DR \u0026amp; Migration Planning and Preparation #  Everything you need to know before deploying VMware Cloud Director Availability.\nHardware Requirements #     Appliance Type Description and Services Hardware Requirements     Cloud Replication Management Appliance A dedicated appliance, that runs the following VMware Cloud Director Availability services: * Manager Service * Cloud Service with embedded VMware Cloud Director Availability Tenant Portal - 2 vCPUs - 4 GB RAM\n- 10 GB Storage   Cloud Replicator Appliance A dedicated appliance for the Replicator Service that handles the replication traffic for a site. For large-scale environments, you can deploy more than one Cloud Replicator Appliance per cloud site. - 4 vCPUs - 6 GB RAM - 10 GB Storage   Cloud Tunnel Appliance A dedicated appliance for the Tunnel Service. - 2 vCPUs - 2 GB RAM\n- 10 GB Storage    Other Requirements #  The resource vCenter Server instances within a VMware Cloud Director site must be within the same single sign-on domain. All Replicator Service, Manager Service, Cloud Service, and Tunnel Service instances within the respective site must be configured with that same single sign-on domain.\nNetwork Ports #     Source Destination Port Number Protocol Description     VMware Cloud Director Availability Replicator ESXi Hosts 902 TCP and UDP Used by the VMware Cloud Director Availability Replicator service for replication traffic to the destination ESXi hosts.   VMware Cloud Director Availability Replicator VMware Platform Services Controller® 443 TCP Used for single sign-on and Lookup Service communication.   VMware Cloud Director Availability Replicator vCenter Server 443 TCP Used by the local VMware Cloud Director Availability vApp Replication Manager service or the VMware Cloud Director Availability Replicator service for communication with the local vCenter Server.   VMware Cloud Director Availability Replicator VMware Cloud Director Availability vApp Replication Manager 8044 TCP Used for vCloud Availability vApp Replication Manager management from the vCloud Availability Replicator.   VMware Cloud Director Availability Replicator VMware Cloud Director Availability Tunnel 8048 TCP Used for VMware Cloud Director Availability vApp Replication Manager management from the VMware Cloud Director Availability Replicator.   VMware Cloud Director Availability vApp Replication Manager VMware Platform Services Controller® 443 TCP Used for single sign-on and Lookup Service communication.   VMware Cloud Director Availability vApp Replication Manager VMware Cloud Director service 443 TCP Used for VMware Cloud Director Director management from the VMware Cloud Director Availability vApp Replication Manager.    VMware Cloud Director Availability vApp Replication Manager VMware Cloud Director Availability Replicator 8043 TCP Used for VMware Cloud Director Availability Replicator management from the VMware Cloud Director Availability vApp Replication Manager.    VMware Cloud Director Availability vApp Replication Manager VMware Cloud Director Availability Tunnel 8047, 8048 TCP Used for VMware Cloud Director Availability Tunnel management from the VMware Cloud Director Availability vApp Replication Manager.    VMware Cloud Director Availability Tunnel VMware Cloud Director Availability Replicator 8043, 44045 TCP Used for VMware Cloud Director Availability Replicator management from the VMware Cloud Director Availability Tunnel.    VMware Cloud Director Availability Tunnel VMware Cloud Director Availability vApp Replication Manager 8044, 8046 TCP Used for VMware Cloud Director Availability vApp Replication Manager service management from the VMware Cloud Director Availability Tunnel appliance.    VMware Cloud Director Availability Tunnel VMware Platform Services Controller® 443 TCP Used for VMware Platform Services Controller® communication management from the VMware Cloud Director Availability vApp Replication Manager and VMware Cloud Director Availability Replicator.    ESXi Hosts VMware Cloud Director Availability Replicator 31031, 44045, 44046 TCP Used by the ESXi hosts for replication traffic to the destination VMware Cloud Director Availability Replicator service.    Firewall VMware Cloud Director Availability Tunnel 8048 TCP Used for redirecting external traffic management to the VMware Cloud Director Availability Tunnel service.     Important Considerations #  Replication Traffic #  A typical good practice is to separate the management and resource vCenters/clusters. The number of hosts in resource vCenters/clusters is significantly higher than the number in the management cluster. A recommendation is to deploy Replicators on resource hosts and not in the management cluster so more Replicators can be deployed and a DRS rule can be created to keep the Replicator VMs on different hosts for better load distribution. Also, the replication traffic path from the Replicator appliances to the replication network on the resource hosts will be enhanced. The hosts can use the management vmkernel interface to communicate with the Replicator. Another option is to have a dedicated vmkernel interface for replication purposes only. Using the management vmkernel simplifies the configuration but significantly reduces the control options available to the administrator and can lead to a risk for routing uncompressed replication traffic, which is highly non-desired. The general recommendation is to use a dedicated vmkernel for the replication traffic. In this case, the administrator will have better control over the infrastructure. Using NIOC, the administrator will be able to set shares for different types of vmkernel traffic. It enables carrying the replication traffic over dedicated uplinks.\nNote: If any other VMware HBR-based replication products (vSphere Replication, VMware Site Recovery Manager or VMware HCX) are used in this cloud, configuring a dedicated vmkernel interface marked with “vSphere Replication” and “vSphere NFC Replication” will lead to all of these products try to use it. This means the network connectivity between their appliances and the replication vmkernel interfaces will be required to enable all of these products to operate successfully. For more information, please consult with respective product documentation.\nStorage Requirements #  For a successful test failover, the destination storage must accommodate double the source virtual machine disk size. VMware Cloud Director Availability 4.2 and later do not consume double the disk size during failover.\n Example required space in the datastore, for a source virtual machine with a 2 TB virtual disk. When the replication is created, VMware Cloud Director Availability allocates 2 TB in the destination storage. VMware Cloud Director Availability allocates additional 2 TB when starting a test failover task. After finishing the test failover task, the additional 2 TB space is unallocated. Example for a VMware vSAN storage, with the same virtual machine. The same storage implication applies, where the vSAN must accommodate double the virtual machine disk size. When the replication is created in this example, VMware Cloud Director Availability allocates 2 TB multiplied by the vSAN_Protection_Level_Disk_Space_Penalty. When starting a test failover task, additional 2 TB are allocated multiplied by the vSAN_Protection_Level_Disk_Space_Penalty.  "},{"id":6,"href":"/docs/networking-security/","title":"Networking \u0026 Security","section":"Docs","content":"Networking \u0026amp; Security #  Welcome! Here you can find suggestions on how to plan, set up and operate your networking and security services, either in a self-service mode, or via managed services.\nConfiguring VMware Cloud Director service for Load Balancing as a Service #  See the instructions below on how to deploy and configure VMware NSX Advanced Load Balancer in combination with VMware Cloud Director to provide Load Balancing as a Service capabitilies for end users.\n Load Balancing as a Service in VMware Cloud Director  "},{"id":7,"href":"/docs/networking-security/planning/","title":"Networking \u0026 Security Planning and Preparation","section":"Networking \u0026 Security","content":"Networking \u0026amp; Security Planning and Preparation #  Before any solution inplementation, you must set up an environment that has a specific compute, storage, and network configuration and that provides external services to the components of the solution. Please find below a list of resources that can help you to plan accordingly for networking and security services in VMware Cloud Director:\n Planning and preparation for LBaaS in VMware Cloud Director  "},{"id":8,"href":"/docs/sovereign-cloud/","title":"Sovereign Cloud","section":"Docs","content":"Sovereign Cloud #  Coming soon!\n"},{"id":9,"href":"/docs/sovereign-cloud/planning/","title":"Sovereign Cloud Planning and Preparation","section":"Sovereign Cloud","content":"Sovereign Cloud Planning and Preparation #  Coming soon!\n"},{"id":10,"href":"/docs/cloud-infrastructure/design/","title":"Cloud Infrastructure Design","section":"Cloud Infrastructure","content":"Cloud Infrastructure Design #  Coming soon!\n"},{"id":11,"href":"/docs/developer-ready-cloud/design/","title":"Developer Ready Cloud Design","section":"Developer Ready Cloud","content":"Developer Ready Cloud Design #  Coming soon!\n"},{"id":12,"href":"/docs/dr-migration/design/","title":"DR \u0026 Migration Design","section":"DR \u0026 Migration","content":"DR \u0026amp; Migration Design #  Architecture #  The following diagram shows the network flow between the VMware Cloud Director Availability components and the other infrastructure components part of the replication process.\nOther Components #  Platform Services Controller #  All the VMware Cloud Director Availability appliances need to be configured with a Lookup Service for two purposes:\n Service authentication and resource discovery – the Replicator and Management appliances require registration with a Lookup Service to authenticate to the resource vCenters and discover resources which is required for operations from the VMware Cloud Director Availability workflow. Authentication to the VMware Cloud Director Availability service by the administrator which is not mandatory. The Lookup Service has to be the one used by vCenters providing compute resources to VMware Cloud Director. For cases when the management and resource vCenters use different Lookup Services and the VMware Cloud Director Availability appliances are deployed at the management vCenter, they need to be configured with the Lookup Service of the resource vCenter(s) from/to which they will protect VMs/vApps.  The Lookup Service and the associated Single Sign-On (SSO) domain is the boundary of a single VMware Cloud Director Availability instance. This is because a single Cloud Replication Management appliance can be registered with a single Lookup Service providing DR and migration services only for workloads in this SSO domain. For cases when the cloud provider has multiple PVDCs in more than one SSO domain, a VMware Cloud Director Availability instance (tunnel, manager, replicators) needs to be deployed per each SSO domain. It is possible to deploy a dedicated VMware Cloud Director Availability instance per PVDC, but one can protect as many PVDCs as exist in a single SSO domain.\nvCenter Server #  VMware Cloud Director Availability needs to discover the workload VMs that need to be protected or migrated and destination resources (clusters, datastores, networks, etc.) for incoming replications and/or migrations in vCenters that are configured to provide the resources for VMware Cloud Director.\nThe Replicator is the component that communicates with the vCenter Servers. The tunnel has no such requirement.\nVMware Cloud Director #  The integration with VMware Cloud Director is done through the Cloud Replication Management appliance. The Cloud service communicates with VMware Cloud Director to discover the vApps/VMs for outgoing replications/migrations or destination resources (OrgVDC, network, storage) for incoming replications/migrations. VMware Cloud Director Availability uses the VMware Cloud Director API to request the information and initiate any operations.\nAs part of the initial setup, the administrator provides the VMware Cloud Director API HTTPS Base URL followed by /api and user account, which is a member of the System organization with its password. Then VMware Cloud Director Availability inspects the certificate presented by VMware Cloud Director. If the FQDN of the API HTTPS Base URL is not set as the Subject Alternative Name DNS entry, the registration cannot be completed. It is not enough to have this FQDN present in the certificate Common Name field. When a TLS connection to VMware Cloud Director is established and the authentication with the provided credentials is successful, VMware Cloud Director Availability uses the VMware Cloud Director REST API to discover its UI endpoint. Depending on its value, the VMware Cloud Director Availability HTTP server is dynamically reconfigured - the respective domain is added to the CORS allowlist.\nAdditional Information: CORS is an HTTP mechanism that provides better security to end-users. Depending on how the server is configured, browsers may reject attempts to load resources from a server that is on domain A, but someone tries to load it in domain B. The VMware Cloud Director Availability Cloud service is set to allow only one VMware Cloud Director endpoint - the VMware Cloud Director UI endpoint or, if it is not configured, the API endpoint. Once the certificate and credentials check are successful, VMware Cloud Director Availability deploys the files required for extension operation in the VMware Cloud Director cells.\nESXi hosts #  The Replicators communicate with hosts to:\n Configure the source VM for replication by enabling IO filter and configure it where to send replication data Allocate resources in the host for the destination replication  These operations are performed by the HBRsrv service. For authentication purposes, it needs to communicate to the hosts on port 80/tcp. For the data traffic, it uses 902/tcp to hosts to send the replication data (for incoming replications) and 44046/tcp from the hosts to receive replication data (for outgoing replications).\nData Paths #  Two appliances are responsible for the replication data path – the Replicator and the Tunnel. The Cloud Replication Management appliance does not handle any replication data.\nCloud Tunnel #  The Tunnel is a transparent proxy that is aware of the TLS protocol. It receives the incoming traffic and redirects it to the backend components based on its type. The management and HTTPS/API traffic is sent to the Cloud Replication Management, and the replication data traffic is sent to Replicator. It does not perform any operations on the replication traffic. The Tunnel appliance does not consume significant compute resources, but in environments with many replications may handle a significant amount of network traffic. As it is published to the Internet via DNAT, it is recommended to connect it to a DMZ network where all the necessary security measures for such publish rules are taken.\nCloud Replicator #  The Replicator appliance runs three services – HBRsrv, LWDproxy, and Replicator.\nThe HBRsrv is a low-level component that communicates with the replication IO filter in ESXi hosts. The protocol used for communication is called LWD (Light Weight Delta) - a VMware proprietary protocol. This service is not aware of vCenter and does not communicate with it. The HBRsrv service is responsible for saving the VM deltas generated during the RPO window on the destination datastore. It is a common service for other VMware replication products, and VMware Cloud Director Availability reuses it for moving replication data at the last mile. It configures the ESXi filter to send the replication traffic between the ESXi host and source Replicator in \u0026ldquo;plain text\u0026rdquo;. When the data is received by the source LWDproxy, it gets encrypted and optionally compressed. On the destination site, there\u0026rsquo;s another LWDproxy that decrypts and decompresses the LWD traffic, and the destination Replicator sends the replication data to an ESXi host.\nThe LWDproxy is a VMware Cloud Director Availability service that ensures that replication traffic is sent to the Tunnel and is encrypted and compressed outside the local infrastructure. The LWDProxy service communicates with the Tunnel for sending/receiving replication data. Encryption and compression result in increased CPU utilization for the Replicator appliance. This may reduce the number of replications a single Replicator can handle.\nThe Replicator service is a VMware Cloud Director Availability management service that sends configuration instructions to the HBRsrv and LWDproxy services for every replication handled by the Replicator appliance. It understands the vCenter objects like hosts, VMs, datastores, etc. In this way, the Replicator discovers and prepares the source VMs for replication and creates objects (independent disks) for the incoming replications. It is supposed there will be hundreds or thousands of VM replications/migrations in a cloud environment. Even if the number of replications is lower than the maximum supported handled by a single Replicator, it is highly recommended to deploy at least two or more replicators. With several Replicators, the load generated by the replications can be spread over multiple hosts. Balance the replication traffic on more physical uplinks and distribute the load better. A single Replicator can be put in maintenance mode during a maintenance window without affecting replication traffic.\n"},{"id":13,"href":"/docs/networking-security/design/","title":"Networking \u0026 Security Design","section":"Networking \u0026 Security","content":"Networking \u0026amp; Security Design #  Coming soon!\n"},{"id":14,"href":"/docs/sovereign-cloud/design/","title":"Sovereign Cloud Design","section":"Sovereign Cloud","content":"Sovereign Cloud Design #  Coming soon!\n"},{"id":15,"href":"/docs/cloud-infrastructure/implementation/","title":"Cloud Infrastructure Implementation","section":"Cloud Infrastructure","content":"Cloud Infrastructure Implementation #  Welcome!\nConfiguring VMware Cloud Director service for SDDC Endpoints #  The following sections describes how to configure various SDDC endpoints for VMware Cloud Director service.\nConfiguring VMware Cloud Director service for VMware Cloud on AWS #  See the instructions below on how to deploy VMware Cloud on AWS (VMConAWS) for multi-tenancy with VMware Cloud Director service.\nVMware Cloud Director Service for VMware Cloud on AWS Deployment Guide\nConfiguring VMware Cloud Director service for Google Cloud VMware Engine #  See the instructions below on how to deploy Google Cloud VMware Engine for multi-tenancy with VMware Cloud Director service.\nVMware Cloud Director Service for Google Cloud VMware Engine Deployment Guide\n"},{"id":16,"href":"/docs/developer-ready-cloud/implementation/","title":"Developer Ready Cloud Implementation","section":"Developer Ready Cloud","content":"Developer Ready Cloud Implementation #  Coming soon!\n"},{"id":17,"href":"/docs/dr-migration/implementation/","title":"DR \u0026 Migration Implementation","section":"DR \u0026 Migration","content":"DR \u0026amp; Migration Implementation #  Setting Up VMware Cloud Director Availability at VCD cloud #  You can find all the configuration steps needed in the Setting up VMware Cloud Director Availability document.\nSetting Up VMware Cloud Director Availability at a GCVE SDDC #  You can find all the configuration steps needed in the Setting up VMware Cloud Director Availability for Cloud Director service with a GCVE SDDC attached document.\nSetting Up VMware Cloud Director Availability at a VMC on AWS SDDC #  You can find all the configuration steps needed in the Setting up VMware Cloud Director Availability for Cloud Director service with a VMC on AWS SDDC attached document.\nUpgrading VMware Cloud Director Availability #  Coming soon!\n"},{"id":18,"href":"/docs/networking-security/implementation/","title":"Networking \u0026 Security Implementation","section":"Networking \u0026 Security","content":"Networking \u0026amp; Security Implementation #  Configuring VMware Cloud Director service for Load Balancing as a Service #  See the instructions below on how to deploy and configure VMware NSX Advanced Load Balancer in combination with VMware Cloud Director to provide Load Balancing as a Service capabitilies for end users.\n Load Balancing as a Service in VMware Cloud Director  "},{"id":19,"href":"/docs/sovereign-cloud/implementation/","title":"Sovereign Cloud Implementation","section":"Sovereign Cloud","content":"Sovereign Cloud Implementation #  Coming soon!\n"},{"id":20,"href":"/docs/cloud-infrastructure/operations/","title":"Cloud Infrastructure Operations","section":"Cloud Infrastructure","content":"Cloud Infrastructure Operations #  How to start metering your VCPP products with Usage Meter #  The following information helps you understand how Usage Meter detects and meters the usage of the listed VMware products and their features.\nIt also provides insight into how the reported usage data is calculated and appears in the Usage Insight reports.\nvCenter Server\nVMware Cloud Foundation\nvSAN\nTanzu\nSRM\nNSX\nCloud Director\nCloud Director Availability\nvRealize Operations\nvRealize Network Insight\nvRealize Automation\nHorizon\nHorizon DaaS\n"},{"id":21,"href":"/docs/developer-ready-cloud/operations/","title":"Developer Ready Cloud Operations","section":"Developer Ready Cloud","content":"Developer Ready Cloud Operations #  API Tokens in Cloud Director #  VMware Cloud Director 10.3.1 introduced API Tokens. This allows a user to generate API tokens for programmatic access to VCD. It works for both, provider, and tenant users. An automation script or 3rd-party solution can then use the API token to make API requests to Cloud Director on behalf of the user.\nThese steps are used to create API tokens:\n The provider propagates the right to use and manage API token to the tenant The Cloud Director user (provider as well as tenant user) creates an API token An API client (e.g. an automation script) uses the API token to make requests (If needed) The user revokes the API token  Preparation #  As for most new features, fine-grained access control through rights bundles is possible. To enable a tenant to use API token, the provider must publish a rights bundle to the tenant. Privileges can be defined for a user to manage the user’s own tokens, and to manage all Organization user’s token (for example for an Organization Administrator).\nCreate the API Token #  As provider or tenant user with proper privileges you can use the “User Preferences” menu to create the API token. Each token can be labeled with a name. Be aware that the actual token key is only visible once in the creation wizard and cannot be retrieved afterwards.\nRevoke the API Token #  API tokens do not expire, but existing API tokens can be revoked. This also invalidates active API client sessions that used the token to authenticate. All users can revoke their own tokens. Administrators (those with \u0026lsquo;manage all user\u0026rsquo;s API tokens\u0026rsquo; right) can revoke other user\u0026rsquo;s tokens. Tenant administrators can do so within their own Organization, while system administrators can do so for any user.\nUse the API Token #  Semantically the API token usage follows the OAuth 2.0 specification (RFC 6749, section 6).\nThe API token can then be used by a 3rd-party solution or custom API client to access the VCD API as the user, without the need to authenticate with username and password credentials.\nAPI Client Example #  Request the bearer token for subsequent calls using the API token:\nPOST https://host_name/oauth/provider/token Accept: application/json Content-Type: application/x-www-form-urlencoded Body: grant\\_type=refresh\\_token\u0026amp;refresh\\_token=Generated\\_API_Token  Security note: It’s recommended to send the API Token as part of the request body (and not as part of the URL, even if that technically works for x-www-form-urlencoded type requests), to avoid it being logged in transit.\nResponse containing the Bearer token:\nHTTP/1.1 200 OK Content-Type: application/json Body: { \u0026quot;access\\_token\u0026quot;:\u0026quot;Generated\\_Access_Token\u0026quot;, \u0026quot;token_type\u0026quot;:\u0026quot;Bearer\u0026quot;, \u0026quot;expires_in\u0026quot;:86400, \u0026quot;refresh_token\u0026quot;:null }  Subsequent API call now can use the returned Bearer token in the \u0026ldquo;access_token\u0026rdquo; field as usual. There is no need for any changes in the client code.\nSubsequent Call using the Bearer token:\nGET https://host_name/api/org Accept: application/*+xml;version=36.1 Authorization: Bearer Generated\\_Access\\_Token  Notes #  The session expiration can be configured in the provider portal under General \u0026gt; Timeouts.\nThe VCD Provider for Terraform for example supports API Token authentication as of version 3.5:\nprovider \u0026quot;vcd\u0026quot; { user = \u0026quot;none\u0026quot; password = \u0026quot;none\u0026quot; auth\\_type = \u0026quot;api\\_token\u0026quot; api_token = Generated API token sysorg = \u0026quot;System\u0026quot; ...  For security reasons, certain tasks are not possible when authenticated through an API token:\n Change the user password Perform user management tasks Create more tokens View or revoke other tokens  When accessing VMware Cloud Director by using an API access token, applications have only view rights for the following resources.\n User Group Roles Global roles Rights bundles  The API Token feature in VMware Cloud Director offers a secure way for automation solutions to access its API, even in environments that enforce Multi-factor Authentication for user logins.\nReferences #   VMware Cloud Director Tenant Portal Documentation VMware Cloud Director Provider Portal Documentation OAuth 2.0 Specification (RFC 6749) VCD Provider for Terraform Documentation  "},{"id":22,"href":"/docs/dr-migration/operations/","title":"DR \u0026 Migration Operations","section":"DR \u0026 Migration","content":"DR \u0026amp; Migration Operations #  VMware Cloud Director Availability Multi-NIC Configuration #  Below you can find the full instructions how to properly configure multiple network interfaces on each of the VMware Cloud Director Availability appliances - Cloud Replications Management, Cloud Replicator and Cloud Tunnel.\nMulti-NIC Configuration Instructions\nVMware Cloud Director Availability Certificate Management #  Below you can find the full instructions how to maintain the VMware Cloud Director Availability certificates.\nCertificate Management Instructions\n"},{"id":23,"href":"/docs/networking-security/operations/","title":"Networking \u0026 Security Operations","section":"Networking \u0026 Security","content":"Networking \u0026amp; Security Operations #  Coming soon!\n"},{"id":24,"href":"/docs/sovereign-cloud/operations/","title":"Sovereign Cloud Operations","section":"Sovereign Cloud","content":"Sovereign Cloud Operations #  Coming soon!\n"},{"id":25,"href":"/docs/cloud-infrastructure/cloud-director-availability-metering/","title":"Cloud Director Availability Metering and Reporting","section":"Cloud Infrastructure","content":"Cloud Director Availability Metering and Reporting #  Usage Meter 4.5.0.1 can detect configured for protection or migration virtual machines using VMware Cloud Director Availability, where the source or destination is a replication-enabled VMware Cloud Director.\nConfiguration #  VMware Cloud Director Availability Management Appliance instance must be registered with Usage Meter by providing the appliance IP address or hostname and root credentials.\nFeature Detection #  Usage Meter collects the number of all replications (migrations and protections) during the month. The collection is performed on an hourly basis. Replications for both powered-on and powered-off VMs are counted. At the end of the month, the report shows the number of all the unique migrated VMs and all the unique protected VMs. Replications are reported in the following cases:\n Cloud-to-Cloud migrations and protection, where the registered VMware Cloud Director Availability is the destination vCenter-to-Cloud migrations and protection, where the registered VMware Cloud Director Availability is the destination Cloud-to-vCenter migrations and protections where the registered VMware Cloud Director Availability is the source  When Usage Meter fails to connect to a registered VMware Cloud Director Availability Management Appliance instance, the following actions are taken:\n The failure is logged in the Usage Meter log files An error message is shown on the Notifications page in the Usage Meter web application An email is sent to the defined Usage Meter admin email account If the metering fails due to VMware Cloud Director Availability not being available or Usage Meter is not functioning, Usage Meter will report the last known \u0026ldquo;Protected VMs\u0026rdquo; count. This can lead to some replications not being reported in the total number of unique replications for the month.  If there is no successful collection during the month, VMware Cloud Director Availability usage will not be reported.\nReporting #  A sample Monthly Usage Report for VMware Cloud Director Availability can be seen below.\n   Product Unit of Measure Units to be Reported     VMware Cloud Director Availability Cloud-to-Cloud Migrations Protected VMs 100   VMware Cloud Director Availability Cloud-to-Cloud Protections Protected VMs 200   VMware Cloud Director Availability Cloud-to-vCenter Migrations Protected VMs 75   VMware Cloud Director Availability Cloud-to-vCenter Protections Protected VMs 105   VMware Cloud Director Availability vCenter-to-Cloud Migrations Protected VMs 46   VMware Cloud Director Availability vCenter-to-Cloud Protections Protected VMs 27    "},{"id":26,"href":"/docs/cloud-infrastructure/cloud-director-metering/","title":"Cloud Director Metering and Reporting","section":"Cloud Infrastructure","content":"Cloud Director Metering and Reporting #  Configuration #  Usage meter collects product usage information from all VMware Cloud Director endpoints that are registered with Usage Meter. The Usage Meter administrator must configure the endpoint and credentials through the Usage Meter web application.\nFeature Detection #  Usage Meter calculates an Average Capped Billed vRAM (GB) value for all virtual machines managed by Cloud Director. Refer to the vCenter Server section for a detailed description of the Average Capped Billed vRAM formula. The different reservation models in VMware Cloud Director are only relevant if memory reservations are applied directly to the VM, not on resource pool level. When Usage Meter fails to connect to a registered Cloud Director instance, the following actions are taken:\n The failure is logged in the Usage Meter log An indication of the failure is shown on the Notifications page in the Usage Meter web application  "},{"id":27,"href":"/docs/cloud-infrastructure/horizon-daas-metering/","title":"Horizon DaaS Metering and Reporting","section":"Cloud Infrastructure","content":"Horizon DaaS Metering and Reporting #  Product Detection #  For setting up VMware Horizon DaaS usage collection, for every Horizon DaaS deployment (each unique Horizon DaaS Service Provider implementation), the provider needs to register it in the Usage Meter 4.5.0.1 web application.\nTo set up the Horizon Desktop as a Service usage collection, the provider selects Horizon Desktop as Service in the Usage Meter web application, adds a new entry, and then enters the Horizon DaaS appliance IP address or hostname, port, username, password, and active directory domain. Validation of the user credentials is performed, and if correct, the data collection starts. The Usage Meter collector connects to the Horizon DaaS Service Provider appliance and collects license data. Usage metrics are collected every 6 hours.\nMetering #  For every reporting period, Usage Meter collects the maximum number of Desktop VMs under management for VDI and the maximum number of sessions under management for RDSH.\nReporting #  Sample Monthly Usage Report for Horizon Desktop as a Service\n   Product Hostname Version VC UUID Unit of Measure Units to be Reported     VMware Horizon DaaS Bundle - VDI Edition    Maximum Number of Desktop VMs under management for VDI 10   VMware Horizon DaaS Bundle - RDSH Edition    Maximum Number of Sessions under management for RDSH 80    "},{"id":28,"href":"/docs/cloud-infrastructure/horizon-metering/","title":"Horizon Metering and Reporting","section":"Cloud Infrastructure","content":"Horizon Metering and Reporting #  Product Detection #  Horizon is added for metering in Usage Meter 4.5.0.1. The provider needs to enter the endpoint IP address or hostname, user and password, and the domain to which Horizon belongs. After the product certificate is accepted in the Usage Meter web application and the connection to Horizon is successful, the data collection starts.\nMetering #  Usage Meter 4.5.0.1 collects data for the number of concurrent connections from the Horizon Connection Server and the license with which the concurrent connections are associated. Usage Meter 4.5.0.1 collects data every 1 hour and sends it to vCloud Usage Insight. Usage Meter stores in-memory the last received number of concurrent connections. It compares the latest collected data with the in-memory data and pushes the value to vCloud Usage Insight if it has changed. The Horizon usage is calculated based on the maximum number of concurrent connections per license edition for the month. If Horizon is not licensed or not reachable, Usage Meter 4.5.0.1 will generate a warning message in the Usage Meter Notifications tab.\nReporting #  Horizon usage is reported per license edition. The following license editions are reported by Usage Meter 4.5.0.1:\n VMware Horizon Standard VMware Horizon Advanced VMware Horizon Enterprise VMware Horizon Apps Standard VMware Horizon Apps Advanced  Sample Monthly Usage Report for Horizon\n   Product Hostname Version VC UUID Unit of Measure Units to be Reported     VMware Horizon Standard    Concurrent Connection 15   VMware Horizon Enterprise    Concurrent Connection 20    "},{"id":29,"href":"/docs/networking-security/vcd-lbaas-implementation/","title":"Load Balancing as a Service in VMware Cloud Director","section":"Networking \u0026 Security","content":"Load Balancing as a Service in VMware Cloud Director #  Introduction #  This document is intended for VCPP Cloud Providers who are interested in providing Load Balancing as a Service (LBaaS) in their multi-tenant environments managed by VMware Cloud Director (VCD).\nFrom VMware Cloud Director 10.2 and onwards, VMware Cloud Director provides load balancing services in NSX-T backed organization virtual data centers (VDCs) by leveraging the capabilities of VMware NSX Advanced Load Balancer (Avi).\nThe content below describes the deployment and configuration procedures and also clearly delineates the cloud provider actions from the actions of the tenant, addressing both self-service and managed service offerings that are possible.\nVMware NSX Advanced Load Balancer (Avi) provides multi-cloud load balancing, web application firewall and application analytics across on-premises data centers and any cloud. The software-defined platform delivers applications consistently across bare metal servers, virtual machines and containers to ensure a fast, scalable, and secure application experience.\nLBaaS Anatomy #  The NSX Advanced Load Balancer Platform (Avi) is architected on software-defined principles, decoupling the data and control planes. As a result, it centrally manages and dynamically provisions pools of application services, including load balancing, across multi cloud environments.\nArchitecturally, the Platform comprises three core elements:\n The Avi Controller - provides central control and management of the Avi Service Engines. It orchestrates policy-driven application services, monitors real-time application performance (leveraging data provided by the Avi Service Engines), and provides for predictive auto-scaling of load balancing and other application services. Furthermore, it is capable of delivering per-tenant or per-application load balancing — increasingly in demand in multi cloud contexts — and also facilitates troubleshooting with traffic analytics. The Avi Service Engines (SEs) - distributed software that runs on bare metal servers, virtual machines, and containers. They implement application services across on-premises datacenters, colocation datacenters, and public clouds. They also collect data relating to application performance, security, and clients. As distributed software, Avi Service Engines are capable of horizontal auto-scaling within minutes while functioning as service proxies for micro services. The Avi Console - provides web-based administration and monitoring. It is a web server running on the controller and offers a UI for configuration of application services, delivers visualization of network configurations and virtual IPs (VIPs), and displays application health scores and transaction round-trip times. It’s also where customers can view performance, security, and client insights, as well as where they can view service interactions.  VMware Cloud Director Support #  Starting with version 10.2, VMware Cloud Director provides load-balancing services by using the capabilities of VMware NSX Advanced Load Balancer (Avi). VMware Cloud Director supports L4 and L7 load balancing that you can configure on an NSX-T Data Center edge gateway.\nAs a system administrator, you deploy the NSX Advanced Load Balancer controller cluster with the other management solutions in the management infrastructure.\nThe NSX Advanced Load Balancer Controller uses APIs to interface with NSX Manager and vCenter Server to discover the infrastructure. It also manages the lifecycle and network configuration of Service Engines (SE). The Avi Controller cluster uploads the SE OVA image to the vCenter Server content library and uses vCenter APIs to deploy the SE VMs.\nThis integration happens through an NSX-T Cloud configured in NSX ALB before being imported into VMware Cloud Director.\nLoad balancing services are associated with NSX-T edge gateways, which can be scoped to an organization VDC or a data center Group.\n​The system administrator has the flexibility to decide whether a service engine group is dedicated to a single edge gateway or shared between several edge gateways.\nTenant users have full self-service UI and API load balancing capabilities in VMware Cloud Director.\nA service engine node is a VM with up to 10 network interfaces (NICs). The first NIC is always used for the management and control traffic. The other nine are used to connect to the NSX-T edge gateway (tier-1 gateway) using a service network logical segment. The service networks are created by VMware Cloud Director when you enable the load balancing service on an edge gateway with the DHCP service to provide IP addresses for the attached SEs.\nBy default, the IP address used is 10.255.255.0/25 subnet. The system administrator can change the IP address if it coincides with the existing organization\u0026rsquo;s VDC networks.\nService Engines run each service interface in a different VRF. As a result, IP conflicts or cross-tenant communication does not occur. Avi automatically picks a service engine to instantiate the load balancing service when the tenant configures a load balancing pool and virtual service.\nWhen an SE is assigned, Avi configures a static route (/32) on the organization VDC edge gateway pointing the virtual service VIP (virtual IP) to the service engine IP address from the tenant\u0026rsquo;s load balancing service network.\nProvider and Tenant Responsibilities #  As a system administrator, you deploy an Avi Controller Cluster, complete the initial configuration and the association with NSX-T and VMware Cloud Director. Once the integration is ready, you deploy and enable load balancing on an NSX-T edge gateway and assign it a service engine group.\nAn organization administrator creates load balancer server pools and virtual services.\nDeployment \u0026amp; Configuration #  Requirements #  Please find the full list of requirements in the planning and preparation page.\nDeploying the Avi Controller Cluster #  To ensure complete system redundancy, the Avi Controller must be highly available. Three Avi Controller VMs will form a highly available control plane for the NSX Advanced Load Balancer.   Download the Controller OVA from my.vmware.com portal. Follow this KB article to download the Controller OVA image. Log into the vCenter server through a vSphere Client and deploy a first Avi Controller: Follow the Deploy OVA Template wizard instructions:  Choose a port group for Destination Network in Network Mapping. This port group is the management network for the Controller and will be used for all management communication (e.g., Avi Controller communication with vCenter). Specify the management IP address and default gateway (only static IP addresses should be used in a production environment). The \u0026lsquo;Sysadmin login authentication\u0026rsquo; key is used to specify an SSH public key and is NOT required.   Repeat the last steps to create two additional Avi Controllers to be used to form a three-node Controller cluster which will form the control plane for the NSX Advanced Load Balancer. Create an anti-affinity \u0026lsquo;VM/Host\u0026rsquo; rule to make sure Controller VMs are placed on separate hosts. Power on Controller VMs.  Just like any other infrastructure management system, CPU and memory should be 100% reserved on Avi controllers.  Avi Controller Cluster Initial Setup #  This section shows the steps to perform initial configuration of the Avi Controller using its deployment wizard. You can change or customize settings following initial deployment using the Avi Controller’s web interface.\nConfigure NSX Advanced Load Balancer Controller cluster to provide a highly available control plane for the NSX Advanced Load Balancer.\n Initialize the first NSX Advanced Load Balancer Controller VM  In a web browser, navigate to the first controller IP or FQDN. Note: While the system is booting up, a 503 status code or a page with following message will appear: \u0026ldquo;Controller is not yet ready. Please try again after a couple of minutes\u0026rdquo;. Wait for about 5 to 10 minutes and refresh the page.   Once the NSX Advanced Load Balancer welcome screen appears, create an admin account. Complete the remaining steps by configuring all required parameters (DNS, NTP, SMTP, etc.). Select No Orchestrator in the Orchestrator Integration page. Leave the Tenant Settings configured by default.   Configure an NSX Advanced Load Balancer Controller cluster  Navigate to Administration \u0026gt; Controller and select Edit. Specify the Name and the Controller Cluster IP. Add the details for each of the three NSX Advanced Load Balancer Controller nodes.  Click on Save. It will take a few minutes for the services to restart and the Controller cluster to be up. In a web browser, log in to the Controller cluster VIP. Navigate to Administration \u0026gt; Controller and ensure all the Controllers show State as Active which represents a healthy Controller cluster. Setup licensing in Administration \u0026gt; Settings \u0026gt; Licensing. Basic or Enterprise licenses are set at the controller cluster level. You cannot mix both licenses in a single Avi Controller cluster instance.   Finish the Controller Cluster configuration (alerting, backup, etc.). Note: the full configuration of the Avi Controllers general settings is outside the scope of this document.    VMware Cloud Director integration with NSX Advanced Load Balancer fails if the default self-signed certificate is used.\nBy default, the Controller cluster Portal will be setup with a self-signed certificate which does not have a valid SAN (Subject Alternative Name) and makes the integration with VMware Cloud Director impossible. VMware Cloud Director will reject any URL that does not match the values present in the certificate, which is to conform with industry standard security guidelines and our platform criteria.\n Setup of Avi Controller cluster Portal certificate is outside the scope of this document.\nAdditional resources:\n Deploying an Avi Controller Cluster Avi SSL/TLS Certificates  NSX-T Cloud #  The point of integration in Avi, with any infrastructure, is named a cloud. For NSX-T environment, an NSX-T cloud has to be configured.\nAn NSX-T cloud is defined by an NSX-T manager and a transport zone. If an NSX-T manager has multiple transport zones, each will map to a new NSX-T cloud. To manage load balancing for multiple NSX-T environments each NSX-T manager will map to a new NSX-T cloud.\nNSX-T cloud general considerations:\n An NSX-T cloud has a one-to-one relationship with a network pool backed by an NSX-T transport zone. DHCP checkbox: the Service Engines are expected to get an IP via DHCP on the management subnet  To create an NSX-T cloud, log in in to the Avi Controller and:\n Navigate to Infrastructure \u0026gt; Clouds. Click on Create and select NSX-T Cloud.  Following the general parameters, the NSX-T section allows to configure the future service engines network configuration:\n The transport zone must match the overlay transport zone configured in the network pool in VMware Cloud Director. The NSX-T cloud requires two types of network configurations:  Management Network: the tier-1 logical router and overlay segment to be used for management connectivity of the service engine VMs has to be selected. The first vNic of each service engines will be connected to that management network (which must be created upfront, with DHCP enabled). Data Network: although not required for the VMware Cloud Director integration, it is required to set a dummy data network to avoid having the NSX-T cloud object in a degraded state.    VMware Cloud Director will automatically complete the data network tier-1 logical routers and segments when load balancing is enabled on tier-1 acting as NSX-T edge gateways.   Each NSX-T cloud can have one or more vCenters associated to it. vCenter objects must be configured on Avi for all the vCenter compute managers added to the NSX-T that has ESXi hosts that belong to the transport zone configured in the NSX-T cloud.  Additional resources:\n Multiple vCenters with NSX-T Cloud  Service Engine Groups #  A service engine group is an isolation domain that also defines service engine node sizing (CPU, memory, storage), bandwidth restrictions, availability modes, and network access.\nResources in a service engine group can be used for different virtual services, depending on your tenant\u0026rsquo;s needs.\nTo create a service engine group, log in in to the Avi Controller and:\n Navigate to Infrastructure \u0026gt; Cloud Resources \u0026gt; Service Engine Group. Using the \u0026ldquo;Select Cloud\u0026rdquo; drop down menu, select the relevant NSX-T cloud. Click on Create.  The basic settings page allows to configure the high availability mode, the service engine capacity and limit settings, as well as other advanced parameters. The official Avi documentation can help to size appropriately the service engines in terms of CPU, memory and disk.\nService engines VMs may be automatically deployed on any host and storage that most closely matches the resources and reachability criteria for placement.\nStarting in Avi 20.1.3 and onwards, it is possible to tailor the service engine placement in terms of:\n vSphere folder vSphere hosts and cluster vSphere datastore  You may create multiple service engine groups depending on your tenants requirements for high availability, placement or performances.  Additional resources:\n Sizing Service Engines  VMware Cloud Director Service Admin Portal #  After the NSX Advanced Load Balancer deployment and configuration with the NSX-T infrastructure, the next step is to register the controller cluster with VMware Cloud Director.\nTo provide virtual service management capabilities to your tenants:\n Register your Avi Controller instances with your VMware Cloud Director instance. Register your NSX-T Cloud instances with VMware Cloud Director. Import all the relevant service engine groups to your VMware Cloud Director deployment.  Consumption #  Enabling Load Balancing #  Before an organization administrator can configure load balancing services, a system administrator must enable the load balancer on the NSX-T edge gateway and assign at least one service engine group to it.\n Navigate to the NSX-T edge gateway on which you want to enable load balancing. Under Load Balancer, click General Settings. Click Edit and enable the Load Balancer function for this particular edge. (optional) Enter a network CIDR for a service network subnet.  The service network is an internal construct; as such, it is not exposed to the tenant. Only change the default specification (192.168.255.1/25) if it overlaps with an existing organization VDC network.  Once load balancing is enabled, the next step is to manage the service engine group assignment on the edge gateway.\n Under Load Balancer, click Service Engine Groups. Select an available service engine group from the list. For shared service engine groups, the system administrator must set the maximum and reserved number of virtual services that can be placed on the edge gateway (within the capacity of the service engine group)  Design considerations:\n A system administrator can assign one or more service engine groups to an NSX-T Data Center edge gateway. All service engine groups that are assigned to a single edge gateway use the same service network.  Load Balancer Server Pool #  After a system administrator assigns a service engine group to an edge gateway, an organization administrator can create and configure virtual services that run in a specific service engine group.\nThe heart of a load balancer is its ability to effectively distribute traffic across healthy servers. A server pool is a group of one or more servers that you configure to run the same application and to provide high availability.\nIf persistence is enabled, only the first connection from a client is load balanced. While the persistence remains in effect, subsequent connections or requests from a client are directed to the same server.\n Under Load Balancer, click Pools, and then click Add. Configure the general settings for the load balancer pool.  Add members to the server pool.  Note: pool health status and pool member health status will remain Down until a virtual service is created and service engines are deployed.  Additional resources:\n Add a Load Balancer Server Pool  Virtual Service #  A virtual service listens for traffic to an IP address, processes client requests, and directs valid requests to a member of the load balancer server pool.\nA virtual service is a combination of an IP address and a port that uses a single network protocol. The virtual service is advertised to outside networks and is listening for client requests. When a client connects to the virtual service, the load balancer directs the request to a member of the configured load balancer server pool.\nTo secure SSL termination for a virtual service, you can use a certificate from the certificate library. For more information, see Import Certificates to the Certificates Library.\n Under Load Balancer, click Virtual Services, and then click Add. Configure the general settings for the load balancer pool. Enter a meaningful name and, optionally, a description, for the virtual service. To activate the virtual service upon creation, toggle on the Enabled option. Select a service engine group for the virtual service. Select a load balancer pool for the virtual service. Enter an IP address for the virtual service. Select the virtual service type.  The Virtual IP (VIP) can be any arbitrary IPv4 address. The VIP can be a routable external IP address allocated to the organization VDC edge gateway or any internal routed address:\n An external organization VDC edge gateway allocated IP address; no DNAT is required, but you cannot use this IP for NAT anymore due do the internal packet processing. An arbitrary internal IP (DNAT required). In that situation, the VIP must not coincide with any existing organization VDC networks or with the load balancer service network.  A static route will be automatically created on the Tier-1 from the VIP to the relevant service engine node IP.\n Additional resources:\n Create a Virtual Service  Health Monitoring #  VMware Cloud Director manages the health of the following load balancing components:\n NSX-T Cloud Virtual services Server pools  VMware Cloud Director provides basic monitoring and metrics about the virtual services and pools to both providers and tenant administrators.\nProviders can see the basic usage metrics for each service engine groups deployments (number of applications, usage, and running SE engines).\nTenants can see some basic metrics about each virtual service (up or down and traffic). Analytics is only available if the controller are imported with Enterprise License.\nAdvanced Features Consumption #  Although some advanced features are not exposed in VMware Cloud Director, they can be provided as a managed service. This includes (but is not limited to) Web Application Firewall (WAF) or custom Health Monitors.\nAvi Intelligent Web Application Firewall #  Web application firewalls (WAFs) are intended to protect businesses from web app attacks and proactively prevent threats. Traditional web application security solutions do not provide visibility and security insights that administrators can use to create an effective application security posture. Enterprises need real-time visibility into application traffic, user experience, security and threat landscape, and application performance to identify and protect against the most sophisticated attacks.\nAvi leverages software-defined architecture and its strategic location on the network to gain real-time application insights. The built-in WAF solution provides application security and networking teams with an elastic and analytics-driven solution that scales and simplifies policy customization and administration through central management.\nAvi intelligent WAF (iWAF) plays an integral role in a defense-in-depth strategy that does comprehensive threat analysis, mitigates risk, provides zero-day protection against unpublished exploits and optimizes application security.\nAs of today, Avi iWAF capabilities are not exposed in VMware Cloud Director for self-service configuration and consumption. However, a system administrator can assign WAF policies to existing virtual services.\nAdditional resources:\n Avi Intelligent Web Application Firewall  Custom Health Monitor #  Avi validates if the backend servers are functioning efficiently by sending active health monitors on a periodic basis. Avi Vantage also tests if they can accommodate additional workloads before load balancing a client to a server. Health monitors originate from the service engines assigned to the application\u0026rsquo;s virtual service. The health monitors are attached to the pool for the virtual service.\nA pool may have multiple actively concurrent health monitors (such as Ping, TCP, and HTTP), as well as a passive monitor. All active health monitors must be successful for the server to be marked up.\nWhen configuring a server pool from the tenant portal, a tenant administrator can choose between 5 health monitors: HTTP, HTTPS, TCP, UDP and PING. However, a system administrator can create and customize advanced health monitors from the Avi UI, and assign them to existing server pool.\nOnce the additional health monitor is associated with the server pool, it will appear in the tenant portal: a tenant administrator can remove it but not add it in self-service.\nResources #   NSX Advanced Load Balancer Integration with NSX-T NSX-T Support in NSX Advanced Load Balancer  "},{"id":30,"href":"/docs/networking-security/vcd-lbaas-design/","title":"Load Balancing as a Service in VMware Cloud Director Design","section":"Networking \u0026 Security","content":"(this page is here as a placeholder)\nClouds are containers for the environment that NSX Advanced Load Balancer is installed. During the initial setup of the Avi Networks platform, a default cloud named Default-Cloud is created. The first controller is deployed in Default-Cloud. You can add clouds containing SEs and virtual services. The network adapter 1 of the service engine VM is reserved for management connectivity and the remaining nine data interfaces (network adapter 2 to 10) for the service engine VM to the VIP or data segment.\nAvi controller deployed and managed by provider SEs are outside of tenant domains Service Engine Groups can be dedicated or shared. It is a provider decision on how to manage this.\nDedicated: If a tenant wants to pay to have dedicated LB compute (for throughput reasons) Shared: Normal model for providers to leverage multiple edges and potentially multiple tenants on the same compute resources\nNote: the shared VS dedicated is more about how the provider wants to manage how the tenants uses which SEGs As we must follow Basic licensing, Service Engine Groups configuration need to be configured in Legacy HA (Active/Standby) Mode This impact the number of Virtual Services per Service Engine Group\nDesign considerations\n NSX Advanced Load Balancer supports load balancing only in an NSX-T Data Center transport zone of type overlay.  The network adapter 1 of the Service Engine VM is reserved for management connectivity. You can connect only one of the remaining nine data interfaces (network adapter 2-10) of the Service Engine VM to the VIP or data segment. The other interfaces must be left disconnected. The service engines are deployed in one arm mode. The same interface is used for the client and backend server traffic. The SE routes to backend servers through the Tier 1 router. The SEs on a dedicated logical segment: • Allow to manage the IP address assignment separately for SE interfaces. • In the current version, this segment must be created on NSX-T Data Center before adding it to the cloud configuration on NSX Advanced Load Balancer. Only logical segments connected to the tier-1 router are supported. The cloud automation for NSX-T Data Center integration does not support placing SEs on logical segments directly connected to tier-0 routers.\nThe Controller cluster VMs can be deployed adjacent to the NSX Manager for NSX-T Data Center, connected to the management port group: • Dedicated tier-1 gateway and logical segment for the SE management. • Management IP address of the SEs must be reachable from controller. • Tier-0 must advertise the learned routes to the external router using BGP.\nVCD will prevent you to use more that one Avi NSX-T Cloud per VCD Geneve network pool. But you can have (in theory) multiple Geneve TZs on the same vSphere cluster, but that means separate physical NICS for each TZ Avi: You can\u0026rsquo;t share a transport zone, so you need a different pVDC when you have different Avi controllers with different transport zone \u0026gt; because VCD check the TZ match the NP during import\nIDEAS / PLACEHOLDER #   have a link to what\u0026rsquo;s next (design, prep or implementation)\n  show design options for where to host/connect the controllers vs the SEs  https://vmware-wwcp.screenstepslive.com/m/100574/l/1333937-nsx-advance-load-balancer https://avinetworks.com/docs/20.1/nsx-t-design-guide/#nsx-t-cloud-configuration-model\n"},{"id":31,"href":"/docs/cloud-infrastructure/nsx-metering/","title":"NSX Metering and Reporting","section":"Cloud Infrastructure","content":"NSX Metering and Reporting #  Usage Meter 4.5.0.1 can detect specific NSX features available to Virtual Machines, which determines the NSX edition billed to a VM.\nUsage Meter 4.5.0.1 tracks NSX usage by Virtual Machine based on the networking services available to the Virtual Machine.\nConfiguration #  The Usage Meter administrator must configure the NSX Manager endpoint and credentials using the Usage Meter web application. There is one NSX Manager for each vCenter server instance utilizing NSX.\nAll virtual machines deployed to an NSX prepared host are candidates for NSX usage. The administrator needs to consider partitioning tenant VMs onto networks depending on their NSX usage. To avoid NSX metering for Virtual Machines that are not utilizing NSX, consider deploying the VMs to a vCenter cluster that is not prepared for NSX. The figure below shows two different vCenter clusters within the same vCenter domain. Virtual machines that are not utilizing NSX should be deployed to the cluster that is not prepared for NSX.\nFeature Detection #  The fact that a virtual machine can potentially access a service through the network will result in the VM being metered for the service. A VM will be considered as using NSX for metering purposes if:\n The virtual machine is connected to a network (backed by any type of switch) with access to an NSX edge. The virtual machine is connected to an NSX logical switch or distributed switch on an NSX prepared host. The virtual machine is referenced by a non-default distributed firewall rule, including groups/policies.  Note: The default DFW rule is the ‘Allow’ action.\n The virtual machine is connected to a network (backed by any type of switch) with access to a distributed logical router. If a virtual machine is connected to an Edge service through the network, the VM will be metered for the service.  Usage Meter examines the switches, routers, and gateways managed by the NSX Manager, and creates a graph of connected networks. It then determines the VMs that are connected to each switch. VMs are metered for NSX usage based on their ability to reach a gateway or router through the network. Usage Meter does not examine individual Virtual Machines, network traffic, or routing tables to determine actual usage.\nUsage Meter examines the list of NSX features available to a Virtual Machine and selects the minimum license needed to enable the features. Each VM is metered based on the NSX edition and the VMs vRAM configuration. The following table lists the NSX components examined by Usage Meter to determine available NSX-V features.\nNote: NSXFint is a column that appears in the Virtual Machine History report for VMs that are using NSX. NSXFint has an integer value and represents the NSX features used by a metered VM. The NSXFint value per NSX-v feature is shown in the table below. The NSXFInt value for each metered VM will be the integer value of the NSX feature or the integer sum of all NSX features used by that VM.\nFor example, a VM using the NSX Edge load balancing and Distributed firewalling features will have the following value in the NSXFInt column = 2+64=66.\nNote: Each NSX-v feature has a unique NSXFint value. However, the following NSX-v features are all reported as integer 1 and are essentially undistinguished: Distributed switching and routing, NSX Edge Firewall, NAT, Integration with vRealize, and OpenStack, VPN (IPSEC and SSL). These features are Base features, and if a VM uses a couple of them, then Usage Meter will only meter them as a single Base feature used by that VM. As a result, the sum of these NSX-v features will have an NSXFint value equal to 1.\nFor example, if a VM uses Distributed switching and routing and NSX Edge Firewall, this VM will only have a value of NSXFint=1.\nTo find the features used by a VM, subtract from the NSXFint value the highest possible value equal to or less than the NSXFint value, and continue the subtraction until NSXFint =0.\nFor example, let’s have a VM that has NSXFint = 9.\n We will check in the table below which NSX feature has a value equal to or less than 9 and subtract it from NSXFint. In this case, the highest number close to NSXFint =9 is 8, corresponding to Dynamic Routing with ECMP (Active-Active). The result of the subtraction is NSXFint - 8= 9-8=1. We will check again in the table what NSX feature has a value equal to 1. This feature is a Base feature and could be any of the following features: Distributed switching and routing, NAT, VPN (IPSEC \u0026amp; SSL), NSX Edge Firewall, Integration with vRealize and OpenStack. When we also subtract it from NSXFint, the NSXFint value will equal 0. In conclusion, NSXFint =9 is NSXFint=8+1, corresponding to ECMP dynamic routing and a Base feature.     NSX-V Feature NSX Feature String NSX-V Edition Flex NSXFint VMs declared as using this feature Provided By     Distributed switching and routing, NAT, VPN (IPSEC \u0026amp; SSL), NSX Edge firewall, Integration with vRealize and OpenStack BASE Base 1 All VMs that are connected to a Logical Switch. All VMs on all networks serviced by the edge. ESXi Hosts prepared Edge gateway   NSX Edge load balancing DLB Base 2 All VMs on all networks serviced by the edge. Edge gateway   Dynamic routing with ECMP ECMP Base 4 All VMs on all networks serviced by the edge. Edge gateway   Dynamic Routing with ECMP (Active-Active) DYNRT Base 8 All VMs on all networks serviced by the edge. Edge gateway   Software L2 bridging to physical environment SWL2 Base 16 All VMs on all networks serviced by the edge. SW L2 bridging to a physical environment   Distributed firewalling DFW Professional 64 All VMs referenced in the Source rule or Target rule sections of a firewall rule. Default Firewall rules excluded. Hosts prepared   Remote Gateway (also known as L2VPN) RemoteGW Professional 128 All VMs on all networks serviced by the edge. Edge gateway   Multi-Site NSX Optimizations Universal Advanced 256 All VMs on the ULS serviced by the edge. ULS, UDFW   Integration with HW VTEPs HWVTEP Advanced 512 All VMs on the LS bridged with HW VTEP. Integration with HW VTEPs   Service insertion (3rd party integration) DFW3 Advanced 1024 Same as Distributed Firewalling. Distributed firewalling   Active Directory Integrated firewall DFW_AD Advanced 2048 Same as Distributed Firewalling. Distributed firewalling   Context Aware firewall with Layer 7 DFWL7 Advanced 4096 All VMs on all networks serviced by the edge. Edge gateway   Server activity monitoring SAM Advanced 65536 Same as Distributed Firewalling. Distributed firewalling             The following table lists the combinations of NSX features available to a Virtual Machine and the resulting NSX edition that will be metered.\n   Is the host prepared for NSX-V? Is the VM connected to Logical Switch? At least one non-default rule is applicable to a VM Is NSX Edge available to a VM? NSX Edition metered Scenario in Diagram below     No n/a n/a No none    No No n/a Yes at least* Base Edition    Yes No No No none 1   Yes No No Yes at least* Base Edition+ 5   Yes No Yes No Advanced Edition    Yes No Yes Yes at least* Advanced Edition    Yes Yes No No Base Edition 1   Yes Yes No Yes at least* Base Edition 2, 3, 6   Yes Yes Yes No Advanced Edition    Yes Yes Yes Yes at least* Advanced Edition     *Depending upon the edge services configured. NSX-V Scenario Samples #  Scenario One: Minimum NSX configurations\nA virtual machine VM1 is deployed to a vSphere host on an NSX enabled cluster. The virtual machine is not connected to a switch. The VM is automatically connected to the NSX Distributed Firewall.\nMetering By default, the VM is connected to a distributed Firewall. VM1 will not be metered for NSX usage unless the default distributed firewall rules have been modified to reference the VM. If the DFW is in use using non-default rules, then the VM will be metered for NSX Advanced Edition.\nA virtual machine VM1 is deployed to a host on an NSX enabled cluster. The VM is connected to a switch. The VM is automatically connected to the NSX Distributed Firewall. The administrator has not modified the default DFW rules.\nMetering VM1 will be billed for NSX (Base Edition) since it is connected to a distributed switch.\nScenario Two: Base Edition Example\nTwo virtual machines VM1 and VM2 are connected to different VXLAN backed networks with routing through a Distributed Logical Router. Both virtual machines have access to a remote network through an Edge Gateway ESG, running a Firewall service. Neither of the VMs are referenced by non-default DFW rules.\nMetering VM1 and VM2 are both metered for NSX Base edition since both distributed switching and routing, and edge firewall are base edition features. **Scenario Three:**Active / Active Gateway\nDynamic routing with ECMP (Active-active) Edge\nMetering VM1, VM2, and VM3 are billed as NSX Base since ECMP is a Base feature, and all VMs have potential access to the edge. This scenario illustrates the feature selection based on the most advanced feature reachable by a Virtual Machine through the network.\nScenario Four: NSX Partitioned by Tenant\nTwo different Tenants A and B, each with their own network and ESG. No shared Switch, DLR or ESG.\nMetering VM1 is metered for NSX usage based on the services running on the ESG1 accessible to VM1. VM2 is metered for NSX usage based on the services running on the ESG2 accessible to VM2.\n Scenario Five: ESG on vSphere Distributed Switch (Not a logical Switch)\nVM1 on an NSX prepared Cluster and attached to vSphere Distributed Switch, not a Logical Switch. ESG running a load balancer service attached to the same vSphere Distributed Switch. VMs are using only default DFW rules.\nMetering VM1 is metered for NSX (Base Edition) since Edge Load balancing is a Base feature and is accessible to the VM through the distributed switch.\nScenario Six: NSX Cross vCenter shared ESG\nAn NSX Edge Gateway, ESG shared between vCenter Domains using a Universal Logical Switch. Both virtual machines are using only default DFW rules.\nMetering VM1 and VM2 are both billed as NSX Enterprise since the ESG connects to a Universal Logical Switch, and all VMs have access to the ESG.\nNSX Reporting #  NSX usage can be included in reports as a license edition line item or as a standalone line item. Usage reporting is based on editions unless standalone reporting is requested. NSX-V Q\u0026amp;A\nQ1: What are the minimum configurations that Usage Meter will detect as NSX usage?\n A VM connected to a Logical Switch will be detected as an NSX BASE edition. This is the case even when no distributed logical routers or NSX edges are created. A VM with a vNIC referenced in a distributed firewall rule (non-default) will cause the VM to be detected as NSX Advance, even if the VM is not connected to a logical switch, distributed logical router, or edge.  Q2: Are the Virtual Machines that implement NSX functionality (for example, NSX Controller VMs and Edge Gateways) billed for NSX usage?\n Yes, NSX management VMs are billed for NSX Base usage. The only VM excluded from metering is the Usage Meter appliance.  Q3: An NSX Edge Gateway can be installed on a vCenter cluster that is not prepared for NSX. In this case, is the NSX Edge and the Virtual Machines utilizing the edge billed for NSX usage?\n Installation of an edge gateway on unprepared servers is supported only when the Edge is running the L2VPN Client. It is assumed that the client hosting the gateway will not be metered by Usage Meter.  Q4: Does Usage Meter infer the content of a distributed firewall rule? For example, if two rules cancel out one another.\n No, Usage Meter does not examine the relationships between firewall rules. Usage Meter will meter the VM for DFW usage if any of the DFW rules in a policy reference the vNIC either directly or through a security group (static or dynamic).  Q5: If a VM is connected to multiple networks, what rate is metered at?\n The VM is metered at the rate of the network with the highest level of service features.  Q6: If a VM is listed in the NSX Firewall exclusion list, does Usage Meter bill for Firewall usage?\n No, Usage Meter does not bill DFW usage for VMs in the NSX firewall exclusion list.  Q7: On an Edge configured as a Remote Gateway (also known as L2VPN), what VM’s are metered for NSX Usage?\n L2VPN can be configured as \u0026ldquo;Server\u0026rdquo; and \u0026ldquo;Client\u0026rdquo;. In both cases, all VM\u0026rsquo;s with access to the Edge are metered by Usage Meter.  Q8: How does Usage Meter detect if NSX Cross vCenter is configured?\n Usage Meter detects the Universal logical switch configured on a local edge.  Q9: Does Usage Meter detect and bill for NSX vShield Endpoint (guest introspection to support anti-malware) solutions?\n Yes, Usage Meter detects vShield Endpoint as a base NSX feature.  Q10: An NSX Edge can be deployed to an ESX Server that is not prepared for NSX. Can the edge be connected to a logical switch?\n No, it is not possible to create a Logical Switch on a server that is not prepared for NSX.  Q11: Are Virtual Machines connected to a vNetwork Standard Switch (vSwitch) or vNetwork Distributed Switch (dvSwitch) metered for NSX Usage?\n Yes, all switch types that are serviced by an NSX Edge are metered. Logical switches will be metered even if there is no NSX Edge servicing it.  NSX-T Features #  Usage Meter 4.5.0.1 inspects the topology of the virtual NSX network, obtaining lists of all known:\n Virtual machines Virtual Network Interfaces Logical ports Logical switches (aka \u0026ldquo;segments\u0026rdquo;) Logical routers (aka \u0026ldquo;gateways\u0026rdquo;)  Usage Meter reports usage for each VM, based on the feature and license editions described in the table below. Note: NSXFint is a column that appears in the Virtual Machine History report and represents the sum of the integer code of the NSX features used by a VM. Each NSX- T feature has a unique NSXFint value. In the table below, you can find the NSXFInt value for each NSX-T feature metered by Usage Meter. The value of the NSXFInt column in the Virtual Machine History report for each metered VM will be the sum of all NSX-T features used by that VM. For example, a VM using the Layer 2 VPN (aka \u0026ldquo;Remote Gateway\u0026rdquo;) and 3rd party service insertion features will have the following value in the NSXFInt column = 128+1024=1152. To find the features used by a VM, subtract from the NSXFint value, the highest possible value equal to or less than the NSXFint value, and continue the subtraction until NSXFint =0. For example, let’s have a VM that has NSXFint value = 3.\n We will check in the table below which NSX feature has a value equal to or less than 3 and subtract it from NSXFint. In this case, the highest number close to NSXFint =3 is 2, corresponding to Edge load balancing. The result of the subtraction is NSXFint - 2= 3-2=1. We will check again in the table what NSX feature has a value equal to 1. This feature is Distributed switching and routing. When we also subtract it from NSXFint, the NSXFint value will equal 0. In conclusion, NSXFint =3 is NSXFint=2+1, corresponding to Edge load balancing and Distributed switching and routing.     NSX Feature NSX Feature String NSX Edition Flex NSXFint VMs declared as using this feature     Distributed switching and routing BASE Base 1 All VMs connected to any NSX-T logical switch have this feature.   Edge Load Balancing DLB Base 2 A VM is metered as using this feature if it is connected (via switches and routers) to a router that has the load balancing service enabled.   Dynamic Routing with ECMP ECMP Base 4 Any VM connected (via switches and tier-1 routers) to a tier-0 router that is ECMP-enabled, will be metered as using this feature.   SW L2 Bridging to physical environment SWL2 Base 16 Any logical ports with attachment type \u0026ldquo;BRIDGEENDPOINT\u0026rdquo; use this feature, and all VMs that are connected to a logical switch that has such a port, will be metered as using this feature.   Network Address Translation NAT Base 32 All VMs connected to logical switches connected directly or indirectly to a router that has NAT rules are metered using this feature.   Distributed Firewall DFW Prof 64 Any VM that is referenced directly or indirectly in the sources or destinations of an enabled Distributed Firewall rule, will be metered for this feature, unless the appliedTos property of the rule indicates that the rule is not applied to that VM.   Layer 2 VPN (aka \u0026ldquo;Remote Gateway\u0026rdquo;) L2VPN Prof 128 Any segment/logical switch with a logical port connected to an L2VPN session uses this feature, and VMs attached to that logical switch will be metered as using this feature.   3rd party service insertion DFW3 Adv 1024 Any VM that is referenced directly or indirectly in the sources or destinations of an enabled service insertion rule will be metered for this feature, unless the appliedTos property of the rule indicates that the rule is not applied to that VM.   Identity firewall (aka \u0026ldquo;Integration with Active Directory\u0026rdquo;) DFW_AD Adv 2048 Any VM metered for DFW because of an Active Directory-enabled firewall rule will also be metered for this feature.   Context-aware Firewall with Layer 7 DFWL7 Adv 4096 Any VM metered for DFW will also be metered for this feature if the firewall rule that references the VM has a non-empty context profiles list.   Virtual Private Network using IPSEC IPSEC Base 8192 Any VM that is connected (via logical switches and routers) to a logical router that has IPsec service and IPsec sessions enabled is metered as using this feature.   Edge Firewall (aka Gateway firewall) GFW Base 16384 Any VM connected (via switches and routers) to a router that has one or more specified gateway firewall rules, will be metered as using this feature. The difference between distributed and gateway firewall rules is that distributed firewall sections (groups of rules) have an enforced_on attribute set to \u0026ldquo;VIF\u0026rdquo;, and gateway firewall sections have the enforced_on attribute set to LOGICALROUTER.   IPv6 Layer 3 forwarding IPV6STATIC Base 32768 If an NSX-T manager node has api/v1/global-configs/RoutingGlobalConfig configured to support IPv6 Layer 3 forwarding, then all the VMs which have a network managed by NSX-T manager node are metered with this feature.   URL Filtering (as part of a firewall) URL Adv 131072 Any VM metered for the DFWL7 feature will also be metered for this URL feature if it is referenced by a firewall rule with a context profile that specifies a domain name.   Container networking NCP Adv 262144 VMs are metered as using this feature if their VIF is connected to a logical port that has a tag with a scope of ncp/cluster.   NSX Federation FED Ent Plus 524288 This feature is detected if Global Manager is installed and there is at least 1 Local Manager connected to the Global Manager. This is a global configuration. If this feature is enabled, all the hosts under the Local Manager are said to be using Federation.   Kubernetes VM in vSphere PKS cluster PKS Adv 2097152 If a VM would be metered for the NCP feature and if its ncp/cluster tag specifies a cluster name and if the named cluster has a type of \u0026ldquo;Kubernetes\u0026rdquo;, and an infrastructure type of \u0026ldquo;vSphere\u0026rdquo;, then the VM is metered as using this PKS feature instead of the NCP feature.   Multi vCenter Network and Security MVC Adv 4194304 If an NSX-T manager is connected to more than one vCenter compute manager, then all the VMs that the NSX-T manager knows about will be metered with this feature.   IPv6 Layer 3 forwarding and bgp configured on corresponding Tier-0 router IPV6DYN Adv 8388608 If an NSX-T manager node has api/v1/global-configs/RoutingGlobalConfig configured to support IPv6 Layer 3 forwarding and bgp, is configured on the corresponding Tier-0 router, then all the VMs which are connected to Tier-0 router gateway are metered with this feature.   Tier-0 router configured as VRF VRF Prof 16777216 If an NSX-T manager node has a Tier-0 router configured as VRF, then all the VMs connected to Tier-0 router gateway on which this VRF instance is configured will be metered with this feature.   NSX intelligence appliance NSXINT Ent+ 33554432 If an NSX-T manager node has an NSX intelligence appliance configured, then all the VMs which have their network managed by NSX-T manager node are metered with this feature.   Tier-0 router configured with EVPN EVPN Ent+ 67108864 If an NSX-T manager node has a Tier-0 router configured with EVPN, then all the VMs connected to those Tier-0 router gateway will be metered with this feature.   NSX Distributed IDS IDS_STANDALONEHOST Adv 2147483648 This feature is enabled on a per host basis. When it is enabled, all the VMs connected to that host are counted.   Integration with Distributed Firewall IDFW Adv 4294967296 This is a global feature. When it is enabled, all the hosts/VMs under the NSX Manager are counted.    NSX-T Scenario Sample #  Only metering of eVPN inline mode and non-telco use case is supported. The eVPN server mode is not covered. In this scenario, non-telco eVPN inline mode uses VRF on T0, getting mapped to MP-eBGP via RD/RT combination and VMs belonging to VRF on south-side/downlink of T0 will be behind T1.\n "},{"id":32,"href":"/docs/networking-security/vcd-lbaas-planning-and-preparation/","title":"Planning and Preparation for Load Balancing as a Service in VMware Cloud Director","section":"Networking \u0026 Security","content":"Planning and Preparation for Load Balancing as a Service in VMware Cloud Director #  Introduction #  Before you start implementing the Load Balancing as a Service in VMware Cloud Director solution, you must set up an environment that has a specific compute, storage, and network configuration and that provides external services to the components of the solution.\nRequirements #  Software #  To implement load balancing as a service with VMware Cloud Director, your software versions must meet the requirements specified in the VMware Product Interoperability Matrix.\nResources #  Before you deploy NSX Advanced Load Balancer, you must provide sufficient compute and storage resources to meet the footprint requirements of the Controller cluster and the Service Engines.\n Avi Controller Sizing Sizing Service Engines  Networking #  This load balancing as a service solution is based on several management virtual appliances that require to be deployed in a management infrastructure. Latency requirements are critical to guarantee proper functioning and performance:\n Latency among Avi controllers – Less than 10 ms Latency between any Avi SE to any Avi Controller – Less than 75 ms recommended Latency between Avi Controller and NSX-T Manager – Less than 10 ms recommended  Best practice is to co-locate in the same port group/management infrastructure as NSX-T   Latency between Avi Controller and VMware Cloud Director – Best practice is to have have VCD cells in the same management infrastructure as NSX-T manager and Avi Controller  The Avi Controller and service engines use several ports for management and control communication: Protocol Ports Used by Avi Vantage for Management Communication.\nThe firewall should allow traffic for these ports.\nPreparation #  The solution comprises of the Avi Controller which uses APIs to interface with the NSX-T manager and vCenter to discover the infrastructure. It also manages the lifecycle and network configuration of the service engines.\nThe NSX-T Cloud is the object that permits the integration with the NSX-T manager and the vCenter server(s).\nThe user accounts configured on the Avi Controller require the following roles and permissions for the integration to work successfully:\n Roles and Permissions for vCenter user Roles and Permissions for NSX-T user  vSphere #  When using an NSX-T Cloud, the Avi Controller uploads the service engine image to the content library on the vCenter server and uses this to create new virtual machine every time a new service engine is required. The content library must be created on vCenter before configuring the NSX-T cloud.\nNSX-T #  The first network adapter of the service engine VM is reserved for management connectivity, and the remaining 9 data interfaces (network adapter 2 to 10) for the service engine VM to the VIP or data segment.\nThe Avi SE management interface can be connected to an overlay (recommend) or a VLAN logical segment. When connected to an overlay segment, it also needs a tier-1 gateway to provide external connectivity to be able to reach the Avi controller management IP. It is recommended to have a dedicated tier-1 gateway and segment for Avi service engine management.\nIf VLAN-backed logical segments are used instead of overlay transport zone for the management network in the NSX-T Cloud, refer to this page: NSX-T VLAN Logical Segment.  Regardless of the solution (overlay or VLAN segment for the SE management network), the NSX-T topology must be created upfront the NSX-T Cloud configuration. In the case of overlay segment for the SE management network:\n Create a tier-1 gateway that will be used to connect the SE management network. Create 2 overlay segments: one for the management network, and one as a dummy data network segment. Enable DHCP Server at the tier-1 gateway level and configure DHCP on the management segment.  More details here: Configuring Management Networking for SE.\nNext Steps #  One the environment is ready, you can proceed with the VMware NSX Advanced Load Balancer deployment and configuration in combination with VMware Cloud Director to provide Load Balancing as a Service: Load Balancing as a Service in VMware Cloud Director.\n"},{"id":33,"href":"/docs/dr-migration/setup-vcda/","title":"Setting Up VMware Cloud Director Availability","section":"DR \u0026 Migration","content":"Setting Up VMware Cloud Director Availability #  Port mapping #  The communication between the different appliances occurs at the following ports as shown on the diagram.\nRequirements #  Here is a high-level list of the requirements to deploy VMware Cloud Director Availability. For more details, please refer to the official documentation.\n  DNS and NTP server\n  SSO Lookup Service Address\n  Routing and Firewall Ports in place\n  Compatible versions of vCenter and VMware Cloud Director – open the VMware Product Interoperability Matrices\n  VMware Cloud Director Availability initial configuration #  The deployment of the VMware Cloud Director Availability appliances in the vCenter Server is done by following these steps:\n  Select the OVF template.\n  Name the virtual machine.\n  Select a destination compute resource.\n  Review the virtual machine details.\n  Read and accept the license agreement.\n  Select the appliance that we are going to deploy.\n  Pick the storage for the disk files.\n  Specify the network.\n  Enter the deployment properties such as root password, NTP server, IP address, gateway, DNS, and more.\n  Validate that all the details are correctly entered on the summary screen, and deploy the virtual machine.\n  After the deployment of the appliances, you need to run the initial setup wizard. It is designed to guide you through the essential steps to configure all the appliances at the same time, no matter if it is a combined or a distributed environment (only the distributed setup is recommended for production).\nFor example, you can define the Lookup service, attach a Cloud Replicator Appliance or a Cloud Tunnel Appliance, and even change the root password for of all appliances, without any necessity to log in to each of them and change it in their UI.\nA detailed look at the initial setup wizard #  YouTube video showing the exact steps\nAfter you successfully deploy the OVA templates for all the appliances, the next step is to run the initial setup wizard.\nFirst, you need to login to the appliance by opening your browser and entering the following URL: https://\u0026lt;Cloud-Replication-Management-Appliance-URL\u0026gt;/ui/admin.\nYou will see the login screen of the VMware Cloud Director Availability Cloud Service admin UI. There, you have to enter the password defined during the deployment process.\nSince it is the first login to the system, you will be prompted to change the password.\nAfter you enter a new password that matches the password policy (it should be at least 8 symbols and contain at least one uppercase, lowercase, number, and special character), you will see the welcome screen (Picture 3).\nAfter reading and complying with the information on the welcome screen, proceed with the configuration of the Cloud Director Availability, by clicking Run Initial Setup Wizard. Note that all the steps are mandatory, and you cannot skip any of them.\nThe first thing to do is to enter your license key.\nNext, you need to give your site a name and provide the Service Endpoint address. In case you are not sure about the address at the moment of setup, you can leave it empty and fill it later in the VMware Cloud Director Availability portal (Configuration menu).\nOn the next screen, you have to fill in your VMware Cloud Director Endpoint address and an administrator’s credentials. Note that you only need to type the Endpoint URL, and the wizard will automatically add the https:// prefix and /api suffix.\nBy clicking Next, you are prompted to reject or accept the VMware Cloud Director certificate. To proceed further, you need to accept it.\nAfter accepting the certificate, you will be asked for your consent to participate in the VMware\u0026rsquo;s Customer Experience Improvement Program (\u0026ldquo;CEIP\u0026rdquo;). By default, it is checked, but you can uncheck it and proceed with the wizard.\nThe penultimate step is to connect your Cloud Replicator Appliance(s) (if you have more than one). An important note - on this step, you enter the Lookup Service Address. It is used for all your replicators and the tunnel.\nAfter entering all the Lookup Service Address, Replicator Service Address, and root password, you need to click TEST CONNECTION to verify the connectivity and connect to the Cloud Replicator Appliance. Similarly, to the VMware Cloud Director Service Endpoint, you only need to put the IP/URL, and the port, prefix, and suffix are added automatically. You need to perform the same set of actions for all the Cloud Replicator Appliances you want to attach.\nRight after clicking the TEST CONNECTION button, you will be prompted to reject or accept the Cloud Replicator Appliance certificate. To proceed, accept the certificate.\nIf you haven\u0026rsquo;t logged in the Cloud Replicator Appliance UI so far, you will see a message that the root password has expired. You can directly set a new root password from the wizard. The password policy is the same as for the Cloud Replication Management Appliance. If you change the password before running the wizard, you will proceed directly to the last step without being asked to change the root password.\nYou also need to provide the SSO administrator credentials.\nThe ADD A REPLICATOR SERVICE INSTANCE button will let you connect additional Replicator Services if wanted.\nIf all the details are entered correctly, you will be prompted to reject or accept the Lookup Service certificate. To proceed, accept the certificate.\nThe very last step of the setup process is to connect to the Cloud Tunnel Appliance. You don\u0026rsquo;t need to define the Lookup Service Address again as the wizard uses the one set on the previous step. You only need to enter the API URL and the root password. Note that the prefix and port are added automatically.\nSimilarly, to the previous step where we connected to the Cloud Replicator Appliance(s), if you haven\u0026rsquo;t changed the root password yet, you could do it after clicking the CHECK PASSWORD button. Note that if you have already changed the password, the UI will direct you to the next action.\nBy clicking Next, you will be prompted to reject or accept the Cloud Tunnel Appliance certificate. To go through, you need to accept it.\nThe last stage of the wizard is a summary that will show you some of the details you entered. Use the Finish button to close the initial setup wizard and start using your newly configured VMware Cloud Director Availability.\nOn the picture below, you can see how the home screen looks like after performing a successful configuration.\n"},{"id":34,"href":"/docs/dr-migration/vcda-vmc-on-aws-sddc/","title":"Setting Up VMware Cloud Director Availability at a VMC on AWS SDDC","section":"DR \u0026 Migration","content":"Setting Up VMware Cloud Director Availability at a VMC on AWS SDDC #  Prerequisites #  To be able to successfully deploy and run VMware Cloud Director Availability in your VMC on AWS environment, you will need to make sure the following requirements are met:\n Have a properly deployed Software-Defined Data Center (SDDC). Have a VMware Cloud Director deployed at VMC on AWS (Cloud Director service) that is linked to the SDDC. Have defined at least one Organization, OrgVDC with Hardware Version (Default is Hardware Version 14 – vCenter 6.7.0) higher than one you have in the vCenter you would like to use as a source location Have defined at least one tenant admin user. (Recommended) Have a dedicated routed network for the VMware Cloud Director Availability appliances. (You can still use any existing routed network). Obtain its CIDR from Networking \u0026amp; Security \u0026gt; Network \u0026gt; Segments.  Obtain the proper Source NAT Public IP of your SDDC from Networking \u0026amp; Security \u0026gt; Overview.  Obtain the proper DNS Service IP of your SDDC from Networking \u0026amp; Security \u0026gt; System \u0026gt; DNS.  Create a Trusted IPs group from Network \u0026amp; Security \u0026gt; Inventory \u0026gt; Group \u0026gt; Compute Groups where you will add your public IP address so you can access the VMware Cloud Director Availability portal. Then in this group you will need to add all your tenant IP addresses so they can connect their on-premises appliances to your VMware Cloud Director Availability cloud.  Create a Compute Gateway Firewall Rule with the following settings to allow access from your trusted IPs to the environment:  Create a new Resource Pool for the VMware Cloud Director Availability Appliances under the Compute-Resource Pool.  Deployment #  Below you can find the necessary configuration steps for the separate appliances case and NOT for the combined appliance.\nPlease repeat the mentioned steps for each of the appliances – Cloud Replication Management appliance, Cloud Replicator appliance and Cloud Tunnel appliance.\n  Log in to the vCenter UI from your VMC console.\n  Deploy the OVA template in the Resource pool created in Requirement #8 in the Prerequisites section.\n  The deployment steps are similar to the typical VMware Cloud Director Availability Deployment. There are only a few considerations to be taken:\n On Step 7 – Select Storage: Select Workload Datastore   On Step 8 – Select networks: Select the dedicated network for VMware Cloud Director Availability from Requirement #4 in the Prerequisites section.    On Step 9 – Customize template:\n In the Address field provide an address in the dedicated network for VMware Cloud Director Availability from Requirement #4 in the Prerequisites section. In the DNS servers field provide the DNS Service IP address from Requirement #6 in the Prerequisites section.      After you have successfully deployed the 3 appliances, you should see something similar to:\n  Additional SDDC configuration #  To be able to successfully pair any on-premises instance to the VMware Cloud Director Availability cloud instance hosted at VMC on AWS, you need to perform some additional steps and prepare your SDDC network settings.\nPlease follow the procedures in their exact order as they are listed in this document.\nAdd Inventory Services #  You need to define 2 Services that will be later used in the Firewall settings. One is for the Cloud Management Portal and the other one is for the Cloud Tunnel endpoint.\nFollow these steps to get your services defined:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; Inventory \u0026gt; Services. To add the Management Portal service, click on ADD SERVICE. Give the service a name.  Click on Set Service Entries. Enter a name for the entry, select the Service Type to be TCP and the Destination Port to be 8046.  Click Apply and then Save. To add the Tunnel endpoint service, click on ADD SERVICE. Give the service a name.  Click on Set Service Entries. Enter a name for the entry, select the Service Type to be TCP and the Destination Port to be 8048.  Your services are ready.  Request Public IPs #  You will need to request 2 new Public IP addresses – one for the Cloud Management Portal and one for the Cloud Tunnel. To request them, please follow the steps below:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; System \u0026gt; Public IPs. Click on REQUEST NEW IP. Put a meaningful note for your Cloud Management Portal IP. Click Save. Click on REQUEST NEW IP. Put a meaningful note for your Cloud Tunnel IP. Click Save. Your 2 new Public IPs are ready.  Create a Compute Group #  You need to create a Compute Group that will be later used in the Firewall configuration. To create a Compute Group, please follow the steps below:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; Inventory \u0026gt; Groups \u0026gt; Compute Groups. Click on ADD GROUP. Give the Compute Group a meaningful name.  Click on Set Members and select the IP Addresses tab. Enter the network details from Requirement #4 in the Prerequisites section.  Click on Apply and then Save. The Compute Group is now ready.  Create Management Groups #  For enabling your Cloud Replicator to perform its replication jobs with ESXi, you need to create 2 Management Groups that will be later used in the Management Gateway Firewall configuration. To create them, please follow these steps:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; Inventory \u0026gt; Groups \u0026gt; Management Groups. Click on ADD GROUP. Give the first Management Group a meaningful name.  Click on Set Members. Enter the private IP that you will set to the Cloud Replicator.  Click on Apply and then Save. Click on ADD GROUP. Give the second Management Group a meaningful name.  Click on Set Members. Enter the Public IP that you collected in Requirement #5 in the Prerequisites section.  Click on Apply and then Save. Your Management Groups are created.  Configure the Compute Gateway Firewall #  You need to do some configurations to the Compute Gateway Firewall in order to allow the inbound traffic to the Cloud Tunnel and also the outbound traffic from your VMware Cloud Director Availability appliances.\nThese are the necessary steps:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; Security \u0026gt; Gateway Firewall \u0026gt; Compute Gateway. Click on ADD RULE. Give the Appliances Outbound Rule a meaningful name. Select the Compute Group that you created in section Create a Compute Group in the Sources column. Leave everything else with its default value. Make sure the Rule is enabled. Click on ADD RULE. Give the Cloud Tunnel Inbound Rule a meaningful name. Select the Cloud Tunnel Endpoint service that you created in Add Inventory Services section. Leave everything else with its default value. Make sure the Rule is enabled. Click on Publish. The Firewall Rules are ready.  Configure the Management Gateway Firewall #  To enable the internal communication between the different VMware Cloud Director Availability components and the ESXi and vCenter, you need to configure 2 Compute Gateway Firewall rules. To create them, please follow these steps:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; Security \u0026gt; Gateway Firewall \u0026gt; Management Gateway. Click on ADD RULE. Give the ESXi Provisioning Rule a meaningful name. Select as follows:  Sources – the Cloud Replicator Private IP Management Group that you defined in the Create Management Groups section. Destinations – ESXi. Services – Provisioning and Remote Console (TCP 902).   Click on ADD RULE. Give the Appliances Inbound rule a meaningful name. Select as follows:  Sources – the Management Group that has the Public IP as a member that you defined in the Create Management Groups section. Destinations – vCenter. Services – HTTPS.   Click on Publish. The Firewall Rules are defined.  Add NAT rules #  NAT rules are necessary to forward the incoming traffic to the correct appliances. You need to add 2 NAT rules – one for the Cloud Management Portal and one for the incoming Cloud Tunnel traffic.\nThe Cloud Management Portal rule can be removed after the initial configuration is done as the Portal is accessible through the VMware Cloud Director Availability Plug-in in Cloud Director service.\nThe steps to add NAT rules are:\n Navigate to your SDDC Network \u0026amp; Security \u0026gt; Network \u0026gt; NAT. Click on ADD NAT RULE. Give the Cloud Management Portal Rule a meaningful name. The rule settings should be as follows:  Public IP – the Public IP that you requested for the Cloud Management Portal in the Request Public IPs section. Service – the Cloud Management Service that you defined in the Add Inventory Services section. Public Port – 8046. Internal IP – the Cloud Management Replicator Appliance internal IP address. Internal Port – 8046. Firewall – Match Internal Address. Click Save.   Click on ADD NAT RULE. Give the Cloud Tunnel Inbound Rule a meaningful name. The rule settings should be as follows:  Public IP – the Public IP that you requested for the Cloud Tunnel in the Request Public IPs section. Service – the Cloud Tunnel Service that you defined in the Add Inventory Services section. Public Port – 443. Internal IP – the Cloud Management Replicator Appliance internal IP address. Internal Port – 8048. Firewall – Match Internal Address. Click Save.   The NAT rules are created.  Initial setup #   Make sure your external IP address is in the Trusted IP list that was defined in Requirement #7 in the Prerequisites section. Navigate to (https://\u0026lt;Cloud_Management_Portal_Public_IP\u0026gt;:8046/admin). Log in as root and change the password when prompted. Click on Run the initial setup wizard. Provide the VMware Cloud Director Availability license. Give the site a meaningful name and check only the VMC data engine to be activated.  Provide the Cloud Director service public URL in the following format – (https://CDs_URL/api). Enter a System Administrator or CDS Provider Admin user and its password. For example, vcdaadmin@sytem. Any other user types except Local users are currently not supported.  Provide the VMC Lookup Service URL which is the vCenter public URL. Use this format – (https://vCenter_URL:443/) lookupservice/sdk. Enter the internal IP address of the Replicator (for example, (https://172.26.46.202:8043)) and its root password. You might be prompted to change the root password, if you haven’t done so yet. Enter cloudadmin@vmc.local as SSO user name and provide its password.  Enter the Cloud Tunnel Appliance internal IP address and its root password. You might be prompted to change the root password, if you haven’t done so yet.  Finalize the wizard.  Pairing with another Cloud #  To enable migrations from private clouds running VMware Cloud Director, you need to upgrade and pair the existing instance of VMware Cloud Director Availability operating in this private cloud.\nOnce its version is 4.2, you will need to change the Data Engine similarly to what you did in the VMware Cloud Director Availability provider instance running in Cloud Director service (step 3 from the Additional Configuration in the Provider setup section).\nTo continue supporting the existing replications, it should have both options selected – Classic and VMC.\nIn cases where you perform a fresh installation of VMware Cloud Director Availability 4.2, you can select both data engines to be enabled during the Initial Config Wizard.\n"},{"id":35,"href":"/docs/cloud-infrastructure/srm-metering/","title":"SRM Metering and Reporting","section":"Cloud Infrastructure","content":"SRM Metering and Reporting #  VMware Site Recovery Manager (SRM) is a disaster recovery solution that provides automated orchestration and non-disruptive testing of centralized recovery plans for all virtualized applications. It allows service providers to offer reliable, automated protection for workloads hosted on their own cloud infrastructure and being replicated to another cloud destination (DR of the cloud) or workloads running on customer premises and being replicated to the cloud infrastructure (DR to the Cloud). Usage Meter 4.5.0.1 supports the metering of SRM Protected VMs. A protected VM is any virtual machine that is part of an SRM protection group, regardless of whether the VM is powered-on or off. A license is only required for the VM on the active site; no license is required at the failover site. The SRM collector is part of the vCenter collector on the Usage Meter 4.5.0.1 appliance. All collected SRM protected VM details will be in the vCenter collection archives. The collector collects data once a day, and it tracks only the protected VMs present in the protection group.\nNote: vCloud Usage Meter 4.5.0.1 only supports metering of protected VMs in READY state and not protected VMs in SHADOWING state.\nProduct Detection #  SRM is automatically detected in vCenter when the latter is added for metering in the Usage Meter 4.5.0.1 web application. The appliance checks if there is an SRM instance installed on the metered vCenter, and if there is, it receives information from the SRM server for the count of the protected VMs. The service providers do not need to provide site details for the SRM metering. They can deactivate the SRM metering from the UM 4.5.0.1 web application for any added vCenter. Usage Meter 4.5.0.1 detects and meters SRM versions from 8.2.1 to 8.5.\nMetering of SRM provides enhanced billing bi-directional support, for example, from protection to failover site and from failover to protection site. Usage Meter 4.5.0.1 supports the metering of multiple failover sites, for example, from site A to site B and from site A to site C. If any site part of a pair of sites is not added to a Usage Meter appliance, then no VM will be counted as a protected VM. Metering of active and failover sites with a single UM 4.5.0.1 appliance:\n   Setup Expected Monthly Usage Report Count     Both sites A and B are added to the same Usage Meter 4.5.0.1. Both vCenter and SRM are enabled. The Monthly Usage Report will contain 8 protected VMs. 5 protected VMs from site A. 3 protected VMs from site B. Note: In Usage Meter 3.6.1, billing on both sites is not possible.   Site A added to Usage Meter 4.5.0.1, Site B is not added to Usage Meter 4.5.0.1. The Monthly Usage Report will contain 5 protected VMs. 5 protected VMs from site A.   Both A and B sites are added to the same Usage Meter 4.5.0.1. However, site B is deactivated in the Usage Meter web application by deselecting the following checkbox: Meter VMs protected by all SRM\u0026rsquo;s associated with this VC in the Edit this vCenter/vCloud Foundation Server window. The Monthly Usage Report will contain 5 protected VMs. 5 protected VMs from site A. 0 protected VMs from site B. Note: Metering in this scenario is the same as with UM 3.6.1.   Both sites A and B are added to the same Usage Meter 4.5.0.1. However, site A and B are deactivated in the Usage Meter web application by deselecting the following checkbox: Meter VMs protected by all SRM\u0026rsquo;s associated with this VC in the Edit this vCenter/vCloud Foundation Server window. The Monthly Usage Report will contain 0 protected VMs. 0 protected VMs from site A. 0 protected VMs from site B. Note: In this scenario, there will be no billing for SRM, only vCenter-based VM usage part of the Monthly Usage Report.    Metering of active and failover sites with multiple Usage Meter 4.5.0.1 appliances registered in the same contract:\n   Setup Monthly Usage Report Number of Protected VMs     UM 1 and UM 2 both belong to the same contract. Site A is metered by UM 1; vCenter and SRM are both enabled. Site B is metered by UM 2; vCenter and SRM are both enabled. The Monthly Usage Report of UM 1 will contain 8 protected VMs. The Monthly Usage Report of UM 2 will contain 8 protected VMs. Per the contract, there will be 16 protected VMs.       \nWorkarounds for avoiding duplicate billing:\n \nMonthly Usage Report Number of Protected VMs     \nAvoiding duplicate SRM billing\nUM 1 and UM 2 both belong to the same contract.\nSite A is metered by UM 1; vCenter and SRM are both enabled.\nSite B is metered by UM 2; vCenter is enabled, while SRM is deactivated. Note: SRM metering can be enabled/deactivated in the Usage Meter 4.5.0.1 web application by clicking Edit for a selected vCenter/vCloud Foundation instance and then by selecting/deselecting the checkbox: Meter VMs protected by all SRM\u0026rsquo;s associated with this VC in the Edit this vCenter/vCloud Foundation Server window. \nThe Monthly Usage Report of UM 1 will contain 8 protected VMs.\nThe Monthly Usage Report of UM 2 will contain 0 protected VMs.\nPer the contract, there will be 8 protected VMs.\nNote: In the example with UM 2, there will be no SRM specific billing, but only vCenter-based VM usage information in the Monthly Usage Report of that appliance.\n   \nAvoiding duplicate SRM and vCenter billing\nUM 1 \u0026amp; UM 2 both belong to the same contract.\nSite A is metered by UM 1; vCenter and SRM are both enabled.\nSite B is metered by UM 2; vCenter is deactivated (by applying a demo license to all hosts), and SRM is also deactivated.\nNote: SRM metering can be deactivated in the Usage Meter 4.5.0.1 web application by clicking Edit for a selected vCenter/vCloud Foundation instance and then by deselecting the checkbox: Meter VMs protected by all SRM\u0026rsquo;s associated with this VC in the Edit this vCenter/vCloud Foundation Server window. \nThe Monthly Usage Report of UM 1 will contain 8 protected VMs.\nThe Monthly Usage Report of UM 2 will contain 0 protected VMs.\nPer contract, there will be 8 protected VMs.\nNote: In the scenario where both vCenter and SRM are deactivated, there will be no SRM specific billing and no vCenter-based VM usage in the Monthly Usage Report.\n    Sample Monthly Usage Report #  The following is an excerpt from the Usage Meter 4.5.0.1 Monthly Usage Report and contains an example of SRM-metered data.\n   Product Metric Units to be reported     VMware vCenter Site Recovery Manager Protected VMs 103    "},{"id":36,"href":"/docs/cloud-infrastructure/tanzu-metering/","title":"Tanzu Metering and Reporting","section":"Cloud Infrastructure","content":"Tanzu Metering and Reporting #  Configuration #  Tanzu is automatically detected if the Usage Meter administrator configures the endpoint and credentials for each vCenter to be metered using the Usage Meter web application. The Usage Meter administrator can choose to either meter Tanzu as a vRAM or CPU cores metric by selecting the respective option from the \u0026ldquo;If Tanzu is detected, choose the metric to report:\u0026rdquo; drop-down list. The Usage Meter administrator can switch between the vRAM metric and CPU cores metric any time during the metering period. The default metering metric is vRAM. Any of the following Tanzu editions can be selected in the Usage Meter web application as well.\n Tanzu Basic Tanzu Standard Tanzu Advanced  Note: Currently, Usage Insight will only report Tanzu-related usage as Basic in the monthly usage reports, irrespective of the selected Tanzu edition. In future vCloud Usage Insight releases, there will be a fine-grain mapping of the Tanzu-related usage and its correlating edition.\nNote: It is recommended that a single Usage Meter instance meters a single Tanzu edition. For datacenters using different Tanzu editions, there should be a separate Usage Meter installation for each used Tanzu edition. Metering\nOnly Tanzu Basic-related VMs are metered based on Tanzu Basic usage. The following type of VMs will be considered as Tanzu Basic-related VMs:\n Supervisor VM: These VMs are Kubernetes control plane VMs. Three Kubernetes control plane VMs in total are created on the hosts that are part of the Supervisor Cluster. The three-control plane VMs are load-balanced as each one of them has its own IP address. Additionally, a floating IP address is assigned to one of the VMs. vSphere DRS determines the exact placement of the control plane VMs on the ESXi hosts and migrates them when needed. vSphere DRS is also integrated with the Kubernetes Scheduler on the control plane VMs so that DRS determines the placement of vSphere Pods. When as a DevOps engineer, you schedule a vSphere Pod, the request goes through the regular Kubernetes workflow, then to DRS, which makes the final placement decision. POD VM: A vSphere Pod is a VM with a small footprint that runs one or more Linux containers. Each vSphere Pod is sized precisely for the workload that it accommodates and has explicit resource reservations for that workload. It allocates the exact amount of storage, memory, and CPU resources required for the workload to run. vSphere Pods are only supported with Supervisor Clusters configured with NSX-T Data Center as the networking stack. TKG VM: VMs running and managing Tanzu Kubernetes clusters. A Tanzu Kubernetes cluster is a full distribution of the open-source Kubernetes container orchestration platform that is built, signed, and supported by VMware. You can provision and operate Tanzu Kubernetes clusters on the Supervisor Cluster by using the Tanzu Kubernetes Grid Service. Both main nodes and worker nodes are metered in the Tanzu Kubernetes cluster.  Tanzu Basic supports both time-based average vRAM metering metric and CPU cores metering metric.\n vRAM metric (Time-based average vRAM usage): Average vRAM usage for all Tanzu Basic-related powered-on VMs. CPU cores metric (Time-based average of host CPU core count): Average host cores usage for all hosts with at least one powered-on Tanzu Basic-related VM. A host is metered for the period where there was at least one powered-on Tanzu Basic-related VM running on this host during the metering period. All cores of the host are metered for the duration.  For example, if there is at least one powered-on VM that is running on a host with 6 CPU cores for any of the 10 days in a 30-day period, this host will be billed with an average host CPU core count of 2, (6 * (1/3) = 2). If a host -whether it has been modified or added or placed into migration mode (i.e., host state changes) - has been running even a single VM, the host is metered for the duration where there is at least one powered-on Tanzu Basic-related VM.\nReporting #  Tanzu Basic Usage (if detected) is always reported as a standalone line item in the Monthly Usage Report.\nSample Monthly Usage Report #     Product Hostname Version VC UUID Units of Measure Units to be Reported     Tanzu Basic    Avg Capped Billed vRAM (GB) 20   Tanzu Basic    Avg Number of Cores 6    Note: The report above shows both the vRAM and CPU cores usage are detected by Usage Meter. This could mean that the Usage Meter administrator has registered a single Tanzu Basic enabled vCenter and switched between the vRAM metric and CPU core metric during the metering period. Multiple vCenters with different metering metrics registered to the Usage Meter cloud also produce two-line items in the report. If the same metric is selected during the entire metering period for all vCenters registered in the Usage Meter, only one-line item with the selected metric will be shown in the report.\nVirtual Machine History Report: #  The column \u0026ldquo;vmType\u0026rdquo; is included in the Virtual Machine History report indicating the types of VMs in the report:\n   vmType Translation     SUP Supervisor VM   POD POD VM   TKG VMs created by vSphere Tanzu Kubernetes Grid service   OTHERS VMs that are not categorized as SUP, POD or TKG    Note: Tanzu will not be shown in \u0026ldquo;associated products\u0026rdquo; on the vCenter registration and management page even if detected by Usage Meter.\n"},{"id":37,"href":"/docs/cloud-infrastructure/vcenter-metering/","title":"vCenter Metering and Reporting","section":"Cloud Infrastructure","content":"vCenter Server Metering and Reporting #  Usage Meter collects product usage information from all vCenter servers that are registered with Usage Meter.\nNote: Usage Meter does not traverse Enhanced Linked Mode, and it requires registering all vCenter servers, part of this mode, to meter their usage data.\nNote: VMware Cloud Provider Program partners should utilize VCPP licenses for all servers used in the service delivery path or administration control plane. VMware perpetual licenses, including OEM versions, can only be utilized to support internal IT operations that are not part of service delivery. In addition, VMware perpetual licenses may not be used to support the management or operations of an environment utilized to host unaffiliated third parties. Virtual machines running on hosts with perpetual or demo license keys are metered by Usage Meter but are not reported.\nConfiguration #  The Usage Meter administrator must configure the endpoint and credentials for each vCenter server to be metered using the Usage Meter web application.\nFeature Detection #  When metering vCenter, Usage Meter performs two types of usage collections: event-based and inventory collections.\nEvent history collections #  Usage meter subscribes to vCenter events to detect changes in a virtual machine and host state. Changes in a virtual machine state are delivered every hour to Usage Meter and recorded.\nNote: Usage Meter collects data from hosts and VMs in an active state. Even when there is a connectivity issue with a host, Usage Meter will still report metering data if the host VMs are in a powered-on state. This means that providers will be billed for VMs even when there are temporary interruptions in host connectivity. If providers want to be sure they are not being billed for VMs, they need to turn those VMs off.\nInventory collections #  Usage Meter queries vCenter servers every eight hours to collect full inventory information.\nvCenter Reporting #  The vCenter Server Standard edition is part of the Flex Core bundle and therefore reported as part of that bundle. vCenter usage is bundled by default and cannot be reported standalone. See an exaple below:\n"},{"id":38,"href":"/docs/dr-migration/certificate-management/","title":"VMware Cloud Director Availability Certificate Management","section":"DR \u0026 Migration","content":"VMware Cloud Director Availability Certificate Management #  The SSL certificates are essential for establishing a trusted connection between the different VMware Cloud Director Availability appliances and their proper service.\nEach of them comes with their unique self-signed SSL certificate during the deployment. But still, these certificates need to be replaced when they expire, or if the providers prefer to use CA-signed ones to make sure there will be no browser warnings, for example. This post will review the necessary steps to replace the Cloud Service, Manager Service, Replicator Service, and the Tunnel Service certificates with CA-signed ones. Prerequisites for the CA-signed certificate #    PKCS#12 (.pfx) certificate and the private key should use the same password\n  PKCS#12 file should contain only one entry - the private key and its corresponding certificate and, optionally, the certificate trust chain\n  RSA key size should be 2048-bit or larger\n  The certificate should not use insecure hash algorithms like SHA1 or MD5\n  Useful commands #  Command 1: Generate a new private key and Certificate Signing Request:\n openssl req -out CSR.csr -new -newkey rsa:2048 -nodes -keyout privateKey.key\nCommand 2: Convert .crt to .pem\n openssl x509 -inform der -in certificate.cer -out certificate.pem\nCommand 3: Prepare a PKCS#12 (.pfx) from a .pem (in case your CA didn\u0026rsquo;t provide it to you)\n openssl pkcs12 -export -out certificate.pfx -inkey privateKey.key -in certificate.crt -certfile CACert.crt\nUsual scenario #  It is sufficient for most providers to use a CA-signed certificate for the Cloud Service only and self-signed certificates for all other services. This CA-signed certificate has to be generated for the public address of VMware Cloud Director Availability. Steps #    Login through SSH with the root user to the VMware Cloud Director Availability Cloud Service host.\n   Generate a new private key and Certificate Signing Request using the following command:\n   openssl req -out CSR.csr -new -newkey rsa:2048 -nodes -keyout privateKey.key\nFill in the necessary data similar to this example   Once the CSR is generated, you need to transfer it to the CA for signing.\n  If the received CA-signed certificate is not in PKCS#12 format (.pfx), please use the following command to prepare it:\n   openssl pkcs12 -export -out certificate.pfx -inkey privateKey.key -in certificate.crt -certfile CACert.crt\n(where privateKey.key is the private key used for the CSR and CACert.crt is the CA certificate)\n Load the certificate to the Cloud Service using:\n  VMware Cloud Director Availability UI:     Navigate to the VMware Cloud Director Availability Cloud Service URL (https://Appliance-IP-Address/ui/admin.)    Log in as root.    Select Settings from the left pane called Configuration.      Under Appliance Settings, you will find Certificate.      Click Import.    Fill in the Export Password (specified while creating the .pfx).      Click Apply.     VMware Cloud Director Availability API through command-line:     Transfer the .pfx file to the VMware Cloud Director Availability Cloud Service host.    Log in as root through SSH to the VMware Cloud Director Availability Cloud Service host.    Log in as root to the VMware Cloud Director Availability API through the command-line using:    c4 loginroot 'password'\n    Upload the new certificate using:    c4 upload_certificate /path/to/cert/cert.pfx 'Export password'\n     The VMware Cloud Director Availability Cloud Service will be restarted after the certificate change.\n  You will no longer see any warnings in the browser.\n  Affected services #  To see what other services are affected by the change and fix, you can open System Health under Monitoring in the left pane.\nYou can see that the connectivity to the Tunnel Service is showing failure.\nTo fix it, you need to perform the steps in Procedure 1.\nProcedure 1: #    Click on Settings under Configuration.\n  Find Tunnel Service address in Service Endpoints.\n  Click Edit.\n  Enter the root password.\n   Click Apply.\n  Accept the certificate request.\n  Go back to System Health and check that the connectivity to the Tunnel Service is okay.\n  Affected paired sites #  Such a certificate change impacts both cloud and on-premises sites that are paired to this Cloud Service. In order to restore the regular operation, you will need to re-pair all connected sites.\nOn-premises #  To re-pair with an on-premises site, your tenant needs to:\n  Open the VMware Cloud Director Availability on-premises appliance URL.\n  Log in as root.\n  Click on Settings in the left pane.   Find Pairing under Site Details.\n  Click Repair.\n   Enter all information in the wizard and accept the certificate request.\n  Finish the wizard.\n  Cloud #  To re-pair with a cloud site, the remote cloud site admin needs to:\n  Open the VMware Cloud Director Availability UI of the remote site.\n  Log in as root.\n  Click Peer Sites under Configuration in the left pane.\n  Select the cloud site with the changed certificate (marked with an error).   Click Repair.\n  Click Update and accept the certificate request.\n  A message indicates there are actions to be performed on the other site.  Once these steps are performed, you need to do the same at the local site. Other appliances #  If you plan to replace all self-signed certificates with CA-signed ones, you can follow the steps described for the Cloud Service. The only difference is in the affected services as follows:\n  When changing the SSL certificate of a Manager Service, the trust between all Replicator Service instances is invalidated. To re-establish it, you need to register all Replicator Service instances by performing Repair from their UI and re-pair the cloud sites.\n  When changing the SSL certificate of a Replicator Service, it leads to a paring problem with Manager Service. You need to re-pair to the Manager Service on the local site and re-establish the trust between all cloud sites.\n  When changing the SSL certificate of a Tunnel Service, you need to re-establish the connectivity between it and the Cloud Service. To do so, you can perform Procedure 1. For about 30 minutes, you might see a Generic error occurred during TLS handshake message, but you do not need to perform any actions to fix it. The reason is that the certificate replacement restarts the service, which breaks the sessions with the remote cloud or on-premises replicator. The session initiation happens every 30 minutes, which means that all remote sites should auto-recover pairing in no longer than 30 min.\n  When changing the VMware Cloud Director SSL certificate, you need to re-establish the trust connection from the VMware Cloud Director Availability Cloud Service UI.\n  When changing the Lookup Service SSL certificate, all VMware Cloud Director Availability appliances need to trust the Lookup Service certificate once again.\n  Backup and restore #  One of the features in VMware Cloud Director Availability, which enables backing-up all appliances, is very useful when planning to perform changes to any of the services.\nConsidering that replacing the certificates impacts the operation of VMware Cloud Director Availability, we always recommend generating a backup before proceeding with any of the steps for updating the SSL certificates. You can see how in this blog post.\n"},{"id":39,"href":"/docs/dr-migration/vcda-gcve-sddc/","title":"VMware Cloud Director Availability Certificate Management","section":"DR \u0026 Migration","content":"Setting Up VMware Cloud Director Availability at a GCVE SDDC #  User Permissions for Google Cloud VMware Engine #  The default vSphere administrative user that comes with Google Cloud VMware Engine is CloudOwner@gve.local. Despite its administrative rights, the CloudOwner user misses some privileges (like Host.Config.Connection, for example) for the VMware Cloud Director Availability Classic data engine to operate properly. Even the temporary elevation of the CloudOwner privileges (link) does not help to perform a successful replication.\nHowever, out-of-the-box Google Cloud VMware Engine creates 5 vSphere solution users with full administrative privileges. They are intended to be used by products like VMware Cloud Director, VMware SRM and also other VMware and non-VMware tools. By using one of the solution users during the VMware Cloud Director Availability configuration, the product can successfully create and maintain a healthy replication. A description of the solution user accounts and steps how to enable them is available here.\nNOTE: The password of the solution user expires 365 days after it was changed last. This means resetting it needs to be considered every year not to disrupt the service.\nGoogle Cloud VMware Engine Network Configuration #  In terms of internal networking and communication, Google Cloud VMware Engine is not as restrictive as VMC on AWS, for example. There is no need to define firewall rules for enabling the VMware Cloud Director Availability appliances to access the ESXi hosts, for example.\nStill these requirements need to be met:\n Have a network segment for the VMware Cloud Director Availability appliances to connect to. It can be an existing one or specially created for VMware Cloud Director Availability. Managing network segments is done directly through the VMware NSX-T Data Center UI deployed in Google Cloud VMware Engine. For further instructions, please refer to the VMware NSX-T Data Center user documentation. Request a new public IP and forward it to the VMware Cloud Director Availability Cloud Tunnel appliance internal IP address.  Define a firewall table that will allow incoming traffic to the Cloud Tunnel.  VMware Cloud Director Availability Deployment #  The VMware Cloud Director Availability appliances deployment is following the typical sequence of deployment steps that can be found in the documentation.\nThe only consideration to be made is during the Initial Setup Wizard when specifying the data engine (Classic or VMC). When the Classic data engine is selected, the solution user credentials need to be provided when connecting to the Lookup service. And when the VMC engine is selected, both CloudOwner and solution user can be used for connecting to the Lookup service.\n"},{"id":40,"href":"/docs/dr-migration/multi-nic-config/","title":"VMware Cloud Director Availability Multi-NIC Configuration","section":"DR \u0026 Migration","content":"VMware Cloud Director Availability Multi-NIC Configuration #  Overview #  Placing the VMware Cloud Director Availability appliances in different networks is something common for Service Providers. This leads to the need to use multiple network interfaces to support that scenario and guarantee the product will continue operating as per design.\nThere are a few main reasons that lead to this design:\n  Bypass a router\n  Port forwarding is not possible\n  Inability to route the replication traffic\n  Separation of the incoming replications to different isolated networks\n  Design #  You need to take two considerations before the implementation of VMware Cloud Director Availability appliances that will have multiple NICs:\n  Which interface will be used for the communication with the rest of the VMware Cloud Director Availability appliances?\n  How will the routing be organized - to which interface the default gateway will be configured and what static routes will be required?\n  The General recommendation (GR01) is to deploy the appliances with the interfaces that will be used for communicating with each other. When following this recommendation, the VMware Cloud Director Availability services will discover and set to use the first NIC (ens160) and its first IP address. The only additional change that might be required is to move the default gateway to a different NIC and configure one or more static routes on ens160.\nMoving the Default Gateway #  To perform the necessary steps, you have to connect to the appliance. The recommended way is to use DCUI because the configuration changes might lead to losing network connectivity.\n First, you need to unconfigure ens160. Example command:  /opt/vmware/h4/bin/net.py unconfigure-nic ens160\nThen configure only the IP address of ens160 without setting a default gateway on it. Example command:   /opt/vmware/h4/bin/net.py configure-nic --static -a 192.168.10.10/24 ens160\nConfigure the static route(s) for ens160. Example command:   /opt/vmware/h4/bin/net.py add-route ens160 destination_subnet gateway metric\nNote: You can use this command multiple times for setting more than one static route.\n(Optional) Configure the name servers. Example command:   /opt/vmware/h4/bin/net.py configure-dns --servers ns01_ip ns02_ip --search-domain domain1 domain2 \nOnce you finish the configuration and the appliance is accessible over the network, you can connect through SSH or use the UI.\nReconfigure the VMware Cloud Director Availability Appliances #  For cases where GR01 is not followed, some actions need to be taken so the VMware Cloud Director Availability appliances can use the IP address set to an interface different from ens160.\nCloud Tunnel #  It is essential to know that only one of the Cloud Tunnel interfaces can be used for communication with the rest of the VMware Cloud Director Availability appliances in the local site. For pairing purposes all the interfaces of the tunnel appliance can be used.\nTo reconfigure the Cloud Tunnel, you need to:\n  Set IP addresses to all interfaces and configure the static routes.\n  Log in as root through SSH to the Tunnel appliance.\n  Authenticate as root to the Tunnel service via CLI. Example command:\n   h4 -k tunloginroot 'r00t_Password'\nCheck the current service configuration. Example command:   h4 -k tunendpoints\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8047, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8047, \u0026quot;tunnelAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;tunnelPort\u0026quot;: 8048, \u0026quot;tunnelPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;tunnelPublicPort\u0026quot;: 443 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8047, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8047, \u0026quot;tunnelAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;tunnelPort\u0026quot;: 8048, \u0026quot;tunnelPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;tunnelPublicPort\u0026quot;: 443 } } The important parameter is tunnelAddress. The value of this parameter will be used to configure the other local VMware Cloud Director Availability appliances – Cloud Replication Manager and Cloud Replicator(s) - when they are prepared to communicate with the tunnel. This parameter can be set to a specific IP address or null. Setting it to null will lead to the service discovering the first IP address in the system and use it. In this example, the ens160 IP address is 192.168.1.2, and the ens192 IP address is 172.18.24.4.\nNext, we need to configure the Tunnel service to use the IP address of ens192. Example command:   h4 -k tunsetendpoints \u0026quot;192.168.1.2\u0026quot; \u0026quot;8047\u0026quot; \u0026quot;192.168.1.2\u0026quot; \u0026quot;8047\u0026quot; \u0026quot;172.18.24.4\u0026quot; \u0026quot;8048\u0026quot; \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot; \u0026quot;443\u0026quot;\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8047, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8047, \u0026quot;tunnelAddress\u0026quot;: \u0026quot;172.18.24.4\u0026quot;, \u0026quot;tunnelPort\u0026quot;: 8048, \u0026quot;tunnelPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;tunnelPublicPort\u0026quot;: 443 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8047, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;192.168.1.2\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8047, \u0026quot;tunnelAddress\u0026quot;: \u0026quot;172.18.24.4\u0026quot;, \u0026quot;tunnelPort\u0026quot;: 8048, \u0026quot;tunnelPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;tunnelPublicPort\u0026quot;: 443 } } Note: Make sure the parameters are provided in the correct order.\nWith this, the Cloud Tunnel is properly configured to use a non-default IP address.\nOnce the tunnelAddress has its new value, please navigate to the VMware Cloud Director Availability Portal (\u0026lt;https://manager_appliance_IP_address/ui/admin) and re-enable the tunneling to propagate the change to all local components – Cloud Replication Manager and Cloud Replicator(s).\nCloud Replication Manager #  The Cloud Replication Manager runs two services (Cloud service and Manager service) that require reconfiguration as both need to use the same interface to communicate with the rest of the appliances. For communication with the VMware Cloud Director cells and vCenters, the services can use any interface.\nCloud service #    Set IP addresses to all interfaces and configure the static routes.\n  Log in as root through SSH to the Cloud Replication Manager appliance.\n  Authenticate as root to the Cloud service via CLI. Example command:\n  c4 loginroot 'r00t_Password'\nCheck the current service configuration. Example command:  c4 endpoints\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: null, \u0026quot;mgmtPort\u0026quot;: 8046, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-e2eecba4-8381-4799-a39a-0ec4eca105bb.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;apiAddress\u0026quot;: null, \u0026quot;apiPort\u0026quot;: 8443, \u0026quot;apiPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;apiPublicPort\u0026quot;: 443 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;192.168.2.81\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8046, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-e2eecba4-8381-4799-a39a-0ec4eca105bb.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;apiAddress\u0026quot;: \u0026quot;vcavm1.ber.cloudprovider.local\u0026quot;, \u0026quot;apiPort\u0026quot;: 8443, \u0026quot;apiPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;apiPublicPort\u0026quot;: 443 } } Note: Because the Cloud Replication Manager appliance is already prepared for tunneling, the mgmtPublicAddress parameter has this value tn-\u0026lt;\u0026lt;uuid\\.tnexus.io. It should NOT be changed in the next step but should be used as it is.\nThe parameter that has to be changed is mgmtAddress. In the Configured section it is set to null which means the service will try to discover an interface with a successfully configured IP address and bind itself to this address.\nIn this example, the ens160 IP address is 192.168.2.81, and the service currently uses this interface. The IP address of the ens192 interface is 172.17.31.81.\nWe will configure mgmtAddress to use ens192 with IP address 172.17.31.81. Example command:   c4 -k setendpoints \u0026quot;172.17.31.81\u0026quot; \u0026quot;8046\u0026quot; \u0026quot;tn-e2eecba4-8381-4799-a39a-0ec4eca105bb.tnexus.io\u0026quot; \u0026quot;8048\u0026quot; \u0026quot;vcavm1.ber.cloudprovider.local\u0026quot; \u0026quot;8443\u0026quot; \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot; \u0026quot;443\u0026quot;\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.31.81\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8046, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-e2eecba4-8381-4799-a39a-0ec4eca105bb.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;apiAddress\u0026quot;: \u0026quot;vcavm1.ber.cloudprovider.local\u0026quot;, \u0026quot;apiPort\u0026quot;: 8443, \u0026quot;apiPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;apiPublicPort\u0026quot;: 443 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.31.81\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8046, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-e2eecba4-8381-4799-a39a-0ec4eca105bb.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;apiAddress\u0026quot;: \u0026quot;vcavm1.ber.cloudprovider.local\u0026quot;, \u0026quot;apiPort\u0026quot;: 8443, \u0026quot;apiPublicAddress\u0026quot;: \u0026quot;vcav01.ber.cloudprovider.pub\u0026quot;, \u0026quot;apiPublicPort\u0026quot;: 443 } } Restart the Cloud service. Example command:  systemctl restart manager\nNote: This operation won’t break any of the running replication tasks but will only reload the Cloud Replicator configuration and ensure all paired on-premises and remote cloud appliances receive the correct information for the replicator configuration in the local site.\nOnce you perform these steps, the Cloud service is configured, and you can proceed to the steps for the Manager service.\nManager service #    Log in as root through SSH to the Cloud Replication Manager appliance.\n  Authenticate as root to the Manager service via CLI. Example command:\n  h4 loginroot 'r00t_Password'\nCheck the current service configuration. Example command:  h4 endpoints\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: null, \u0026quot;mgmtPort\u0026quot;: 8044, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-853bd005-b905-4da1-bb75-e17996efb5df.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;192.168.2.81\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8044, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-853bd005-b905-4da1-bb75-e17996efb5df.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048 } } Note: The mgmtAddress has null value. This is the parameter that will be changed in the next step.\nChange the mgmtAddress. Example command:  h4 setendpoints \u0026quot;172.17.31.81\u0026quot; \u0026quot;8044\u0026quot; \u0026quot;tn-853bd005-b905-4da1-bb75-e17996efb5df.tnexus.io\u0026quot; \u0026quot;8048\u0026quot;\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.31.81\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8044, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-853bd005-b905-4da1-bb75-e17996efb5df.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.31.81\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8044, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-853bd005-b905-4da1-bb75-e17996efb5df.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048 } } With this, the configuration of the Replication Manager is completed.\nTo restore the regular operation of VMware Cloud Director Availability, you will need to:\n  Re-register all the Cloud Replicators via the Manager service UI (\u0026lt;https://manager_appliance_IP_address:8441)\n  Enable tunneling via the VMware Cloud Director Availability Portal (\u0026lt;https://manager_appliance_IP_address/ui/admin)\n  Cloud Replicator(s) #  The idea behind having multiple interfaces for the Cloud Replicator is to optimize replication traffic flow. When the Cloud Replicator has more than one NIC, one of the interfaces should be able to communicate with the rest of the VMware Cloud Director Availability appliances and the other one should be in the same Layer 2 broadcast domain as the ESXi vmkernel interface. The Cloud Replicator can\u0026rsquo;t use one interface to communicate with the Cloud Tunnel and another interface to communicate with the Cloud Replication Manager.\n  Set IP addresses to all interfaces and configure the static routes.\n  Log in as root through SSH to the Cloud Replicator appliance.\n  Authenticate as root to the Replicator service via CLI. Example command:\n  h4 rtrloginroot 'r00t_Password'\nCheck the current service configuration. Example command:  h4 rtrendpoints\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: null, \u0026quot;mgmtPort\u0026quot;: 8043, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;nfcAddress\u0026quot;: null, \u0026quot;lwdAddress\u0026quot;: null, \u0026quot;lwdPort\u0026quot;: null, \u0026quot;lwdPublicAddress\u0026quot;: \u0026quot;lw-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;lwdPublicPort\u0026quot;: 8048 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.33.1\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8043, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;nfcAddress\u0026quot;: null, \u0026quot;lwdAddress\u0026quot;: \u0026quot;172.17.33.1\u0026quot;, \u0026quot;lwdPort\u0026quot;: 44045, \u0026quot;lwdPublicAddress\u0026quot;: \u0026quot;lw-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;lwdPublicPort\u0026quot;: 8048 } } Note: Because the Cloud Replicator appliance is already prepared for tunneling, the mgmtPublicAddress parameter has this value tn-\u0026lt;\u0026lt;uuid\\.tnexus.io. It should NOT be changed in the next step but should be used as it is.\nTwo parameters require changing:\n  mgmtAddress – this is the IP address of the interface that will be used for communicating with the other VMware Cloud Director Availability appliances in the local site\n  lwdAddress – this is the IP address of the interface that will be used for communicating with the replication vmkernel interface of the ESXi host(s).\n  Change the mgmtAddress and lwdAddress. Example command:  h4 rtrsetendpoints \u0026quot;172.17.33.1\u0026quot; \u0026quot;8043\u0026quot; \u0026quot;tn-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot; \u0026quot;8048\u0026quot; \u0026quot;\u0026quot; \u0026quot;192.168.3.1\u0026quot; \u0026quot;lw-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot; \u0026quot;8048\u0026quot;\nNote: The parameters need to be in this exact order:\nh4 rtrsetendpoints **mgmtAddress** mgmtPort mgmtPublicAddress mgmtPublicPort nfcAddress **lwdAddress** lwdPublicAddress lwdPublicPort\nExpected output:\n{ \u0026quot;configured\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.33.1\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8043, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;nfcAddress\u0026quot;: null, \u0026quot;lwdAddress\u0026quot;: \u0026quot;192.168.3.1\u0026quot;, \u0026quot;lwdPort\u0026quot;: null, \u0026quot;lwdPublicAddress\u0026quot;: \u0026quot;lw-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;lwdPublicPort\u0026quot;: 8048 }, \u0026quot;effective\u0026quot;: { \u0026quot;mgmtAddress\u0026quot;: \u0026quot;172.17.33.1\u0026quot;, \u0026quot;mgmtPort\u0026quot;: 8043, \u0026quot;mgmtPublicAddress\u0026quot;: \u0026quot;tn-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;mgmtPublicPort\u0026quot;: 8048, \u0026quot;nfcAddress\u0026quot;: null, \u0026quot;lwdAddress\u0026quot;: \u0026quot;192.168.3.1\u0026quot;, \u0026quot;lwdPort\u0026quot;: 44045, \u0026quot;lwdPublicAddress\u0026quot;: \u0026quot;lw-f9abd9de-3978-4383-a951-514deaec522f.tnexus.io\u0026quot;, \u0026quot;lwdPublicPort\u0026quot;: 8048 } } Note: You might get the following output:\n \u0026quot;code\u0026quot;: \u0026quot;TrafficIsolationConfigurationFailure\u0026quot;, \u0026quot;msg\u0026quot;: \u0026quot;Traffic isolation configuration could not be applied to hbr server.\u0026quot;, \u0026quot;args\u0026quot;: [], \u0026quot;stacktrace\u0026quot;: In such cases, just check if the configuration has been updated correctly using:\nh4 rtrendpoints\nWith this, the configuration of the Cloud Replicator is completed.\nWhen the mgmtAddress parameter is changed, the Cloud Replicator needs to be re-registered in the Cloud Replication Manager via the Manager service UI (\u0026lt;https://manager_appliance_IP_address:8441).\nUpgrade #  Upgrading the VMware Cloud Director Availability appliances will not affect the network interface configuration.\n"},{"id":41,"href":"/docs/cloud-infrastructure/cds-gcve-deployment-guide-msp/","title":"VMware Cloud Director Service for Google Cloud VMware Engine Deployment Guide","section":"Cloud Infrastructure","content":"Introduction #  This white paper is intended for VCPP Cloud Providers who need guidance on how to configure Google Cloud VMware Engine with VMware Cloud Director service (CDs). The content below describes the manual deployment process required to setup the Google Cloud projects, configure them, deploy a Software Defined Data Center (SDDC), associate it to CDs, and the process to configure a virtual private network (VPN) solution for connectivity to isolated networks.\nA virtual private network (VPN) provides traffic tunneling through an encrypted connection, preventing it from being seen or modified in transit. VMware NSX® Data Center for vSphere includes a remote access VPN feature (IPsec) that allows the connection from a remote site to connect securely to the private networks and applications in the organization virtual data center.\nDue to a limitation in VPN technology in Google Cloud, the provider will need to select an alternative solution such as an open source or commercially available, depending on the required features and available support. Examples of open source solutions include OpenVPN or StrongSwan. This deployment guide will walk through the steps required to implement the StrongSwan solution to provide the VPN connectivity between the provider-managed customer project and the T1 edge that sits in front of the tenant networks. The VPN solution is deployed and configured manually in addition to being managed separately from CDs.\nDisclamer\nVMware does not endorse, recommend, or support any third-party utility or solution.\nA general knowledge of networking and security, as well as on VMware Cloud Director concepts is also required.\nConfigure Provider Project and Create Required Resources #  The following section describes the steps required to setup the cloud provider project. Before proceeding, the projects should be created, and the account used to configure them have the appropriate permission to configure all aspects of the projects.\nSetup the Provider Project #  The following steps describes the steps required to configure the provider project.\n Log into the Google Cloud console and click \u0026ldquo;Select a project\u0026rdquo; dropdown.     On the \u0026ldquo;Select a project\u0026rdquo; box, click the link for the name of the provider project.     In the top left pane, click the three horizontal bars and navigate down to Network -\u0026gt; Network Services -\u0026gt; and click Cloud DNS.      If prompted to enable the API first, click Enable API.\n  In the top left pane, click the three horizontal bars and navigate down to Compute -\u0026gt; and select VMware Engine. Note that this will open a second browser tab, keep both tabs open for easier navigating when configuring later.\n     Click Enable API.     In the top left pane click the three horizontal bars and navigate to Networking -\u0026gt; VPC Network -\u0026gt; and select VPC Networks.     Click CREATE VPC NETWORK.      Enter in the following:\n  Name: Subnet name such as the region the environment is in such as \u0026ldquo;asia-southeast1\u0026rdquo;\n  Region: Select the region the environment is in to host the subnet such as \u0026ldquo;asia-southeast1\u0026rdquo;\n  IP Address Range: Provide a range such as 100.64.0.0/20\n  Check the box for \u0026ldquo;I confirm that my subnet configuration includes a range outside of the RFC 1918 address space\u0026rdquo;\n  Private Google Access: Select the ON radial button\n        Under Dynamic Routing Mode:\n  Select Global\n  Set the Maximum MTU to 1500\n  Click Create\n       Once the task has completed, scroll to the bottom of the page, and click the name of the provider management network that was just created.     In the VPC network details screen, click the PRIVATE SERVICE CONNECTION tab and click the Enable API Button.     Once enabled, click ALLOCATE IP RANGE.      For Allocate an internal IP range enter:\n  Name: service-network\n  IP Range: The next subnet available in the provider range previously entered, 100.64.16.0/24 in this case\n  Click ALLOCATE\n       Click the PRIVATE CONNECTIONS TO SERVICES tab and click CREATE CONNECTION.     On the private connection screen, ensure Google Cloud Platform is selected and select the service-network that was created under Assigned Allocation and click Connect.    Create and Configure the Software Defined Data Center #  The following section describes the steps required to setup the Software Defined Data Center (SDDC) that tenants will consume for resources.\n Go to the browser tab that has GCVE open and click Create a Private Cloud.      Enter the following information:\n  Private Cloud Name: Name of the SDDC to create\n  Location: The GCP data center to create the SDDC\n  Node Type: Multi Node for production deployments\n  Node Count: Number of nodes to initially use for the SDDC (4 minimum)\n  vSphere/vSAN Range: IP range to use for vSphere and vSAN\n  HCX Deployment Range: IP range to use for HCX (while the input is required, using HCX is not required)\n  Click Review and Create when ready\n     Click Create\n  The SDDC creation process takes around one hour to complete.\n    Once the SDDC completion process works, the next steps are completed most easily by have two browser tabs open: one for the GCVE environment and one on the provider project settings.\n  On the browser tab with GCVE, in the left pane click Network and in the right pane click Add Private Connection.\n      Enter the following information:\n  Service: VPC Network\n  Region: The region the SDDC was created in\n  Peer Project ID: this will be the provider project name, which can be found in the browser tab with the provider project by clicking Google Cloud Platform and then copying the Project Name field and pasting it into the Peer Project ID field\n       Peer Project Number: This is the project ID, which can be found in the tab where the Project Name filed is immediately under that; copy and paste the value into the Peer Project Number field      Peer VPC ID: This will be \u0026ldquo;provider-mgmt-network\u0026rdquo; unless it was named differently. If it was name differently, enter the name used.\n  Tenant Project ID: To get the tenant project ID value, on the browser tab with the provider project, in the left pane click VPC Network -\u0026gt; VPC Network Peering and in the right pane, copy the value for Peered Project ID and paste it into this field.\n     Verify the Routing Mode is set to Global and click Submit.     After about 5 minutes, the Region Status should show Connected.     Navigate back to VPC Network -\u0026gt; VPC Networks -\u0026gt; and click on the provider-mgmt-network.     Click on the VPC NETWORK PEERING tab and click into the servicenetworking name.     Click on EDIT.     Under Exchange Custom Routes, check the boxes for Import Custom Routes and Export Custom Routes and click SAVE.     After saving, the route Status should all change to a Status of accepted.     On the GCVE browser tab, click Network in the left pane and then in the right pane click the REGIONAL SETTINGS tab and click Add Region.     On the Add Region screen, select the Region the SDDC is in, enable Internet Access and Public IP Service, and use the next provider CIDR block for the Edge Services CIDR and click Submit. After a few minutes, it should show that the status is Operational and Enabled.    Setup the Tenant Projects #  The following steps describes the steps required to configure the tenant projects. Many of the steps are the same done in the provider tenant in creating a tenant service network and peering projects.\n In the Google Cloud Portal, click the project name dropdown beside Google Cloud Platform.     On the Select a project screen, click the ALL tab and select the tenant project.     In the top left pane click the three horizontal bars and navigate to VPC Network -\u0026gt; VPC Networks -\u0026gt; and click the Default network name.     Click DELETE VPC NETWORK.      Click DELETE when prompted to confirm\n  Click CREATE VPC NETWORK.\n      Enter the following:\n  Name: Name for the network such as \u0026ldquo;tenantname-transit\u0026rdquo;\n  New Subnet Name: Name to match the region such as asia-southeast1\n  Region: The region the SDDC is in\n  IP Address Range: A CIDR range out of the next available range. Check the box for \u0026ldquo;I confirm the subnet configuration includes a range outside the RFC 1918 address space\u0026rdquo;\n  Private Google Access: Turn On\n  Click Done on the Subnet Section\n  Dynamic Routing Mode: Global\n  MTU: 1500\n  Click CREATE\n    Once the network creation has completed, click into the network name.\n     Click the PRIVATE SERVICE CONNECTION tab and then click ENABLE API.     Once completed, click ALLOCATE IP RANGE.      Set the following:\n  Name: service-network\n  IP Range Custom: Next available CIDR range\n  Click Allocate\n       Click the PRIVATE CONNECTIONS TO SERVICES tab and then click CREATE CONNECTION.     On the private connection screen, ensure Google Cloud Platform is selected and for Assigned Allocation, select the service-network that was just created and click CONNECT.     In the left pane click on VPC Network Peering and in the right pane click into the servicenetworking name.     Click EDIT.     Under Exchange Custom Routes, check the boxes for Import Custom Routes and Export Custom Routes and click SAVE.     Navigate to the GCVE page and in the left pane click Network and in the right pane click Add Private Connection.      Enter the following information:\n  Service: VPC Network\n  Region: Region the SDDC is in\n  Peer Project ID: this will be the provider project name, which can be found in the browser tab with the provider project by clicking Google Cloud Platform and then copying the Project Name field and pasting it into the Peer Project ID field\n       Peer Project Number: This is the project ID, which can be found in the tab where the Project Name filed is immediately under that; copy and paste the value into the Peer Project Number field      Peer VPC ID: This will be \u0026ldquo;tenantname-transit\u0026rdquo; unless it was named differently. If it was name differently, enter the name used.\n  Tenant Project ID: To get the tenant project ID value, on the browser tab with the provider project, in the left pane click VPC Network -\u0026gt; VPC Network Peering and in the right pane, copy the value for Peered Project ID and paste it into this field.\n     Verify the Routing Mode is set to Global and click Submit.      After about 5 minutes the Region status should show Connected.\n  Repeat the previous steps for configure each tenant project.\n  Create a Jumphost in the Provider Project and Allow Network Access #  The following section describes the steps required to create a jumphost in the provider project to use for vCenter and NSX access as well as other potential tasks made easier with local access.\n In a browser navigate to the provider project as previously described and in the top right pane click the three horizontal bars and select Compute Engine -\u0026gt; VM Instances.     Click CREATE INSTANCE.      Enter the following information:\n  Name: A name for the VM to help the region and function\n  Region: Same region as the SDDC\n  Zone: Zone in the region of the SDDC\n       Scroll down to the Boot Disk section and click CHANGE      Change the following:\n  Operating System: Windows Server\n  Version: Select a current version, such as 2019 Datacenter\n  Size: Change the default size if desired\n  Click SELECT\n       Scroll down and expand NETWORKING, DISKS, SECURITY, MANGEMENT, SOLE-TENANCY     Under \u0026ldquo;Edit network interface\u0026rdquo;, click the Network drop down and select the provider-mgmt-network that was previously created and then click DONE\u0026lt; then click CREATE     After several minutes the jumphost should show ready; click on the name of it to open the settings.     Click SET WINDOWS PASSWORD     On the pop-up screen, it will fill in the Username of the person logged in, click SET to set the password.     After a few moments it will display the password that is set. Copy the password and then click CLOSE.     Allow access through the firewall by clicking the three horizontal bars in the top left and select VPC Network -\u0026gt; Firewall.     Click CREATE FIREWALL RULE.      Set the following for the firewall rule:\n  Name: Name stating the service provided such as \u0026ldquo;rdp-in\u0026rdquo;\n  Network: The provider-mgmt-network that was created previously\n  Direction of Traffic: Ingress\n  Targets: All instances in the network\n  Source IPv4 ranges: 0.0.0.0/0\n  Protocols and ports: TCP 3389\n  Click CREATE\n    Click CREATE FIREWALL RULE again\n  Set the following for the firewall rule:\n  Name: This will be for east-west connectivity, so name it to identify what it is such as \u0026ldquo;ew\u0026rdquo; or \u0026ldquo;east-west\u0026rdquo;\n  Network: The provider-mgmt-network that was created previously\n  Direction of Traffic: Egress\n  Targets: All instances in the network\n  Source IPv4 ranges: The range of the management network, such as 100.64.0.0/16\n  Protocols and ports: Allow all\n  Click CREATE\n    On the settings of the jumphost that was created, copy the External IP to use in RDP.\n     Log into the jumphost with the external IP and the credentials previously created to verify access.  Prepare for and Deploy the Reverse Proxy #  The following section describes the steps required to prepare the environment for the proxy, generate it, and then deploy and associate the proxy to the SDDC that was created.\nCreate the CDs Instance (If not already created) #  The following steps describes the steps required to create a CDs instance if one does not already exist that the SDDC should be associated to.\n In Partner Navigator, navigate to VMware Cloud Director service and click CREATE INSTANCE.     Enter the required information and click CREATE INSTANCE.   The CDs instance will take around 30 minutes to complete.  Generate the Proxy #  The following steps describes the steps required to generate the proxy that will be used for the connection from CDs to the SDDC. These steps are completed from the partner navigator portal and require the CDs instance to already exist.\n  On the GCP provider based jumphost, log into Partner Navigator, navigate to VMware Cloud Director service.\n  On the CDs instance to associate the SDDC to click Actions and select Generate VMware Reverse Proxy OVF.\n      Enter the following:\n  API Token: API token from your account in Partner Navigator\n  Datacenter Name: Datacenter\n  vCenter FQDN: The FQDN of the VCSA appliance under Google Cloud VMware Engine -\u0026gt; Resources -\u0026gt; VSHPERE MANAGEMENT NETWORK\n       Management IP for vCenter: The IP address of the VCSA appliance     NSX URL: URL of the NSX manager     Additional hosts within the SDDC to proxy: The IP address of the ESXi hosts that are part of the SDDC. Note that each IP MUST be on a separate line     Once the information has been entered, click GENERATE VMWARE REVERSE PROXY OVF      The Activity Log on the CDs instance can be monitored for the status of the task. Skip ahead to Prepare the Environment for the Proxy if desired to complete those steps while waiting for the proxy to generate.\n  Once the task has completed, click the three horizontal dots, and select View Files.\n     Click the down arrow icon to download the OVF file to the provider jumphost locally.    Prepare the Environment for the Proxy #  Before deploying the proxy, a network segment must be created and DHCP setup so that it can get an IP address. The following steps describe how to create the segment and configure DHCP.\n On the GCVE page in the Google Cloud Portal, click Resource in the left pane and click the name of the SDDC.     Click on VSPHERE MANAGEMENT NETWORK tab and in the right click the NSX Manager FQDN and copy the link location.      RDP to the jumphost that was created on the provider network previously, open a browser and navigate to the FQDN of the NSX manager from the link that was copied.\n  Back on the GCVE browser, click the SUMMARY tab and then in the NSX-T Login Info section, click View.\n     In the Password section, click Copy.      Log into the NSX-T manager URL as admin with the password that was coped.\n  In the NSX-T manager UI, click the Networking tab in the left pane click DHCP.\n     Click ADD DHCP PROFILE.      Enter the following:\n  Profile Name: A name for the DHCP profile\n  Profile Type: DHCP Server\n  Server IP Address: A CIDR subnet for the scope\n  Edge Cluster: The edge cluster that was created\n  Click SAVE\n       In the left pane click Tier-1 Gateways and in the right pane beside Tier1, click the three vertical dots and select Edit.     Click Set DHCP Configuration.     For Type select DHCP Server and for DHCP Server Profile select the DHCP Profile that was created and click SAVE.     Click SAVE.     In the left pane click Segments, then click ADD SEGMENT.      Enter the following:\n  Segment Name: Proxy\n  Connected Gateway: Tier1 | Tier1\n  Transport Zone: TZ-Overlay\n  Subnet: The subnet CIDR\n  Click EDIT DHCP CONFIG\n       Check the DHCP Config enabled, set a DHCP range, enter the DNS servers from GCVE, and then click APPLY (the DNS servers can be found on the GCVE page, clicking Resources in the left tab and then clicking the name of the SDDC, and on the SUMMARY tab under Private Cloud DNS Servers.    Deploy and Connect the Proxy #  The following steps describes the steps required to deploy the proxy and associate it to the CDs instance in Partner Navigator.\n  On the GCP provider based jumphost, open a browser to the vCenter UI.\n  Right click Cluster and select Deploy OVF Template.\n     Select Local File and navigate to the reverse proxy OVF that was downloaded, select it and click NEXT.     Provide a virtual machine name and click NEXT.     Select the Cluster name and click NEXT.      Click Next.\n  Select the vsanDatastore and click NEXT.\n      Select the Proxy network that was previously created and click NEXT.\n  On the Customize Template page, copy and save the root password and click NEXT.\n  Click FINISH to being the deployment.\n  After deployment, power on the appliance.\n  Log into the proxy appliance and verify it has an IP address by running \u0026ldquo;ip a\u0026rdquo;.\n  Run the command \u0026ldquo;systemctl status transporter-client.service\u0026rdquo; and ensure it shows running.\n     If the transporter-client.service is showing an error, verify that DNS resolution is working properly and that it can access the Internet. The below screenshot shows an error when DNS is not working.     Run the command \u0026ldquo;transporter-status.sh\u0026rdquo; and verify it shows connected.     In Partner Navigator, go to the CDs instance the proxy was generated from and click Actions -\u0026gt; Associate a Datacenter via VMware Proxy.      Enter the following:\n  Datacenter name: Datacenter\n  vCenter FQDN: VCSA FQDN that was used to generate the proxy\n  NSX URL: URL of NSX manager that was used to generate the proxy\n  It will attempt an initial connection to the proxy and if it connects, it will display Connection Established\n        On the Credentials page, enter the following:\n  vCenter Username: cloudowner@gve.local\n  vCenter Password: The password for the supplied username\n  Disconnected Network Segment: Enter the name of the network the proxy is on (Proxy)\n  Authentication to NSX Type: Authenticate via NSX Username and Password\n  NSX Username: admin\n  NSX Password: The password for NSX admin account\n       Check the box to acknowledge charges will begin and click SUBMIT.     The Activity Log on the CDs instance can be monitored for the status of the association task.      It should take about 5 to 10 minutes for the task to completed.\n  Once the task has finished, it can take up to 4 hours to show up as an associated SDDC in the CDs instance. Opening the VCD instance to bring up the UI should show the SDDC as a PVCD that can be used to create VDCs for tenants; you do not have to wait for it to show up as associated in the Partner Navigator portal.\n  Deploy and Configure IPsec Tunnel #  The following section describes the steps required to deploy and configure a StrongSwan VPN appliance in the tenant\u0026rsquo;s project to connect to their T1 in the SDDC that was deployed via an IPsec tunnel. This is merely a demonstration of how to deploy a VPN appliance and any suitable appliance can be used.\nThe steps below are based on CentOS 7 as the operating system; using another flavor of Linux may result in different steps or actions required to get it to work properly.\nThe default routes to the Internet will use instance tags to keep from the routes leaking back into the GCVE environment. This tag can be whatever the provider desires, but it must be uniform across all routes that point to the Internet and be applied to any VM that will need to be access to/from to the Internet in the provider owned customer project.\nDeploy a Linux Instance and Configure StrongSwan #  The following steps describes the steps required to deploy a virtual machine, install StrongSwan and configure it for an IPSec tunnel connection to a tenant T1.\n First create a firewall rule in the tenant project by going to VPC Network -\u0026gt; Firewall and click CREATE FIREWALL RULE.      Enter the following:\n  Name: gcve-transit\n  Network: tenatname-transit\n  Priority: 100\n  Direction of Traffic: Ingress\n  Action on match: Allow\n  Targets: All instances in the network\n  Source IPv4 Ranges: Range for transit network \u0026ndash; such as 100.64.0.0/16\n  Protocols and ports: Allow all\n  Click Save\n    Click CREATE FIREWALL RULE again and enter the following:\n  Name: ipsec-egress\n  Network: tenatname-transit\n  Priority: 100\n  Direction of Traffic: Egress\n  Action on match: Allow\n  Targets: All instances in the network\n  Destination IPv4 Ranges: Range for transit network \u0026ndash; such as 100.64.0.0/16\n  Protocols and ports: IPsec Ports\n  Click Save\n    Create a new instance in the tenant and set:\n  Boot Disk: Change to CentOS 7\n  Expand NETWORKING, DISKS, SECRUITY, MANGEMENT, SOLE-TENANCY\n  IP Forwarding: Check the box to Enable\n  Ensure the network interface is on the \u0026ldquo;tenantname-transit\u0026rdquo; network\n  Click CREATE\n      Once completed, click on the name of the instance to bring up its settings.\n     At the top of the screen, click EDIT.\n     Scroll down and in the Network Tags box, put the network tag name, then save the settings.\n     Under the Connect column click SSH to connect to it.\n  Run sudo su and then run \u0026ldquo;yum install strongswan -y\u0026rdquo; to install strongswan.\n  Copy the command below and paste into the shell with ctrl v\n   cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; EOF\nnet.ipv4.ip_forward = 1\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.all.send_redirects = 0\nEOF\n   Run the command sysctl -p\n  CD to /etc/strongswan/swanctl/conf.d and run vi nsxt.conf.\n Enter the following information in the nsxt.conf file replacing localAddr with the local IP of the tunnel, remoteAddr with the remote IP of the tunnel, remoteTS with the network CIDR of the remote end of the tunnel, and PresharedKey with the secret used for the tunnel.     connections {\ngw-gw {\nlocal_addrs = localAddr\nremote_addrs = remoteAddr\nlocal {\nauth = psk\nid = localAddr\n}\nremote {\nauth = psk\nid = remoteAddr\n}\nchildren {\nnet-net {\nlocal_ts = 0.0.0.0/0\nremote_ts = remoteTS\nstart_action = start\nupdown = /usr/local/libexec/ipsec/_updown iptables\nesp_proposals = aes128gcm128-modp2048\n}\n}\nversion = 2\nproposals = aes128-sha256-modp2048\n}\n}\nsecrets {\nike-1 {\nid-1 = localAddr\nsecret = PresharedKey\n}\nike-2 {\nid-2 = remoteAddr\nsecret = PresharedKey\n}\nike-3 {\nid-3a = localAddr\nid-3b = remoteAddr\nsecret = PresharedKey\n}\nike-4 {\nsecret = PresharedKey\n}\nike-5 {\nid-5 = localAddr\nsecret = PresharedKey\n}\n}\n   Run the command swanctl --load-all\n  CD to /etc and run \u0026ldquo;vi ipsec.secrets\u0026rdquo;\n Enter the following line, replacing the words with their values: localTunnelIP remoteTunnelIP : PSK \u0026lsquo;PresharedKey\u0026quot; and save the file.        Run the command sudo strongswan restart\n  Run the command yum install iptables-services and once installed run systemctl start iptables\n  Add the following iptables rules in to allow traffic to be forwarded. Any line that contains remoteNet should have that replaced with the CIDR of the remote network in GCVE. Note that each line must be copied and pasted into the SSH session on the VPN server one by one.\n   iptables -A INPUT -i eth0 -p esp -j ACCEPT\niptables -A INPUT -i eth0 -p ah -j ACCEPT\niptables -A INPUT -i eth0 -p udp -m udp --sport 500 --dport 500 -j ACCEPT\niptables -A INPUT -i eth0 -p udp -m udp --sport 4500 --dport 4500 -j ACCEPT\niptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT\niptables -A FORWARD -s remoteNet -d 0.0.0.0/0 -i eth0 -m policy --dir in --pol ipsec --reqid 1 --proto esp -j ACCEPT\niptables -A FORWARD -s 0.0.0.0/0 -d remoteNet -o eth0 -m policy --dir out --pol ipsec --reqid 1 --proto esp -j ACCEPT\niptables -A OUTPUT -o eth0 -p esp -j ACCEPT\niptables -A OUTPUT -o eth0 -p ah -j ACCEPT\niptables -A OUTPUT -o eth0 -p udp -m udp --sport 500 --dport 500 -j ACCEPT\niptables -A OUTPUT -o eth0 -p udp -m udp --sport 4500 --dport 4500 -j ACCEPT\niptables -A OUTPUT -p tcp -m tcp --sport 22 -j ACCEPT\niptables -A FORWARD -s 0.0.0.0/0 -d remoteNet -i eth0 -m policy --dir in --pol ipsec --reqid 1 --proto esp -j ACCEPT\niptables -A FORWARD -s remoteNet -d 0.0.0.0/0 -o eth0 -m policy --dir out --pol ipsec --reqid 1 --proto esp -j ACCEPT\n  Delete the two listed REJECT rules by running \u0026ldquo;iptables -D SECTION_NAME position#\u0026rdquo;. For example, in the screen shot below, the REJECT under INPUT is the 5^th^ rule down, so the command to delete it is \u0026ldquo;iptables -D INPUT 5\u0026rdquo;. Notice after running the command the REJECT rule under INPUT is no longer present.      To delete the REJECT under FORWARD, run \u0026ldquo;iptables -D FORWARD 1\u0026rdquo; as it is in position 1.\n  Run the command service iptables save\n  Run systemctl restart iptables\n  In the GCP console in the tenant project navigate to VPC Network -\u0026gt; Routes and click CREATE ROUTE.\n  Enter the following:\n  Name: tenantname-networkcidr\n  Network: tenantname-transit\n  Destination Range: IP CIDR range in the SDDC for the tenant\n  Priority: 100\n  Next Hop: Specify an instance\n  Next Hop Instance: The StrongSwan VM.\n  Click CREATE\n      Configure IPSec VPN in NSX and Configure Tenant Firewall Rules #  The following steps describes the steps required to create a CDs instance if one does not already exist that the SDDC should be associated to.\n  Log into Partner Navigator and navigate to Cloud Director Service and open the instance that is managing the GCVE SDDC.\n  In the left pane click Edge Gateways and in the right pane click on the name of the tenant\u0026rsquo;s edge.\n     Click on IPSec VPN and then click NEW.      Enter the following:\n  General Settings:\n  Name: Name the IPSec tunnel such as tenantname-gcve-ipsec\n  Click NEXT\n    Peer Authentication Mode:\n  Authentication Mode: Pre-Shared Key\n  Pre-Shared Key: Enter the PSK used for the tunnel\n  Click NEXT\n    Endpoint Configuration:\n  Local Endpoint:\n- IP Address: Local IP address of the tunnel (edge external network address) - Networks: Local network CIDR(s) for the tunnel    Remote Endpoint:\n- IP Address: Remote IP address of the VPN appliance - Networks: 0.0.0.0/0   Remote ID: Remote IP address of the VPN appliance            Click NEXT, then click FINISH to save the IPSec tunnel configuration.\n  Click on VIEW STATISTICS.\n     After a few moments, the tunnel should show the Tunnel Status and IKE Service Status as Up.      Log into the GCP provider jump host, navigate the NSX URL and log in as admin.\n  Click the Security tab, then in the left pane select Gateway Firewall, and in the right pane click the Gateway dropdown and select the tenant\u0026rsquo;s T1 to add the firewall rule to.\n     Click ADD POLICY.     Click in the Name box for the policy and provide a name such as \u0026ldquo;TenantName Tenant Rules\u0026rdquo;.     Click the three horizontal dots to the left of the policy name and select Add Rule.      Enter the following:\n  Sources: Add the remote GCP tenant project\u0026rsquo;s CIDR block\n  Destination: Select Any for any local network or alternatively it can be locked down to a single CIDR\n  Services: Any (or filter to specifics if desired)\n  Action: Allow\n        Add another rule called Allow Outbound and set the following:\n  Sources: Select Any for any local network or alternatively it can be locked down to a single CIDR\n  Destination: Add the remote GCP tenant project\u0026rsquo;s CIDR block\n  Action Allow\n  Once ready, click PUBLISH.\n       Test the tunnel connectivity by deploying an instance in the GCP tenant project that was configured for the tunnel and the tenant in the SDDC to confirm it is functioning. Here we see that SSH/HTTP is connected between both tenant workloads.    Peer Existing Customer VPC #  The following section describes the steps required to pair a tenant owned customer VPC to an existing customer VPC. This step is optional as a customer may not have an existing GCP project.\nThe steps below will require information from the customer and given to the customer to complete the peering process.\nConfigure Provider Owned Customer VPC for Peering #  The following steps describes the steps required to peer the provider owned customer VPC to an existing customer owned VPC to enable connectivity between them.\n In the GCP console, switch to the tenant to configure project and go to VPC Network -\u0026gt; VPC Network Peering.     Click CREATE PEERING CONNECTION.     Click CONTINUE.      Enter the following:\n  Name: name the VPC connection something obvious such as \u0026ldquo;tenantname-to-gpc-vpc\u0026rdquo;\n  Your VPC Network: Select the tenant\u0026rsquo;s transit network\n  Peered VPC Network: In another project\n  Project ID: The name of the customer owned project\n  VPC Network Name: The default network name in the customer\u0026rsquo;s project\n  Exchange Custom Routes: Ensure both Import and Export custom routes are checked\n  Exchange Subnet Routes with Public IP: Select Export subnet routes with public IP\n  Click CREATE\n       The status of the peering will show with a Status of Inactive until the peering process is completed on the customer VPC side.    Customer to Configure the Customer Owned VPC for Peering #  The following steps describes the steps required to peer the customer owned VPC to the provider owned customer VPC to enable connectivity between them.\n  Complete the same process as shown in the previous step and provide the customer the following information to complete the peering:\n  Peered VPC Network: In another project\n  Project ID: The name of the provider owned customer project\n  VPC Network Name: The default network name in the tenant owned customer\u0026rsquo;s project \u0026ldquo;tenantname-transit\u0026rdquo;\n  Exchange Custom Routes: Ensure both Import and Export custom routes are checked\n  Exchange Subnet Routes with Public IP: Select Export subnet routes with public IP\n  Click CREATE\n    Once the customer has completed the peering process, click REFRESH on the VPC network peering page and the Status should change to Active.\n     Click on Routes on the VPC Network page, then click on the PEERING tab and it should display a list of peering routes discovered through the peering process.     To test the connectivity, try to SSH/ping from a workload on the customers GCVE environment into a workload in the GCP peering VPC. A firewall rule will need to be in place on the customer\u0026rsquo;s VPC side allowing the connectivity if it is not already present.    Setup NAT VMs for Internet Access (Optional) #  Note that this section is optional and only required if the customer will have Internet traffic egressing from the provider owned customer project.\nThe following section describes the steps required to setup a group of VMs for NAT for Internet access for workloads within GCVE.\nThese steps are required if the Internet access is egressing from the provider owned customer project or if a customer is routing all traffic to their peered project. The NAT VMs should be created and configured in the project that Internet access is egressing from.\nPrepare and Deploy NAT VMs #  The following steps describes the steps required to prepare the environment to deploy the VMs used for NAT for Internet access and should be run from the project where the traffic will egress using NAT\u0026rsquo;s and ILB\u0026rsquo;s based on GCP compute instances.\nAnother third party solution can be used for this part which is encouraged for more flexible configuration and easier day two operations.\nTwo NAT VMs will be deployed, one in a different AZ in the region for redundancy in an active/passive configuration. The machine size for the NAT VMs below is small for testing purposes, these should be sized appropriately based on the expected throughput.\nThe shell commands are embedded in the attached text document here. ![Graphical user interface, application Description automatically\n generated](/images/cloud-infrastructure-cds-gcve/cloud-infrastructure-cds-gcve130.emf)\n  In the project where the Internet traffic will egress, in the top blue bar, click the Cloud Shell icon to launch the shell.      Prior to copying the shell commands, do a find and replace and replace the following entries. Note: Be careful not to insert any extra spaces or carriage returns to avoid syntax errors.\n  Find: gcve-team-project ; Replace with: tenant-project-name (ex: tenant1-project)\n  Find: --subnet=cds-tenant01-us-west2 ; Replace with: --subnet=tenant-transit-subnet (ex: asia-southeast1)\n  Find: cds-tenant01 ; Replace with: tenant-transit-network (ex: tenant1-transit)\n  Find: -region=us-west2 ; Replace with: -region=project-region-name (ex: asia-southeast1)\n  Find: --zone=us-west2-a ; Replace with: --zone=project-region-az-a (ex: asia-southeast1-a)\n  Find: --zone=us-west2-b ; Replace with: --zone=project-region-az-b (ex: asia-southeast1-b)\n  Find: cds-natgw-startupscripts/nat_gateway_startup.sh ; Replace with: tenant-bucket-name/name_of_startup_script.sh (ex: tenant1-storage/nat_gateway_startup.sh)\n  Find: us-west2 ; Replace with: project-region-name (ex: asia-southeast1)\n  Fine: n1-standard-2 ; Replace with: name of properly sized instance type requried\n    If there are known instances with private Ips that need public Internet routing, run the command below to allocate public IP(s) to add to the NAT startup script prior to uploading. This would need to be done for each instance that needs incoming Internet traffic.\n  gcloud compute addresses create natgw-asia-southeast1-forwarding-external-01 --region asia-southeast1\n Change the region to match where the project is located    Run the following command to display the IP that was allocated:\n gcloud compute addresses describe natgw-asia-southeast1-forwarding-external-01 --region asia-southeast1          Note this IP address to use in the startup script section below.\n  Create a storage bucket and save the startup script:\n  Under the Google Cloud Platform menu, click Cloud Storage.\n     Click CREATE BUCKET.\n     Enter a name for the bucket such as \u0026ldquo;tenantname-storage\u0026rdquo; and click CREATE.\n  Open the below attached text file.Replace the text line \u0026ldquo;iptables -t nat -A PREROUTING -d $nlb_vip -i ens4 -j DNAT --to $test_endpoint_ip \u0026quot; for $nbl_vip with the public IP allocated for the workload requiring incoming Internet connections and $test_endpoint_ip with the private IP of the workload servicing the traffic (web server, etc).   If there as a public IP address allocated for a device that needs incoming Internet traffic, replace the line below with the public IP that was allocated and the correct private IP. Also delete the other line if it is not required; this iptables command would be a line for each public IP that will be forwarded. Change the public IP after -d to what was previously allocated and the IP after DNAT to the private IP of the host (not the T1 gateway, the private IP of the device, such as the Windows server)\n iptables -t nat -A PREROUTING -d 35.236.94.128 -i ens4 -j DNAT --to 10.0.0.3    Save the file locally as \u0026ldquo;nat_gateway_startup.sh\u0026rdquo; or something similar and close it.\n  Back in the storage bucket that was created, click into the bucket name, and then click UPLOAD FILES.\n  Upload the nat_gateway_startup.sh file that was saved locally.\n    From the text file with the shell commands, copy and paste the contents to create and configure the NAT and ILB:\n  Open the GCP cloud shell\n  Copy and paste first line into Google cloud shell to create the SSH firewall rule.\n  Skip the two lines that being with \u0026ldquo;glcoud compute networks\u0026rdquo; as they should already be created.\n  Copy the two lines with \u0026ldquo;gcloud compute addresses create\u0026rdquo; and paste those into the shell and hit enter to create the addresses.\n  Copy the next two lines with \u0026ldquo;nat_#_ip=$\u0026rdquo; and paste those into the shell and hit enter to set the NAT IP variables.\n  Copy the block commands with \u0026ldquo;gcloud compute instance-templates\u0026rdquo; and paste those into the shell and hit enter the create the templates.\n  Copy the lines \u0026ldquo;gcloud compute health-checks create\u0026rdquo; down through the three lines with \u0026ldquo;glcoud compute firewall-rules create\u0026rdquo; and paste them into the shell and hit enter to create the health check and firewall rules.\n  Copy the lines \u0026ldquo;gcloud compute instance-groups managed create\u0026rdquo; and paste them into the shell and hit enter to create the instances.\n  Copy the line \u0026ldquo;gcloud compute health-checks create\u0026rdquo; and paste it into the shell and hit enter to create the next health check.\n  Copy the line \u0026ldquo;gcloud compute backend-services create\u0026rdquo; and paste it into the shell and hit enter to create the natgw backend.\n  Copy the two lines \u0026ldquo;gcloud compute backend-services add-backend\u0026rdquo; and paste them into the shell and hit enter to add the two nats to the backend that was just created.\\\n  The lines under Just Outbound NAT can be skipped if the customer has both incoming and outgoing traffic.\n  Copy the line \u0026ldquo;gcloud compute forwarding-rules create\u0026rdquo; and paste it into the shell and hit enter to create the forwarding rule.\n  Copy the lines for \u0026ldquo;gcloud compute routes create\u0026rdquo; and paste them into the shell and hit enter to create the two routes.\n  Under Public IP Exposure settings, copy the line \u0026ldquo;gcloud compute backend-services create\u0026rdquo; and paste it into the shell and hit enter to create the backend service for the ILB.\n  Copy the two lines with \u0026ldquo;gcloud compute backend-services add-backend\u0026rdquo; and paste them into the shell and hit enter to add the hosts to the backend.\n  If a public IP was allocated previously for an existing workload, copy the last line with \u0026ldquo;gcloud beta compute forwarding-rules\u0026rdquo; and paste it into the shell and hit enter to add the forwarding rule.\n  Note: When adding a new public IP for a workload, the last two lines in this file would need to be reran to allocate the public IP, then create a forwarding rule for it.\n    Configure Firewall and Routes #  The following steps describes the steps required create the firewall rule and routes required to load balancer Internet traffic across the 3 NAT internet gateways that were previously deployed.\nThe default routes for the NAT will use instance tags to keep from the routes leaking back into the GCVE environment. This tag can be whatever the provider desires, but it must be uniform across all 3 routes that will direct traffic to the Internet via the NAT gateways. This applies to the routes created below for nat1, nat2, and nat3. This tag must match the one used on the instances created previously (VPN host).\n  In the provider owned customer tenant, navigate to Cloud shell and enter the following command to allow intervpc communication, changing the \u0026ldquo;\u0026mdash;network=tenant1-transit\u0026rdquo; to the customer\u0026rsquo;s transit network.\n gcloud compute firewall-rules create intervpc-communication1 --direction=INGRESS --priority=100 --network=tenant1-transit --action=ALLOW --rules=all --source-ranges=10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,100.64.0.0/16 --target-tags=natgw    Create a firewall rule for the NAT health check by running the following command, changing the \u0026ldquo;\u0026mdash;network-tenant1-transit\u0026rdquo; to the name of the customer\u0026rsquo;s transit network.\n gcloud compute firewall-rules create \u0026quot;natfirewall\u0026quot; --allow tcp:80 --target-tags natgw --source-ranges \u0026quot;209.85.152.0/22\u0026quot;,\u0026quot;209.85.204.0/22\u0026quot;,\u0026quot;35.191.0.0/16\u0026quot; --network=tenant1-transit    Click on Routes in the left pane and click CREATE ROUTE and enter the following:\n  Name: natgatewayout\n  Network: tenantname-transit\n  Destination IP range: 0.0.0.0/0\n  Priority: 50\n  Next Hope: Default internet gateway\n  Click CREATE\n        Click CREATE ROUTE and enter the following:\n  Name: nat1\n  Network: tenantname-transit\n  Destination IP range: 0.0.0.0/0\n  Priority: 100\n  Instance tags: intravpcdefault\n  Next Hop: Specify an instance\n  Next Hop Instance: nat-1\n  Click CREATE\n        Create two more routes, one named nat2 and one nat3 with the same priority of 100 and specify the next instance as nat-2 and nat-3 respectively for their routes as well as the same instance tag.\n  To test the functionality of the NAT VMs:\n  Open an SSH session to each NAT VM and installing tcpdump with \u0026ldquo;sudo apt install tcpdump -y\u0026rdquo;\n  On each host, run tcpdump with a filter set to the IP of a test workload in GCVE with \u0026ldquo;tcpdump -ni ens4 host ip_of_workload\u0026rdquo;\n  On the GCVE workload, repeatedly curl a public URL such as vmware.com and the traffic hits should spread across the three NAT VMs.\n      Conclusion #  At this point, the VMware Cloud Director service Instance is ready to deploy tenant VMs. For more information see the documentation for VMware Cloud Director service and VMware Cloud Director.\n"},{"id":42,"href":"/docs/cloud-infrastructure/cds-vmc-deployment-guide-msp/","title":"VMware Cloud Director Service for VMware Cloud on AWS Deployment Guide","section":"Cloud Infrastructure","content":"VMware Cloud Director Service for VMware Cloud on AWS Deployment Guide #  VMware Cloud Director™ service enables cloud providers to use VMware Cloud on AWS in multi-tenant modality with enhanced VMware NSX-T support, allowing provisioning of custom-sized, tenant-based, isolated, and secure VMware Cloud resources. This ability to service multiple smaller and medium sized tenants on the same infrastructure, offers flexibility to right-size the VMware Cloud on AWS environments to meet customer needs and requirements of all customer tiers.\nVMware Cloud Director service is a container-based SaaS version of the proven VMware Cloud Director on-premises service-delivery platform. The service, available through VMware Cloud Partner Navigator, helps cloud providers gain better economies of scale and generate new value and revenue for their cloud businesses.\nThis guide details the process of deploying a VMware Cloud Director service Instance, associating it with a VMware Cloud on AWS SDDC designed for use with VMware Cloud Director service, configuring the Provider Virtual Datacenter to use the resources of the associated VMware Cloud on AWS SDDC, configuring the VMware Cloud on AWS SDDC networking to prepare it for multi-tenant use and deploying the first tenant organization.\nBefore using this guide, it is necessary to join the VMware Managed Service Provider Program and have the require contracts in place to use the VMware Cloud services mentioned in this guide. See the MSP VMware Cloud on AWS Operations Handbook and the Cloud Director service Operations Handbook for more details.\nPrepare a VMware Cloud Partner Navigator Provider Organization #  Provision a new VMware Cloud Partner Navigator Organization\n  Click on Administration, then Add Organization\n  Fill out the Add Organization form and click Add Organization\nRequest access to VMware Cloud Director service by emailing: ask_cloud_director_service@VMware.com\n  Supply the Long ID of the Organization that will be used.\n  A onetime use invitation will be returned to activate VMware Cloud Director service.\nEnable the VMware Cloud on AWS and VMware Cloud Director service services in the new Organization\n  Select the new Provider Organization and click Manage Services\n  Click Continue\n  Click Open on both service tiles to activate the services   Click Open to continue\n  NOTE: It is also acceptable to use an existing Provider Organization enabled for VMware Cloud on AWS [which has no SDDCs currently deployed.]{.underline}\nDeploy a SDDC #  Deploy the SDDC in the same VMware Cloud Partner Navigator Organization activated above.\n  Select the VMware Cloud on AWS service tile   Select SDDCs and click Create SDDC\n  Fill out the form to deploy the SDDC according to your requirements. For more details about the deployment process for VMware Cloud on AWS SDDCs see Deploy an SDDC from the VMC Console.\n  NOTE: All VMware Cloud on AWS SDDCs used with VMware Cloud Director service must be deployed in Organizations that have been enabled for VMware Cloud Director service. SDDCs deployed in other Organizations are not compatible with VMware Cloud Director service.\nDeploy VMware Cloud Director service Instance #    Select the VMware Cloud Director service tile  If someone other than the Organization Owner will be deploying VMware Cloud Director service Instances the Organization Owner must first establish a trust relationship between VMware Cloud services and VMware Cloud Director service.\na. In the Cloud Director Instances screen select Configuration then Configure OAuth Trust Relationshipb. Select Dismiss once the trust is established\n![Graphical user interface, text, application Description automatically generated](/images/cloud-infrastructure-cds-vmc/cloud-infrastructure-cds-vmc12.png)    Select Create Instance to begin the instance deployment process.   Enter the data needed in the form and click on NEXT\n  Note: For the Upgrade Category, selecting Preview (if enabled) identifies this Cloud Director service Instance to be patched or upgraded earlier than when Production is selected. Use Preview for service development environments. The Upgrade Category cannot be changed after deployment.\n Acknowledge costs and then click on CREATE INSTANCE\n  Click on Activity Log for detailed information about the deployment progress.\n  When theÂ VMware Cloud DirectorÂ instance deployment is complete, its card displays aÂ ReadyÂ status.\nGenerate API Token #  An API token for the Organization holding the SDDC is used to associate the SDDC with VMware Cloud Director service.\n  Click on your name in the top right and then click on My Account\n  Click on API Tokens\n  Click on GENERATE TOKEN\n  Enter form fields and then click on GENERATE\n  Notes: This token is only used during the association process, so its Token TTL should be short. Minimum required Organization Role is Organization Member. Minimum required Service Roles are VMware Cloud on AWS - Administrator and VMware Cloud on AWS - NSX Cloud Admin. Make sure to store the generated token in a safe place.\nCreate a DHCP Network #  Need to create a network segment that has routed access to the SDDC management network, provides DHCP service and has a DNS server configured. Note: This step can be skipped if you only have one host in your SDDC.\n  Click on ADD SEGMENT\n  Enter segment details and then click on SET DHCP CONFIG\n  Enter details and then click on APPLY\n  Click SAVE\n  S3 Configuration #  If your SDDC is in Oregon (us-west-2), you will need to either disable S3 or configure a S3 VPC endpoint prior to associating the SDDC. If your SDDC is in any other region, this step can be skipped. By default, S3 traffic in the local region fails until either a VPC endpoint is configured or S3 is disabled. Part of the associate process automatically deploys a proxy appliance into the SDDC. That proxy appliance currently is stored on an S3 bucket in Oregon (us-west-2), which given the default configuration makes it inaccessible.\nDisable S3 #   Click DISABLE\n  Configure S3 VPC endpoint #  See the Amazon Virtual Private Cloud AWS PrivateLink documentation: Endpoints for Amazon S3\nAssociate VMware Cloud Director service Instance #    Click Associate a VMC SDDC\n  Enter fields and then click on ASSOCIATE a VMC SDDC\n  Note: If you only have one host in your SDDC, you can use the predefined network named \u0026ldquo;sddc-cgw-network-1\u0026rdquo; for the Proxy VM Network field.\nAssociate Custom Domain (Optional) #  Allows the provider to use their own domain name for VMware Cloud Director service Instances.\n  Click on ACTIONS and then click on Associate Custom Domain\nSee the VMware Cloud Director service documentation Customize the DNS and Certificate Settings for more details.\n  VMware Cloud Director service Instance Configuration #  Launch the Provider Portal #    To use VMware Cloud Services as an Identity Provider for VMware Cloud Director service Instances, select Configure VMware Cloud Services as instance IDP from the Actions menu of each VMware Cloud Director service Instance.   Select Configure IDP\n  Click on OPEN INSTANCE\n  NOTE: To control user access when VMware Cloud Services is used as the VMware Cloud Director service Instance IDP, use the Role Assignment feature in VMware Cloud Partner Navigator to grant users Admin, Read Only or No Access roles to all IDP enabled VMware Cloud Director service Instances. The Admin role will login to instances with the CDS Provider Admin role. The Read Only role will login to instances with the CDS Provider Admin Read Only role.\nCreate Provider VDC #    Click on NEW\n  Enter details and then click on NEXT\n  Select vCenter and then click on NEXT\n  Select Resource Pool, Hardware version and then click on NEXT\n  Select the VMC Workload Storage Policy \u0026ndash; Cluster-1 and then click on NEXT.   Note: Make sure that the other storage policies are left unselected.\n Select NSX-T manager and Geneve Network pool and then click on Next.\n  Click on FINISH\n  Update External Network with valid IP range #    Click on Tier-0 Gateways in the left pane and then click the name of the pre-existing External Network.\n  Click on Network specification and then Edit.\n  Delete existing network spec by selecting it and then clicking on DELETE\n  Click on NEW\n  Enter Gateway CIDR and click on the pencil.\n  Note: We recommended that you use a subnet in the 100.64.0.0/10 range (RFC-6598 \u0026ndash; Carrier-grade NAT) to avoid conflicts with RFC-1918 private address space used in on premises locations and allow for extensive NATing of tenant IPs to the External Network. For example, you may choose 100.68.1.1/24 for the first SDDC deployed and use 100.68.2.1/24 for the second SDDC and so on. It is important to note that NSX-T uses 100.64.0.0/16 for T0-T1 interlink and is not available for use in an NSX-T environment like VMware Cloud on AWS.\n Enter Static IP Pools, click ADD and then click on SAVE\n  Click on **Save\n**  Create Inventory Group for External Networks #    Click Add Group\n  Enter Name and click on Set Members\n  Click on IP Addresses, enter the range associated with the external network previously specified and then click on APPLY\n  Click on SAVE\n  Create Firewall Rule to Allow VMware Cloud Director service Tenant Traffic #    Click Add Rule\n  Add rule details and then click on PUBLISH.\n  Notes: For Sources, make sure to select the Group created previously (VMware Cloud Director service External Network Ips). For Applied To, make sure to select Internet Interface.\nCreate First Tenant #  Create Organization #   Click on New  Fill in details and click on CREATE\n  Create Organization VDC #    Click on NEW\n  Fill in General details and click NEXT\n  Select Organization and click NEXT\n  Select Provider VDC and click NEXT\n  Select Allocation Model and click NEXT\n  Enter details and click NEXT\n  Select Storage Policy and click NEXT\n  Note: Thin provisioning and Fast provisioning are recommended, but not required.\n Select Network Pool and click NEXT\n  Confirm and click FINISH\n  Create Edge Gateway #    Click on NEW\n  Select Organization VDC and click on NEXT\n  Enter Name and click on NEXT\n  Select External Network and click on NEXT\n  Select Edge Cluster and click on NEXT\n  Add IP Allocation, click ADD, then click on NEXT\n  Review and click FINISH\n  Request a public IP for Tenant\u0026rsquo;s edge #    Click REQUEST NEW IP\n  Enter Notes and then click SAVE\n  Create a NAT pointing to the tenant\u0026rsquo;s edge gateway #    Click on ADD NAT RULE.\n  Select previously created public IP and make sure that the Internal IP matches the IP assigned to the edge gateway.\n  Click SAVE\n  Create Organization network #    Open Tenant portal by click on the box with the arrow next to the selected organization\n  Click on NEW\n  Select Scope and then click NEXT\n  Select Routed and then click NEXT\n  Select edge and the click NEXT\n  Enter name and CIDR and then click on NEXT\n  Enter Static IP Pool and then click on NEXT\n  Enter DNS and then click on NEXT  Review and click FINISH\n  Create SNAT to allow outbound traffic #    Select proper edge gateway and then under NAT click on NEW\n  Enter SNAT information and then click on SAVE.\n  Note: Make sure External IP is on the Edge and that the Internal IP matches the org network subnet\nConclusion #  At this point, the VMware Cloud Director service Instance is ready to deploy tenant VMs. For more information see the documentation for VMware Cloud on AWS, VMware Cloud Director service and VMware Cloud Director.\n"},{"id":43,"href":"/docs/cloud-infrastructure/vcf-metering/","title":"VMware Cloud Foundation Metering and Reporting","section":"Cloud Infrastructure","content":"VMware Cloud Foundation Metering and Reporting #  Configuration #  The Usage Meter administrator configures a vCenter and selects any of the VCF license editions to enable VCF for metering. Metering Time-based average of host CPU core count aggregated by the following different VCF editions:\n VMware Cloud Foundation for Cloud Providers Standard VMware Cloud Foundation for Cloud Providers Advanced VMware Cloud Foundation for Cloud Providers Enterprise VMware Cloud Foundation for Cloud Providers Standard without vSAN VMware Cloud Foundation for Cloud Providers Advanced without vSAN VMware Cloud Foundation for Cloud Providers Enterprise without vSAN  Time-based average capped billed vRAM GB per VM VCF edition:\n VMware Cloud Foundation for SDDC Manager  Feature Detection #  When metering VMware Cloud Foundation, Usage Meter performs two types of usage collections: event-based collections and inventory collections. What is reported are the host core CPUs and vRAM. Host core CPUs are collected from vCenter event-based collections and inventory collections. vRAM is collected from vCenter usage collections.\nHost Core Reporting #  Usage is calculated on a per ESXi host level for the duration the host is in use. The logic applies to editions with or without vSAN. Avg number of Cores = (sum of cores consumed per-hourly collections) / (hours in month)\n If at least one VM is running on a host (VM is in powered-on state), the host cores are reported for the duration of the VM is powered-on. If a host, whether it’s been modified, added, or placed into migration mode (i.e., host state changes) If a host is running with no powered-on VMs, the host cores will not be reported no matter if storage is consumed or not. Change of the VCF edition: Edition is applied to all cores of a host for the duration the edition is configured. When the VCF edition changes, the new edition is immediately applied to all cores of the host.  vRAM Reporting #  Usage is calculated as an average of the hourly vRAM usage over a month on vSphere instances marked as Rental (the same as vCenter). Avg Capped Billed vRAM (GB) = (sum of consumed GB per-hourly collections) / (hours in month) Hourly GB = min(max(memorySizeMB/2,reserved-memoryReservation),memory cap) memory cap = 24 * 1024 (24 GBs)\n Only powered-on VMs will be reported. Only one edition, SDDC Manager, will be reported as a vRAM metering. It is an add-on product, separate from the host core metering. It is not bundled with any other product.  Sample Monthly Usage Report #  Monthly Usage Units\n   Product Hostname Version VC UUID Unit of Measure Units to be Reported     VMware Cloud Foundation for Cloud Providers Standard    Avg Number of Cores 50   VMware Cloud Foundation for Cloud Providers Advanced    Avg Number of Cores 100   VMware Cloud Foundation for Cloud Providers Enterprise    Avg Number of Cores 100   VMware Cloud Foundation for Cloud Providers Standard without vSAN    Avg Number of Cores 50   VMware Cloud Foundation for Cloud Providers Advanced without vSAN    Avg Number of Cores 100   VMware Cloud Foundation for Cloud Providers Enterprise without vSAN    Avg Number of Cores 100   VMware Cloud Foundation for SDDC Manager    Avg Capped Billed vRAM (GB) 456    "},{"id":44,"href":"/docs/cloud-infrastructure/vra-metering/","title":"vRealize Automation Metering and Reporting","section":"Cloud Infrastructure","content":"vRealize Automation Metering and Reporting #  VMware vRealize Automation (vRA) is available to service providers in two editions:\n vRealize Automation Advanced vRealize Automation Enterprise  Product Detection #  Service Provider registers vRA instance(s) in the Usage Meter 4.5.0.1 web application. Then the service provider enters a Cafe and IaaS hostname, port, username, and password. A validation of the user credentials is performed and if correct, the data collection starts. Usage Meter connects to the vRA Café server and collects its license edition. The appliance also connects to vRA IaaS and collects the number of managed VMs. Data is collected on an hourly basis.\nMetering #  The metering of vRealize Automation is per VM. The reported usage is calculated by the monthly average hourly count of non-unique VMs in the month. Example: If during the first 15 days of the month, there are 2 VMs and the last 15 days of the same reporting month, there are 4 VMs, the at the end of the month, the Monthly Usage Report will report 3 VMs (an average count of the non-unique VMs detected throughout the month).\nReporting #  Sample Monthly Usage Report adds a new line item for vRA.\n   Product Version Unit of Measure Units to be Reported     vRealize Automation Advanced  Managed VMs 1547    Note: When VMware Cloud Foundation is added for metering, all vRA lines are visible on vCloud Usage Insight but will not be sent to the VMware Commerce Portal. Providers update manually their vRA reporting in the Commerce Portal after reviewing the report in vCloud Usage Insight.\n"},{"id":45,"href":"/docs/cloud-infrastructure/vrni-metering/","title":"vRealize Network Insight Metering and Reporting","section":"Cloud Infrastructure","content":"vRealize Network Insight Metering and Reporting #  VMware vRealize Network Insight is available to service providers in two editions:\n vRealize Network Insight Advanced vRealize Network Insight Enterprise  Metering #  vRealize Network Insight is reported depending on the provider’s preference. The following are the available reporting options for vRealize Network Insight:\n Standalone – When vRNI is reported as a standalone product, then the metering is per VM or OSI per Month. The usage is calculated as the average hourly count of non-unique VMs in the month. Flex model add-on – When vRNI is reported as a Flex Add-on product, then the metering is per GB per Month.  Feature Detection #  Usage Meter detects if a VM is managed by vRealize Network Insight. If NSX-T Enterprise Plus edition and vRealize Network Advanced is detected for the same VM, the VM will not be reported for vRealize Network Insight.\n"},{"id":46,"href":"/docs/cloud-infrastructure/vrops-metering/","title":"vRealize Operations Metering and Reporting","section":"Cloud Infrastructure","content":"vRealize Operations Metering and Reporting #  Usage Meter supports several vRealize Operations (vROps) configurations:\n A Service Provider managed vRealize Operations server monitoring a single tenant. A Service Provider hosted vRealize Operations server for multiple tenants. A Service Provider hosted vRealize Operations server that manages VMs hosted by SP or hosted on customer’s premises. A vRealize Operations server configured with multiple vCenter servers.  Configuration #  vRealize Operations in Usage Meter 4.5.0.1 is added the same way as the rest supported VMware products. Providers need to enter the endpoint IP address or hostname, username, and password. If a vCenter server monitored by vROps is added to Usage Meter, then this vCenter will appear as a referenced vCenter in the details of the added vROps instance.\nvCenter servers monitored by a vRealize Operations server, which are also added for metering by Usage Meter, are named “Managed vCenter” servers. vCenter servers monitored by a vRealize Operations server but not added for metering to Usage Meter are named “Unmanaged Servers”. Both Managed and Unmanaged vCenter servers are listed in Usage Meter vRealize Operations report lines.\n“Unmanaged Servers” can be associated with only one vRealize Operations instance if monitored by multiple instances. This will be the vRealize Operations instance with the highest edition. Virtual machines part of that “Unmanaged Servers” will be billed once no matter how many vRealize Operations instances are monitoring them. For the billing of monitored VMs will be taken the vRealize Operations instance with the highest edition.\nUsage Meter connects to a vRealize Operations server using the account provided by the Usage Meter administrator. Usage Meter will meter all VMs that are visible through the provided account.\nTo meter only a subset of monitored VMs, for example, to divide monitoring by Tenant, vRealize Operations should be configured to filter the monitored virtual machines to include only a subset of the VMs deployed to the monitored vCenter instance.\nThis is accomplished by creating a new account in vRealize Operations and configuring Role Base Access Controls to restrict the view of the account to the desired Virtual Machines. The RBAC restricted account and credentials are provided to Usage Meter when the vRealize Operations server registration is completed.\nRefer to the Product Usage Guidefor detailed configuration instructions.\n) Figure 10\nMetering #  The vRealize Operations license edition is retrieved by Usage Meter from the vRealize Operations server during each data collection. vRealize Operations metering is based on Virtual Machines monitored by a vRealize Operations Server. Service providers are responsible for reporting usage for monitored VMs and OSI Instances (which represent non-virtualized servers). Usage Meter 4.5.0.1 does not track OSI instances monitored by vRealize Operations or add-on packages such as Blue Medora. Usage Meter 4.5.0.1 automatically detects the following vRealize Operations editions:\n vRealize Operations Standard vRealize Operations Advanced vRealize Operations Enterprise  When Usage Meter connects to a vRealize Operations instance, it checks for its version. If the version of the metered vRealize Operations instance is 8.6 or later, Usage Meter checks if there are VMs part of a license group. If there are such VMs, Usage Meter 4.5.0.1 reports them against the highest vRealize Operations edition assigned to that license group. In case there is a VM part of multiple license groups, then Usage Meter will report the highest edition of the first license group it queries.\nFor vRealize Operations instances with previous versions, Usage Meter 4.5.0.1 will only report the monitored VMs against the highest vRealize Operations edition that it detects. Usage Meter 4.5.0.1 will not check if there are VMs part of license groups for vRealize Operations before version 8.6.\nFor example, if Usage Meter 4.5.0.1 connects to vRealize Operations 8.5 and it detects two license editions: Advanced and Standard, then for all VMs monitored by that vROps, the reported license by Usage Meter 4.5.0.1 will be Advanced. In the Monthly Usage Report, the highest detected vROps license edition by Usage Meter will be reported irrespective of the vROps version. The highest license against which a VM is reported can be checked in the Virtual Machine History Report.\nReporting #  vRealize Operations is reported by Usage Meter 4.5.0.1 as either Flex Add-on or standalone. When usage is reported as a Flex Add-on, the metric is the Avg Capped Billed vRAM value of the monitored Virtual Machine. When reported standalone, the metric is the Average Number of Virtual Machines monitored by vRealize Operations during the reporting month.\nThe following rules apply to the reporting of vRealize Operations:\n vRealize Operations Standard, Advanced and Enterprise editions are reported as a Flex add-on or standalone depending on the customer’s preference. vCenter servers using vRealize Operations will always be reported as a standalone line item on the monthly usage report as (unmanaged) if they are not added to vCloud Usage Meter. When vRealize Operations edition is reported as standalone, and the vCenter servers monitored by it are added to Usage Meter 4.5.0.1, then the monthly usage report will include a single line vRealize Operations edition (managed). Unmanaged vCenter Servers are associated with only one vRealize Operations instances if monitored by multiple vRealize Operations instances. The VMs of those unmanaged vCenter Servers will be billed once based on the vRealize Operations instance with the highest edition.  Metering Scenarios #  Scenario One: vRealize Operations Server monitoring both SP and Tenant vCenters\nFigure 11\nConsider the following reporting scenario: A vCenter server is registered with Usage Meter.\nA vRealize Operations server that monitors the same vCenter is also added for metering to Usage Meter 4.5.0.1. Usage Meter queries the vRealize Operations server to determine what vCenter servers it monitors. The vRealize Operations server returns a list of three vCenter servers, only one of which is registered with Usage Meter. Usage Meter monthly report will include 2 Unmanaged vRealize Operations servers and 1 Managed vRealize Operations server line items.\n"},{"id":47,"href":"/docs/cloud-infrastructure/vsan-metering/","title":"vSAN Metering and Reporing","section":"Cloud Infrastructure","content":"vSAN Metering and Reporting #  vSAN is activated at a host-cluster level either while creating new clusters or updating existing host clusters. vSAN aggregates all local storage devices across all hosts in the vSAN cluster into a single datastore. More storage devices or hosts can be added to the cluster to expand its datastore capacity. If a host contributes to the vSAN datastore, it must provide at least one device for flash cache and another for persistent storage. Capacity devices are also called data disks. The devices on the contributing hosts form one or more disk groups, and each disk group contains one flash cache device and one or more capacity devices. Devices used for caching cannot be shared across disk groups. Each host can be configured to use multiple disk groups. The amount of physical capacity from the vSAN datastore consumed by one or more VMs at any point is called the consumed capacity.\nConfiguration #  With Usage Meter 4.5.0.1, vSAN is automatically detected. There should be at least one vSAN cluster per vCenter Server so that Usage Meter detects and meters it. When one or more clusters in the vCenter are enabled with vSAN, the vCenter product configuration list page in the Usage Meter web application indicates that vSAN is associated and used for that vCenter.\nFeature Detection #  vSAN usage is metered per vSAN enabled cluster (see Cluster History Report) and reported aggregated by vSAN license editions (see Monthly Usage Report). vSAN usage is collected on an hourly basis by the Usage Meter appliance. vSAN usage collection intervals are closed when one or more of the following scenarios occur:\n vSAN collection fails to send collected data for 24 hours or more vSAN enabled cluster is deleted vSAN license has changed When vCenter product is not metered and/or deleted from the Usage Meter web app Products page.  Note: The collection intervals are shown in the vSAN Cluster History Report.Specific space efficiency and QoS features are detected during these hourly collections and are used to determine the billing rate of vSAN usage.\nvSAN usage is metered at a cluster level. While there are many features and capabilities that vSAN offers, from the perspective of usage metering, Usage Meter 4.5.0.1 records the following cluster-level and VM-level features:\n Cluster level: Base , Deduplication, Compression, Stretched Cluster, Data-at-rest Encryption, File Services, HCI Mesh, Shared Witness, Data in Transit Encryption, and Cloud Native Storage. When these features are enabled, the entire cluster is marked as using these features. At a VM level, usage is reported per cluster: Raid 5/6 Erasure Coding feature is enabled at the individual virtual machine level but scoped and reported at the cluster level. If one or more virtual machines (regardless of the power state) have enabled this feature, then this feature is considered enabled for the entire cluster. Usage Meter 4.5.0.1 examines the vSAN features enabled for the cluster to determine the reported license edition. The following vSAN features are automatically detected by Usage Meter and reported in the vSAN Cluster History Report.     vSAN Features vSAN Standard vSAN Advanced vSAN Enterprise     Stretched Cluster   X   Raid 5/6 Erasure Coding  X X   File Services   X   Deduplication and Compression  X X   Data-at-rest Encryption   X   Base X X X   HCI Mesh   X   Shared Witness X X X   Data in Transit Encryption   X   Cloud Native Storage X X X    Note: Usage Meter 4.5.0.1 detects VMs compliant with Storage Policy - Raid 5/6 Erasure Coding. When one or more VMs in a vSAN enabled cluster are compliant with this policy, customers are billed at vSAN Advanced edition.\nvSAN Reporting #  vSAN usage is reported as monthly aggregated total used storage capacity by a vSAN license edition in the Monthly Usage Report generated by Usage Insight. More granular information about the vSAN usage can be seen in the vSAN Cluster History Report (see an example below). This report includes information about what storage capacity and features have been used by a cluster per time interval. It also shows the detected vSAN license edition of a cluster. The features that are used by a vSAN cluster can be calculated from the vSANFint column in that report. See the vSAN Cluster History Report information. The average used storage for vSAN is calculated as: Average GB = (Sum of consumed storage capacity in GB per-hourly collections) / (Hours in a month). The value can be zero if no vSAN usage exists or vSAN was not configured. Average Used Storage means the storage capacity consumed by all virtual machine disks (VMDK) and not available for new allocations in GB averaged during the applicable reporting period. This includes swap space (if utilized) and the RAID overhead set within the storage policy.\nNote: Usage Meter 4.5.0.1 only reports usage against the vSAN license the detected features are part of. For example, a cluster with a vSAN Enterprise license is detected to use features also part of vSAN Advanced, like Compression and Deduplication. In this case, if there are no other features used that are only part of vSAN Enterprise during the reporting month, then the Monthly Usage Report will report usage against the vSAN Advanced edition. The following vSAN editions are reported in the generated reports by Usage Insight:\n vSAN Standard. vSAN Advanced. vSAN Enterprise  Note: vSAN is not reported standalone and included in Horizon DaaS bundles. Also, vSAN with Desktop and ROBO licenses are excluded from the usage reports.\nvSAN Cluster History Report #  vSAN Cluster History Report contains the used capacity (in MB) per time interval for vSAN enabled clusters in vCenter Servers metered by Usage Meter 4.5.0.1. Additionally, it displays the vSAN license edition and the features used by a cluster. Note: To verify which vSAN cluster features were detected by Usage Meter 4.5.0.1, use the integer value in the table below and compare it with the vsanFInt column in the Cluster History Report.\nA vSAN cluster might use several features, and the way to find them is to subtract from the vSANFint value for a cluster the highest possible int value of a vSAN feature detected by Usage Meter.\nFor example, if you have a value of 6 in the vSANFint column in the cluster history report, subtract 4 (the highest possible value of a vSAN feature that can be subtracted from 6). The int value of 4 represents Compression, as seen from the table below. The result of the subtraction will be 2 (6-4), which stands for Deduplication. In conclusion, the cluster with a vSANFint value of 6 in the vSAN cluster history report uses two vSAN features: Compression and Deduplication.\n   Feature Feature Int Value     BASE 1   DEDUPLICATION 2   COMPRESSION 4   ERASURE_CODING 8   STRETCHED_CLUSTER 16   DATA_AT_REST_ENCRYPTION 32   FILE_SERVICES 64   DATA_IN_TRANSIT_ENCRYPTION 128   CLOUD_NATIVE_STORAGE 256   HCI_MESH 512   SHARED_WITNESS 1024    Sample Cluster History Report and Monthly Usage Report\n   VCHostName vSAN ClusterId vSAN ClusterName vSAN License vSAN Used (MB**)** From To Interval vSANFint     0UfypLEAUZCz domain-xyz 0UfypLEAUZCy std 2500 2021-12-18 11:32:14 2021-12-18 17:05:31 05:33:17 1   0UfypLEAUZCz domain-xyz 0UfypLEAUZCy std 3500 2021-12-19 11:32:14 2021-12-19 19:05:31 08:33:17 1   0UfypLEAUZCz domain-abc 0bxyzABCkMy ent 9500 2021-12-18 17:05:31 2021-12-19 18:05:31 1 day 00:59:59 6   0UfypLEAUZCz domain-abc 0bxyzABCkMy ent 2500 2021-12-20 11:05:31 2020-12-20 18:45:50 07:40:19 6    In the Cluster History Report, the vSAN license is the edition activated for a cluster and detected by Usage Meter 4.5.0.1. Another important column in the report is vSANFint. With the values in this column, you can identify the features used by a cluster per time interval. The detected features determine the vSAN license edition by which the aggregated usage will be reported in the Monthly Usage Report. There might be a difference between the detected license edition activated for a cluster and the reported license edition in the Monthly Usage Report. This happens when the cluster has used only a portion of the features part of the license. These features are also part of another license, e.g., a cluster with an Enterprise license has only used features that are also part of the vSAN Standard edition. As a result, only what has been used during the reporting period is reported against the respective license edition in the Monthly Usage Report. See below what the Monthly Usage Report will be for the vSAN consumption shown in the above Cluster History Report:\n   Product Hostname Version VC UUID Unit of Measure Units to be Reported     VMware Virtual SAN Standard    Avg Billed vSAN Storage (GB) 2   VMware Virtual SAN Advanced    Avg Billed vSAN Storage (GB) 5    The units reported in the Monthly Usage Report for vSAN are calculated based on the average vSAN storage capacity consumption throughout the month in GB per vSAN license edition, rounded down to the nearest whole number.\nNote: The vSAN storage consumption in the Cluster History Report is shown in MB; thus, it must be converted to GB to check if the usage in the Cluster History Report matches that in the Monthly Usage Report.\n"}]